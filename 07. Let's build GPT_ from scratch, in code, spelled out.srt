1
00:00:00,000 --> 00:00:05,680
 Hi everyone. So by now you have probably heard of chat GPT. It has taken the world and AI community

2
00:00:05,680 --> 00:00:12,560
 by storm and it is a system that allows you to interact with an AI and give it text-based tasks.

3
00:00:12,560 --> 00:00:17,200
 So for example, we can ask chat GPT to write us a small haiku about how important it is that people

4
00:00:17,200 --> 00:00:22,000
 understand AI and then they can use it to improve the world and make it more prosperous. So when we

5
00:00:22,000 --> 00:00:29,440
 run this, AI knowledge brings prosperity for all to see embraces power. Okay, not bad. And so you

6
00:00:29,440 --> 00:00:35,200
 could see that chat GPT went from left to right and generated all these words sequentially.

7
00:00:35,200 --> 00:00:40,640
 Now I asked it already the exact same prompt a little bit earlier and it generated a slightly

8
00:00:40,640 --> 00:00:46,480
 different outcome. AI's power to grow, ignore insults us back, learn, prosperity, weights.

9
00:00:46,480 --> 00:00:52,160
 So pretty good in both cases and slightly different. So you can see that chat GPT is a probabilistic

10
00:00:52,160 --> 00:00:57,360
 system and for any one prompt it can give us multiple answers sort of replying to it.

11
00:00:58,240 --> 00:01:02,720
 Now this is just one example of a prompt. People have come up with many, many examples and there

12
00:01:02,720 --> 00:01:09,440
 are entire websites that index interactions with chat GPT. And so many of them are quite humorous,

13
00:01:09,440 --> 00:01:15,840
 explain HTML to me like I'm a dog, write release notes for chess too, write a note about Elon Musk

14
00:01:15,840 --> 00:01:22,400
 buying a Twitter and so on. So as an example, please write a breaking news article about a leaf

15
00:01:22,400 --> 00:01:28,000
 falling from a tree and a shocking turn of events. A leaf has fallen from a tree in the local

16
00:01:28,000 --> 00:01:32,400
 park. Witnesses report that the leaf which was previously attached to a branch of a tree

17
00:01:32,400 --> 00:01:37,360
 detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty

18
00:01:37,360 --> 00:01:45,120
 remarkable system and it is what we call a language model because it models the sequence of words

19
00:01:45,120 --> 00:01:50,640
 or characters or tokens more generally and it knows how sort of words follow each other in

20
00:01:50,640 --> 00:01:56,960
 English language. And so from its perspective what it is doing is it is completing the sequence.

21
00:01:56,960 --> 00:02:01,920
 So I give it the start of a sequence and it completes the sequence with the outcome.

22
00:02:01,920 --> 00:02:07,440
 And so it's a language model in that sense. Now I would like to focus on the under the hood of

23
00:02:07,440 --> 00:02:12,960
 under the hood components of what makes chat GPT work. So what is the neural network under the

24
00:02:12,960 --> 00:02:19,280
 hood that models the sequence of these words. And that comes from this paper called Attention is

25
00:02:19,280 --> 00:02:27,680
 All You Need. In 2017 a landmark paper a landmark paper in AI that proposed the transformer architecture.

26
00:02:27,680 --> 00:02:35,520
 So GPT is short for generatively pre-trained transformer. So transformer is the neural

27
00:02:35,520 --> 00:02:41,040
 nut that actually does all the heavy lifting under the hood. It comes from this paper in 2017.

28
00:02:41,040 --> 00:02:46,080
 Now if you read this paper this reads like a pretty random machine translation paper and that's

29
00:02:46,080 --> 00:02:50,000
 because I think the authors didn't fully anticipate the impact that the transformer would have on the

30
00:02:50,000 --> 00:02:55,600
 field. And this architecture that they produced in the context of machine translation in their case

31
00:02:55,600 --> 00:03:02,320
 actually ended up taking over the rest of AI in the next five years after. And so this architecture

32
00:03:02,320 --> 00:03:09,040
 with minor changes was copy-pasted into a huge amount of applications in AI in more recent years.

33
00:03:09,040 --> 00:03:15,280
 And that includes at the core of chat GPT. Now we are not going to what I'd like to do now is

34
00:03:15,280 --> 00:03:20,000
 I'd like to build out something like chat GPT. But we're not going to be able to of course

35
00:03:20,000 --> 00:03:24,560
 reproduce chat GPT. This is a very serious production grade system. It is trained on

36
00:03:24,560 --> 00:03:31,520
 a good chunk of internet. And then there's a lot of pre-training and fine-tuning stages to it.

37
00:03:31,520 --> 00:03:37,360
 And so it's very complicated. What I'd like to focus on is just to train a transformer based

38
00:03:37,360 --> 00:03:42,880
 language model. And in our case it's going to be a character level language model. I still think

39
00:03:42,880 --> 00:03:47,600
 that is a very educational with respect to how these systems work. So I don't want to train on

40
00:03:47,600 --> 00:03:52,960
 the chunk of internet. We need a smaller data set. In this case I propose that we work with my

41
00:03:52,960 --> 00:03:58,720
 favorite toy data set. It's called tiny Shakespeare. And what it is is basically it's a concatenation

42
00:03:58,720 --> 00:04:04,080
 of all of the works of Shakespeare in my understanding. And so this is all of Shakespeare in a single

43
00:04:04,080 --> 00:04:10,640
 file. This file is about one megabyte. And it's just all of Shakespeare. And what we are going to do

44
00:04:10,640 --> 00:04:15,840
 now is we're going to basically model how these characters follow each other. So for example,

45
00:04:15,840 --> 00:04:22,320
 given a chunk of these characters like this, given some context of characters in the past,

46
00:04:22,320 --> 00:04:26,320
 the transformer neural network will look at the characters that I've highlighted and it's going

47
00:04:26,320 --> 00:04:31,040
 to predict that G is likely to come next in the sequence. And it's going to do that because we're

48
00:04:31,040 --> 00:04:36,960
 going to train that transformer on Shakespeare. And it's just going to try to produce character

49
00:04:36,960 --> 00:04:41,920
 sequences that look like this. And in that process is going to model all the patterns inside this

50
00:04:41,920 --> 00:04:48,720
 data. So once we've trained the system, I just like to give you a preview, we can generate infinite

51
00:04:48,720 --> 00:04:57,920
 Shakespeare. And of course it's a fake thing that looks kind of like Shakespeare. Apologies for

52
00:04:57,920 --> 00:05:05,440
 there's some jank that I'm not able to resolve in here. But you can see how this is going character

53
00:05:05,440 --> 00:05:11,520
 by character. And it's kind of like predicting Shakespeare-like language. So verily my lord,

54
00:05:11,520 --> 00:05:18,960
 the sights have left the again, the king coming with my curses with precious pale. And then

55
00:05:18,960 --> 00:05:23,840
 Tranios says something else, etc. And this is just coming out of the transformer in a very similar

56
00:05:23,840 --> 00:05:29,440
 manner, as it would come out in chat GPT. In our case, character by character in chat GPT,

57
00:05:30,560 --> 00:05:35,200
 it's coming out on the token by token level. And tokens are these sort of like little subword

58
00:05:35,200 --> 00:05:42,720
 pieces. So they're not word level, they're kind of like work chunk level. And now I've already

59
00:05:42,720 --> 00:05:49,600
 written this entire code to train these transformers. And it is in a GitHub repository that you can

60
00:05:49,600 --> 00:05:55,280
 find. And it's called nano GPT. So nano GPT is a repository that you can find on my GitHub.

61
00:05:55,840 --> 00:06:02,400
 And it's a repository for training transformers on any given text. And what I think is interesting

62
00:06:02,400 --> 00:06:06,560
 about it, because there's many ways to train transformers, but this is a very simple implementation.

63
00:06:06,560 --> 00:06:13,280
 So it's just two files of 300 lines of code, each one file defiles the GPT model, the transformer,

64
00:06:13,280 --> 00:06:18,160
 and one file trains it on some given text data set. And here I'm showing that if you train it on

65
00:06:18,160 --> 00:06:22,880
 the open WebText data set, which is a fairly large data set of web pages, then I reproduce the

66
00:06:23,600 --> 00:06:32,240
 the performance of GPT two. So GPT two is an early version of open AI's GPT from 2017,

67
00:06:32,240 --> 00:06:38,000
 if I recall correctly. And I've only so far reproduced the smallest one 24 million parameter model.

68
00:06:38,000 --> 00:06:42,720
 But basically, this is just proving that the code base is correctly arranged. And I'm able to load

69
00:06:42,720 --> 00:06:48,720
 the neural network weights that open AI has released later. So you can take a look at the

70
00:06:48,720 --> 00:06:53,600
 finished code here in an GPT, but what I would like to do in this lecture is I would like to

71
00:06:53,600 --> 00:06:59,120
 basically write this repository from scratch. So we're going to begin with an empty file,

72
00:06:59,120 --> 00:07:04,960
 and we're going to define a transformer piece by piece. We're going to train it on the tiny

73
00:07:04,960 --> 00:07:10,960
 Shakespeare data set. And we'll see how we can then generate infinite Shakespeare. And of course,

74
00:07:10,960 --> 00:07:16,160
 this can copy paste to any arbitrary text data set that you like. But my goal really here is to

75
00:07:16,160 --> 00:07:23,600
 just make you understand and appreciate how under the chat GPT works. And really, all that's required

76
00:07:23,600 --> 00:07:31,200
 is a proficiency in Python, and some basic understanding of copula statistics. And it would help if you

77
00:07:31,200 --> 00:07:36,720
 also see my previous videos on the same YouTube channel, in particular, my Make More series,

78
00:07:36,720 --> 00:07:44,480
 where I define smaller and simpler neural network language models. So multi-lil perceptions and so

79
00:07:44,480 --> 00:07:49,520
 on, it really introduces the language modeling framework. And then here in this video, we're

80
00:07:49,520 --> 00:07:54,640
 going to focus on the transformer neural network itself. Okay, so I created a new Google collab

81
00:07:54,640 --> 00:08:00,080
 Jupyter notebook here. And this will allow me to later easily share this code that we're going to

82
00:08:00,080 --> 00:08:04,720
 develop together with you so you can follow along. So this will be in a video description later.

83
00:08:04,720 --> 00:08:10,400
 Now here, I've just done some preliminarys. I downloaded the data set, the tiny Shakespeare

84
00:08:10,400 --> 00:08:15,920
 data set at this URL. And you can see that it's about a one megabyte file. Then here, I open the

85
00:08:15,920 --> 00:08:20,880
 input dot txt file and just read in all the text of the string. And we see that we are working with

86
00:08:20,880 --> 00:08:26,080
 one million characters roughly. And the first 1000 characters, if we just print them out,

87
00:08:26,080 --> 00:08:30,720
 are basically what you would expect. This is the first 1000 characters of the tiny Shakespeare

88
00:08:30,720 --> 00:08:38,160
 data set roughly up to here. So so far, so good. Next, we're going to take this text. And the text

89
00:08:38,160 --> 00:08:43,520
 is a sequence of characters in Python. So when I call the set constructor on it, I'm just going to

90
00:08:43,520 --> 00:08:51,040
 get the set of all the characters that occur in this text. And then I call list on that to create

91
00:08:51,040 --> 00:08:54,960
 a list of those characters instead of just a set so that I have an ordering an arbitrary

92
00:08:54,960 --> 00:09:00,640
 ordering. And then I sort that. So basically, we get just all the characters that occur in the

93
00:09:00,640 --> 00:09:05,760
 entire data set and they're sorted. Now the number of them is going to be our vocabulary size. These

94
00:09:05,760 --> 00:09:10,880
 are the possible elements of our sequences. And we see that when I print here, the characters,

95
00:09:10,880 --> 00:09:17,280
 there's 65 of them in total. There's a space character, and then all kinds of special characters.

96
00:09:17,280 --> 00:09:23,440
 And then capitals and lowercase letters. So that's our vocabulary. And that's the sort of like

97
00:09:23,440 --> 00:09:30,480
 possible characters that the model can see or emit. Okay, so next, we would like to develop some

98
00:09:30,480 --> 00:09:37,360
 strategy to tokenize the input text. Now, when people say tokenize, they mean convert the raw

99
00:09:37,360 --> 00:09:43,520
 text as a string to some sequence of integers, according to some note, according to some vocabulary

100
00:09:43,520 --> 00:09:48,640
 of possible elements. So as an example, here, we are going to be building a character level

101
00:09:48,640 --> 00:09:52,640
 language model. So we're simply going to be translating individual characters into integers.

102
00:09:52,640 --> 00:09:58,080
 So let me show you a chunk of code that sort of does that for us. So we're building both the

103
00:09:58,080 --> 00:10:04,320
 encoder and the decoder. And let me just talk through what's happening here. When we encode an

104
00:10:04,320 --> 00:10:10,720
 arbitrary text, like hi there, we're going to receive a list of integers that represents that

105
00:10:10,720 --> 00:10:17,760
 string. So for example, 46 47, etc. And then we also have the reverse mapping. So we can take

106
00:10:17,760 --> 00:10:23,840
 this list and decode it to get back the exact same string. So it's really just like a translation

107
00:10:23,840 --> 00:10:28,800
 to integers and back for arbitrary string. And for us, it is done on a character level.

108
00:10:28,800 --> 00:10:34,800
 Now the way this was achieved is we just iterate over all the characters here and create a lookup

109
00:10:34,800 --> 00:10:40,080
 table from the character to the integer and vice versa. And then to encode some string,

110
00:10:40,080 --> 00:10:45,600
 we simply translate all the characters individually and to decode it back, we use the reverse mapping

111
00:10:45,600 --> 00:10:51,440
 and concatenate all of it. Now this is only one of many possible encodings or many possible sort

112
00:10:51,440 --> 00:10:56,560
 of tokenizers. And it's a very simple one. But there's many other schemas that people have come up with

113
00:10:56,560 --> 00:11:03,280
 in practice. So for example, Google uses a sentence piece. So sentence piece will also encode text into

114
00:11:03,280 --> 00:11:10,720
 integers, but in a different schema, and using a different vocabulary. And sentence piece is a

115
00:11:10,720 --> 00:11:17,040
 sub word sort of tokenizer. And what that means is that you're not encoding entire words, but you're

116
00:11:17,040 --> 00:11:23,040
 not also encoding individual characters. It's it's a sub word unit level. And that's usually what's

117
00:11:23,040 --> 00:11:28,320
 adopted in practice. For example, also OpenAI has this library called tick token that uses a

118
00:11:28,320 --> 00:11:35,680
 byte pair encoding tokenizer. And that's what GPT uses. And you can also just encode words into

119
00:11:35,680 --> 00:11:41,360
 like hell world into lists of integers. So as an example, I'm using the tick token library here.

120
00:11:42,080 --> 00:11:47,600
 I'm getting the encoding from GPT two, or that was used for GPT two, instead of just having 65

121
00:11:47,600 --> 00:11:54,720
 possible characters or tokens, they have 50,000 tokens. And so when they encode the exact same

122
00:11:54,720 --> 00:12:00,560
 string, hi there, we only get a list of three integers. But those integers are not between zero

123
00:12:00,560 --> 00:12:10,400
 and 64, they are between zero and 5000 50,256. So basically, you can trade off the codebook size

124
00:12:10,400 --> 00:12:15,120
 and the sequence lengths. So you can have very long sequences of integers with very small

125
00:12:15,120 --> 00:12:23,600
 vocabularies, or we can have short sequences of integers with very large vocabularies. And so

126
00:12:23,600 --> 00:12:29,920
 typically people use in practice these sub word encodings. But I'd like to keep our tokenizer very

127
00:12:29,920 --> 00:12:34,880
 simple. So we're using character level tokenizer. And that means that we have very small codebooks.

128
00:12:34,880 --> 00:12:41,760
 We have very simple encode and decode functions. But we do get very long sequences as a result.

129
00:12:41,760 --> 00:12:45,200
 But that's the level at which we're going to stick with this lecture, because it's the simplest

130
00:12:45,200 --> 00:12:50,320
 thing. Okay, so now that we have an encoder and a decoder, effectively tokenizer, we can

131
00:12:50,320 --> 00:12:54,720
 tokenize the entire training set of Shakespeare. So here's a chunk of code that does that.

132
00:12:54,720 --> 00:12:59,920
 And I'm going to start to use the pytorch library, and specifically the tork.tensor from the pytorch

133
00:12:59,920 --> 00:13:05,840
 library. So we're going to take all of the text in tiny Shakespeare, encode it, and then wrap it

134
00:13:05,840 --> 00:13:11,360
 into a torch.tensor to get the data tensor. So here's what the data tensor looks like when I look at

135
00:13:11,360 --> 00:13:17,040
 just the first 1000 characters or the 1000 elements of it. So we see that we have a massive sequence

136
00:13:17,040 --> 00:13:22,480
 of integers. And this sequence of integers here is basically an identical translation of the first

137
00:13:22,480 --> 00:13:29,280
 1000 characters here. So I believe, for example, that zero is a new line character. And maybe one

138
00:13:29,280 --> 00:13:35,360
 is a space, not 100% sure. But from now on, the entire data set of text is rerepresented as just

139
00:13:35,360 --> 00:13:41,200
 it just stretched out is a single very large sequence of integers. Let me do one more thing

140
00:13:41,200 --> 00:13:46,400
 before we move on here. I'd like to separate out our data set into a train and a validation split.

141
00:13:46,400 --> 00:13:52,480
 So in particular, we're going to take the first 90% of the data set and consider that to be the

142
00:13:52,480 --> 00:13:57,440
 training data for the transformer. And we're going to withhold the last 10% at the end of it

143
00:13:57,440 --> 00:14:02,480
 to be the validation data. And this will help us understand to what extent our model is overfitting.

144
00:14:02,480 --> 00:14:06,880
 So we're going to basically hide and keep the validation data on the side, because we don't

145
00:14:06,880 --> 00:14:12,080
 want just a perfect memorization of this exact Shakespeare, we want a neural network that sort of

146
00:14:12,080 --> 00:14:19,040
 creates Shakespeare like text. And so it should be fairly likely for it to produce the actual like

147
00:14:19,040 --> 00:14:26,960
 stowed away, true Shakespeare text. And so we're going to use this to get a sense of the overfitting.

148
00:14:27,280 --> 00:14:32,000
 Okay, so now we would like to start plugging these text sequences or integer sequences into

149
00:14:32,000 --> 00:14:37,680
 the transformer so that it can train and learn those patterns. Now, the important thing to realize

150
00:14:37,680 --> 00:14:41,920
 is we're never going to actually feed entire text into your transformer all at once. That

151
00:14:41,920 --> 00:14:46,240
 would be computationally very expensive and prohibitive. So when we actually train a

152
00:14:46,240 --> 00:14:50,880
 transformer on a lot of these data sets, we only work with chunks of the data set. And when we train

153
00:14:50,880 --> 00:14:55,680
 the transformer, we basically sample random little chunks out of the training set and train on

154
00:14:55,680 --> 00:15:02,640
 just chunks at a time. And these chunks have basically some kind of a length and some maximum length.

155
00:15:02,640 --> 00:15:07,600
 Now the maximum length typically, at least in the code, I usually write, is called block size.

156
00:15:07,600 --> 00:15:13,280
 You can you can find it on the different names, like context length or something like that.

157
00:15:13,280 --> 00:15:17,840
 Let's start with the block size of just eight. And let me look at the first train data characters.

158
00:15:17,840 --> 00:15:22,320
 The first block size plus one characters. I'll explain why plus one in a second.

159
00:15:23,680 --> 00:15:28,240
 So this is the first nine characters in the sequence in the training set.

160
00:15:28,240 --> 00:15:32,720
 Now what I'd like to point out is that when you sample a chunk of data like this,

161
00:15:32,720 --> 00:15:37,520
 so say if these nine characters out of the training set, this actually has multiple

162
00:15:37,520 --> 00:15:42,720
 examples packed into it. And that's because all of these characters follow each other.

163
00:15:42,720 --> 00:15:49,440
 And so what this thing is going to say when we plug it into a transformer is we're going to

164
00:15:49,440 --> 00:15:53,120
 actually simultaneously train it to make prediction at every one of these positions.

165
00:15:53,680 --> 00:16:00,320
 Now in the in a chunk of nine characters, there's actually eight individual examples packed in there.

166
00:16:00,320 --> 00:16:07,920
 So there's the example that when 18 when in the context of 18, 47 luckily comes next.

167
00:16:07,920 --> 00:16:17,120
 In a context of 18 and 47 56 comes next. In a context of 18, 47, 56, 57 can come next and so on.

168
00:16:17,120 --> 00:16:21,120
 So that's the eight individual examples. Let me actually spell it out with code.

169
00:16:22,480 --> 00:16:27,040
 So here's a chunk of code to illustrate. X are the inputs to the transformer.

170
00:16:27,040 --> 00:16:34,400
 It will just be the first block size characters. Y will be the next block size characters. So it's

171
00:16:34,400 --> 00:16:40,400
 offset by one. And that's because Y are the targets for each position in the input.

172
00:16:40,400 --> 00:16:46,400
 And then here I'm iterating over all the block size of eight. And the context is always all

173
00:16:46,400 --> 00:16:53,360
 the characters in X up to T and including T. And the target is always the teeth character,

174
00:16:53,360 --> 00:17:00,560
 but in the targets array Y. So let me just run this. And basically it spells out what I said in words.

175
00:17:00,560 --> 00:17:08,640
 These are the eight examples hidden in a chunk of nine characters that we sampled from the training

176
00:17:08,640 --> 00:17:16,080
 set. I want to mention one more thing. We train on all the eight examples here with context between

177
00:17:16,080 --> 00:17:20,880
 one all the way up to context of block size. And we train on that not just for computational

178
00:17:20,880 --> 00:17:24,480
 reasons because we happen to have the sequence already or something like that. It's not just

179
00:17:24,480 --> 00:17:31,760
 done for efficiency. It's also done to make the transformer network be used to seeing contexts

180
00:17:31,760 --> 00:17:37,200
 all the way from as little as one all the way to block size. And we'd like the transformer to be

181
00:17:37,200 --> 00:17:41,920
 used to seeing everything in between. And that's going to be useful later during inference because

182
00:17:41,920 --> 00:17:47,520
 while we're sampling, we can start to set sampling generation with as level as one character of context.

183
00:17:47,520 --> 00:17:51,840
 And the transformer knows how to predict next character with all the way up to just context of

184
00:17:51,840 --> 00:17:56,880
 one. And so then it can predict everything up to block size. And after block size, we have to start

185
00:17:56,880 --> 00:18:02,480
 truncating because the transformer will never receive more than block size inputs when it's

186
00:18:02,480 --> 00:18:07,840
 predicting the next character. Okay, so we've looked at the time dimension of the tensors that

187
00:18:07,840 --> 00:18:11,360
 are going to be feeding into the transformer. There's one more dimension to care about and that is the

188
00:18:11,360 --> 00:18:17,280
 batch dimension. And so as we're sampling these chunks of text, we're going to be actually, every

189
00:18:17,280 --> 00:18:21,520
 time we're going to feed them into a transformer, we're going to have many batches of multiple chunks

190
00:18:21,520 --> 00:18:25,920
 of text that are always like stacked up in a single tensor. And that's just done for efficiency,

191
00:18:25,920 --> 00:18:32,880
 just so that we can keep the GPUs busy, because they are very good at parallel processing of data.

192
00:18:32,880 --> 00:18:37,680
 And so we just want to process multiple chunks all at the same time. But those chunks are

193
00:18:37,680 --> 00:18:42,160
 processed completely independently. They don't talk to each other, and so on. So let me basically

194
00:18:42,160 --> 00:18:48,160
 just generalize this and introduce a batch dimension. Here's a chunk of code. Let me just run it, and

195
00:18:48,160 --> 00:18:53,920
 then I'm going to explain what it does. So here, because we're going to start sampling random

196
00:18:53,920 --> 00:19:00,480
 locations in the data sets to pull chunks from, I am setting the seed so that in the random number

197
00:19:00,480 --> 00:19:04,080
 generator, so that the numbers I see here are going to be the same numbers you see later,

198
00:19:04,080 --> 00:19:08,960
 if you try to reproduce this. Now the batch size here is how many independent sequences

199
00:19:08,960 --> 00:19:14,880
 we are processing every forward, backward pass of the transformer. The block size, as I explained,

200
00:19:14,880 --> 00:19:20,800
 is the maximum context length to make those predictions. So let's say by size four, block size eight,

201
00:19:20,800 --> 00:19:26,480
 and then here's how we get batch for any arbitrary split. If the split is a training split, then we're

202
00:19:26,480 --> 00:19:33,360
 going to look at train data, otherwise, and val data. That gives us the data array. And then

203
00:19:33,360 --> 00:19:39,840
 when I generate random positions to grab a chunk out of, I actually grab, I actually generate batch

204
00:19:39,840 --> 00:19:47,680
 size number of random offsets. So because this is four, we are, I X is going to be a four numbers

205
00:19:47,680 --> 00:19:53,120
 that are randomly generated between zero and len of data minus block size. So it's just random

206
00:19:53,120 --> 00:20:00,320
 offsets into the training set. And then X is, as I explained, are the first block size characters

207
00:20:00,320 --> 00:20:09,200
 starting at I, the Y's are the offset by one of that. So just add plus one. And then we're going to

208
00:20:09,200 --> 00:20:16,320
 get those chunks for every one of integers I in I X, and use a torch dot stack to take all those

209
00:20:16,320 --> 00:20:25,600
 one dimensional tensors as we saw here. And we're going to stack them up at rows. And so they all

210
00:20:25,600 --> 00:20:32,720
 become a row in a four by eight tensor. So here's where I'm printing them. When I sample a batch

211
00:20:32,720 --> 00:20:41,360
 XB and YB, the inputs the transformer now are the input X is the four by eight tensor, four

212
00:20:41,360 --> 00:20:50,800
 rows of eight columns. And each one of these is a chunk of the training set. And then the targets

213
00:20:50,800 --> 00:20:55,920
 here are in the associated array Y, and they will come in to the transformer all the way at the end

214
00:20:55,920 --> 00:21:02,960
 to create the loss function. So they will give us the correct answer for every single position

215
00:21:02,960 --> 00:21:10,880
 inside X. And then these are the four independent rows. So spelled out as we did before,

216
00:21:10,880 --> 00:21:18,640
 this four by eight array contains a total of 32 examples. And they're completely independent

217
00:21:18,640 --> 00:21:26,000
 as far as the transformer is concerned. So when the input is 24, the target is 43,

218
00:21:26,000 --> 00:21:34,320
 or rather 43 here in the Y array, when the input is 2443, the target is 58. When the input is 2443

219
00:21:34,320 --> 00:21:40,480
 58, the target is five, etc. Or like when it is a 5258 one, the target is 58.

220
00:21:40,480 --> 00:21:46,640
 Right. So you can sort of see this spelled out. These are the 32 independent examples

221
00:21:46,640 --> 00:21:54,720
 packed in to a single batch of the input X. And then the desired targets are in Y. And so now this

222
00:21:54,720 --> 00:22:03,120
 integer tensor of X is going to feed into the transformer. And that transformer is going to

223
00:22:03,120 --> 00:22:08,640
 simultaneously process all these examples, and then look up the correct integers to predict in

224
00:22:08,640 --> 00:22:14,320
 every one of these positions in the tensor Y. Okay, so now that we have our batch of input that we'd

225
00:22:14,320 --> 00:22:18,240
 like to feed into a transformer, let's start basically feeding this into neural networks.

226
00:22:18,240 --> 00:22:22,560
 Now we're going to start off with the simplest possible neural network, which in the case of

227
00:22:22,560 --> 00:22:26,000
 language modeling, in my opinion, is the by Graham language model. And we've covered the

228
00:22:26,000 --> 00:22:30,960
 by Graham language model in my make more series in a lot of depth. And so here I'm going to

229
00:22:30,960 --> 00:22:35,520
 sort of go faster and let's just implement PyTorch module directly that implements the

230
00:22:35,520 --> 00:22:43,120
 by Graham language model. So I'm importing the pytorch and in module for reproducibility.

231
00:22:43,120 --> 00:22:47,680
 And then here I'm constructing a by Graham language model, which is a subclass of an end module.

232
00:22:47,680 --> 00:22:54,480
 And then I'm calling it and I'm passing in the inputs and the targets. And I'm just printing.

233
00:22:54,480 --> 00:22:58,800
 Now when the inputs and targets come here, you see that I'm just taking the index,

234
00:22:58,800 --> 00:23:05,120
 the inputs X here, which I renamed to ID X. And I'm just passing them into this token embedding table.

235
00:23:06,080 --> 00:23:11,200
 So what's going on here is that here in the constructor, we are creating a token embedding table.

236
00:23:11,200 --> 00:23:18,480
 And it is of size vocab size by vocab size. And we're using an end up embedding, which is a

237
00:23:18,480 --> 00:23:24,640
 very thin wrapper around basically a tensor of shape vocab size by vocab size. And what's happening

238
00:23:24,640 --> 00:23:30,640
 here is that when we pass ID X here, every single integer in our input is going to refer to this

239
00:23:30,640 --> 00:23:36,000
 embedding table, and is going to block out a row of that embedding table corresponding to its index.

240
00:23:36,000 --> 00:23:43,040
 So 24 here, we'll go to the embedding table and we'll pluck out the 24th row. And then 43 will go

241
00:23:43,040 --> 00:23:48,960
 here and block out the 43rd row, etc. And then pytorch is going to arrange all of this into a

242
00:23:48,960 --> 00:23:57,040
 batch by time by channel tensor. In this case, batch is four, time is eight, and C, which is the

243
00:23:57,760 --> 00:24:03,920
 channels is vocab size or 65. And so we're just going to block out all those rows, arrange them in a

244
00:24:03,920 --> 00:24:09,600
 B by T by C. And now we're going to interpret this as the logits, which are basically the scores

245
00:24:09,600 --> 00:24:15,200
 for the next character in a sequence. And so what's happening here is we are predicting what

246
00:24:15,200 --> 00:24:21,200
 comes next based on just the individual identity of a single token. And you can do that because,

247
00:24:21,200 --> 00:24:26,080
 I mean, currently, the tokens are not talking to each other. And they're not seeing any context,

248
00:24:26,080 --> 00:24:32,080
 except for they're just seeing themselves. So I'm a fat, I'm a token number five. And then I can

249
00:24:32,080 --> 00:24:36,560
 actually make pretty decent predictions about what comes next just by knowing that I'm token five,

250
00:24:36,560 --> 00:24:43,200
 because some characters know, sir, follow other characters in technical scenarios. So we saw a

251
00:24:43,200 --> 00:24:48,960
 lot of this in a lot more depth in the make more series. And here if I just run this, then we currently

252
00:24:48,960 --> 00:24:55,760
 get the predictions, the scores, the logits for every one of the four by eight positions. Now that

253
00:24:55,760 --> 00:24:59,920
 we've made predictions about what comes next, we'd like to evaluate the loss function. And so in

254
00:24:59,920 --> 00:25:05,280
 make more series, we saw that a good way to measure a loss or like a quality of the predictions

255
00:25:05,280 --> 00:25:10,080
 is to use the negative log likelihood loss, which is also implemented in PyTorch under the name

256
00:25:10,080 --> 00:25:17,440
 cross entropy. So what we'd like to do here is loss is the cross entropy on the predictions

257
00:25:17,440 --> 00:25:22,960
 and the targets. And so this measures the quality of the logits with respect to the targets. In other

258
00:25:22,960 --> 00:25:28,640
 words, we have the identity of the next character. So how well are we predicting the next character

259
00:25:28,640 --> 00:25:34,720
 based on the logits? And intuitively, the correct, the correct dimension of logits,

260
00:25:34,720 --> 00:25:39,440
 depending on whatever the target is, should have a very high number. And all the other

261
00:25:39,440 --> 00:25:44,880
 dimensions should be very low number. Right? Now the issue is that this won't actually,

262
00:25:44,880 --> 00:25:48,320
 this is what we want. We want to basically output the logits and the loss.

263
00:25:50,880 --> 00:25:57,200
 This is what we want. But unfortunately, this won't actually run. We get an error message.

264
00:25:57,200 --> 00:26:03,680
 But intuitively, we want to measure this. Now, when we go to the PyTorch cross entropy

265
00:26:03,680 --> 00:26:11,120
 documentation here, we're trying to call the cross entropy in its functional form. So that

266
00:26:11,120 --> 00:26:16,000
 means we don't have to create like a module for it. But here, we go to the documentation.

267
00:26:16,720 --> 00:26:21,200
 You have to look into the details of how PyTorch expects these inputs. And basically, the issue

268
00:26:21,200 --> 00:26:27,040
 here is PyTorch expects, if you have multi dimensional input, which we do, because we have a B by T

269
00:26:27,040 --> 00:26:35,040
 by C tensor, then it actually really wants the channels to be the second dimension here. So if

270
00:26:35,040 --> 00:26:44,000
 so basically, it wants a B by C by T, instead of a B by T by C. And so it's just the details of how

271
00:26:44,000 --> 00:26:51,120
 PyTorch treats these kinds of inputs. And so we don't actually want to deal with that. So

272
00:26:51,120 --> 00:26:54,880
 what we're going to do is that is we need to basically reshape our logits. So here's what I

273
00:26:54,880 --> 00:27:00,400
 like to do. I like to take basically give names to the dimensions. So logits that shape is B by T

274
00:27:00,400 --> 00:27:07,600
 by C and unpack those numbers. And then let's say that logits equals logits dot view. And we want

275
00:27:07,600 --> 00:27:14,720
 it to be a B times C, B times T by C. So just a two dimensional array. Right, so we're going to take

276
00:27:14,720 --> 00:27:21,520
 all the, we're going to take all of these positions here, and we're going to stretch them out in a

277
00:27:21,520 --> 00:27:26,560
 one dimensional sequence, and preserve the channel dimension as the second dimension.

278
00:27:26,560 --> 00:27:31,280
 So we're just kind of like stretching out the array. So it's two dimensional. And in that case,

279
00:27:31,280 --> 00:27:36,880
 it's going to better conform to what PyTorch sort of expects in its dimensions. Now we have to do

280
00:27:36,880 --> 00:27:45,760
 the same two targets, because currently targets are of shape B by T. And we want it to be just B

281
00:27:45,760 --> 00:27:51,200
 times T. So one dimensional. Now, alternatively, you could always still just do minus one,

282
00:27:51,200 --> 00:27:56,000
 because PyTorch will guess what this should be if you want to lay it out. But let me just be

283
00:27:56,000 --> 00:28:02,080
 explicit and say few times T. Once we reshape this, it will match the cross entropy case.

284
00:28:02,800 --> 00:28:08,640
 And then we should be able to evaluate our loss. Okay, so at that right now, and we can

285
00:28:08,640 --> 00:28:17,520
 do loss. And so currently we see that the loss is 4.87. Now, because our, we have 65 possible

286
00:28:17,520 --> 00:28:22,240
 vocabulary elements, we can actually guess at what the loss should be. And in particular,

287
00:28:22,240 --> 00:28:29,600
 we covered negative log likelihood in a lot of detail, we are expecting log or long of

288
00:28:30,800 --> 00:28:37,680
 one over 65, and negative of that. So we're expecting the loss to be about 4.17. But we're

289
00:28:37,680 --> 00:28:42,880
 getting 4.87. And so that's telling us that the initial predictions are not super diffuse.

290
00:28:42,880 --> 00:28:50,720
 They've got a little bit of entropy. And so we're guessing wrong. So yes, but actually we are

291
00:28:50,720 --> 00:28:56,960
 able to evaluate the loss. Okay, so now that we can evaluate the quality of the model on some data,

292
00:28:56,960 --> 00:29:01,520
 we'd like to also be able to generate from the model. So let's do the generation. Now I'm going

293
00:29:01,520 --> 00:29:05,280
 to go again a little bit faster here, because I covered all this already in previous videos.

294
00:29:05,280 --> 00:29:15,760
 So here's a generate function for the model. So we take some, we take the same kind of input

295
00:29:15,760 --> 00:29:24,800
 IDX here. And basically, this is the current context of some characters in a batch, in some batch.

296
00:29:25,680 --> 00:29:31,200
 So it's also B by T. And the job of generate is to basically take this B by T and extend it to

297
00:29:31,200 --> 00:29:36,000
 be B by T plus one plus two plus three. And so it's just basically, it continues the generation

298
00:29:36,000 --> 00:29:41,520
 in all the batch dimensions in the time dimension. So that's its job. And we'll do that for max

299
00:29:41,520 --> 00:29:46,160
 new tokens. So you can see here on the bottom, there's going to be some stuff here. But on the

300
00:29:46,160 --> 00:29:52,960
 bottom, whatever is predicted is concatenated on top of the previous IDX along the first dimension,

301
00:29:52,960 --> 00:29:58,640
 which is the time dimension to create a B by T plus one. So that becomes a new IDX. So the job

302
00:29:58,640 --> 00:30:04,240
 of generate is to take a B by T and make it a B by T plus one plus two plus three, as many as we

303
00:30:04,240 --> 00:30:09,760
 want maximum tokens. So this is the generation from the model. Now inside the generation, what

304
00:30:09,760 --> 00:30:16,160
 are we doing? We're taking the current indices, we're getting the predictions. So we get those

305
00:30:16,160 --> 00:30:21,520
 are in the logits. And then the loss here is going to be ignored, because we're not, we're not

306
00:30:21,520 --> 00:30:26,320
 using that. And we have no targets that are sort of ground truth targets that we're going to be

307
00:30:26,320 --> 00:30:33,280
 comparing with. Then once we get the logits, we are only focusing on the last step. So instead

308
00:30:33,280 --> 00:30:39,360
 of a B by T by C, we're going to pluck out the negative one, the last element in the time dimension,

309
00:30:39,360 --> 00:30:43,920
 because those are the predictions for what comes next. So that gives us the logits,

310
00:30:43,920 --> 00:30:49,280
 which we then cover to probabilities via softmax. And then we use towards that multinomial to sample

311
00:30:49,280 --> 00:30:56,240
 from those probabilities. And we ask PyTorch to give us one sample. And so idx next will become a B by

312
00:30:56,240 --> 00:31:01,680
 one, because in each one of the batch dimensions, we're going to have a single prediction for what

313
00:31:01,680 --> 00:31:08,000
 comes next. So this num samples equals one, we'll make this be a one. And then we're going to take

314
00:31:08,000 --> 00:31:12,400
 those integers that come from the sampling process, according to the probability distribution given

315
00:31:12,400 --> 00:31:17,840
 here. And those integers can just concatenate it on top of the current sort of like running stream

316
00:31:17,840 --> 00:31:25,120
 of integers. And this gives us a P by T plus one. And then we can return that. Now one thing here is,

317
00:31:25,120 --> 00:31:31,280
 you see how I'm calling self of idx, which will end up going to the forward function. I'm not

318
00:31:31,280 --> 00:31:37,200
 providing any targets. So currently this would give an error because targets is sort of like not

319
00:31:37,200 --> 00:31:44,320
 given. So targets has to be optional. So targets is none by default. And then if targets is none,

320
00:31:44,320 --> 00:31:50,640
 then there's no loss to create. So it's just losses, none. But else, all of this happens,

321
00:31:50,640 --> 00:31:57,200
 and we can create a loss. So this will make it so if we have the targets, we provide them and get

322
00:31:57,200 --> 00:32:03,360
 a loss, if we have no targets, we'll just get the logits. So this here will generate from the model.

323
00:32:03,360 --> 00:32:12,800
 And let's take that for a ride now. Oops. So I have another coach on here, which will generate

324
00:32:12,800 --> 00:32:18,160
 for the model from the model. And okay, this is kind of crazy. So maybe let me let me break this

325
00:32:18,160 --> 00:32:28,720
 down. So these are the idx, right? I'm creating a batch will be just one time will be just one.

326
00:32:28,720 --> 00:32:35,520
 So I'm creating a little one by one tensor, and it's holding a zero. And the D type, the data type

327
00:32:35,520 --> 00:32:41,920
 is integer. So zero is going to be how we kick off the generation. And remember that zero is

328
00:32:42,800 --> 00:32:47,600
 is the element standing for a new line character. So it's kind of like a reasonable thing to to

329
00:32:47,600 --> 00:32:54,160
 feed in as the very first character in a sequence to be the new line. So it's going to be idx,

330
00:32:54,160 --> 00:32:59,440
 which we're going to feed in here. Then we're going to ask for 100 tokens. And then end that

331
00:32:59,440 --> 00:33:07,280
 generate will continue that. Now, because generate works on the level of batches, we then have to

332
00:33:07,280 --> 00:33:15,920
 index into the zero throw to basically unplug the the single batch dimension that exists. And then

333
00:33:15,920 --> 00:33:23,360
 that gives us a time steps is just a one dimensional array of all the indices, which we will convert

334
00:33:23,360 --> 00:33:30,000
 to simple Python list from PyTorch tensor, so that that can feed into our decode function and

335
00:33:30,000 --> 00:33:37,120
 convert those integers into text. So let me bring this back. And we're generating 100 tokens

336
00:33:37,120 --> 00:33:43,840
 let's run. And here's the generation that we achieved. So obviously, as garbage, and the reason it's

337
00:33:43,840 --> 00:33:48,480
 garbage is because this is totally random model. So next up, we're going to want to train this model.

338
00:33:48,480 --> 00:33:54,720
 Now, one more thing I wanted to point out here is this function is written to be general, but it's

339
00:33:54,720 --> 00:34:00,880
 kind of like ridiculous right now because we're feeding in all this, we're building out this context,

340
00:34:00,880 --> 00:34:07,520
 and we're concatenating it all. And we're always feeding it all into the model. But that's kind of

341
00:34:07,520 --> 00:34:11,440
 ridiculous, because this is just a simple by-gram model. So to make, for example, this prediction

342
00:34:11,440 --> 00:34:16,640
 about K, we only needed this w. But actually, what we fed into the model is we fed the entire

343
00:34:16,640 --> 00:34:23,200
 sequence. And then we only looked at the very last piece and predicted K. So the only reason I'm

344
00:34:23,200 --> 00:34:27,920
 writing it in this way is because right now this is a by-gram model. But I'd like to keep this

345
00:34:27,920 --> 00:34:36,080
 function fixed. And I like it to work later when our characters actually basically look further in

346
00:34:36,080 --> 00:34:41,040
 the history. And so right now the history is not used. So this looks silly. But eventually,

347
00:34:41,040 --> 00:34:46,800
 the history will be used. And so that's why we want to do it this way. So just a quick comment on that.

348
00:34:46,800 --> 00:34:53,360
 So now we see that this is random. So let's train the model. So it becomes a bit less random.

349
00:34:53,360 --> 00:34:57,760
 Okay, let's now train the model. So first, what I'm going to do is I'm going to create a pytorch

350
00:34:57,840 --> 00:35:05,440
 optimization object. So here we are using the optimizer, Adam W. Now in a make more series,

351
00:35:05,440 --> 00:35:09,520
 we've only ever used the cast gradient descent, the simplest possible optimizer, which you can get

352
00:35:09,520 --> 00:35:14,960
 using the SGD instead. But I want to use Adam, which is a much more advanced and popular optimizer.

353
00:35:14,960 --> 00:35:20,480
 And it works extremely well for typical good setting for the learning rate is roughly three

354
00:35:20,480 --> 00:35:25,600
 in negative four. But for very, very small networks, like the case here, you can get away with much,

355
00:35:25,600 --> 00:35:30,560
 much higher learning rates, running negative three, or even higher, probably. But let me create the

356
00:35:30,560 --> 00:35:36,000
 optimizer object, which will basically take the gradients and update the parameters using the

357
00:35:36,000 --> 00:35:42,400
 gradients. And then here, our batch size up above was only four. So let me actually use something

358
00:35:42,400 --> 00:35:48,480
 bigger, let's say 32. And then for some number of steps, we are sampling a new batch of data.

359
00:35:48,480 --> 00:35:53,200
 We're evaluating the loss. We're zeroing out all the gradients from the previous step,

360
00:35:53,840 --> 00:35:57,920
 getting the gradients for all the parameters, and then using those gradients to update our

361
00:35:57,920 --> 00:36:04,400
 parameters. So typical training loop, as we saw in the make more series. So let me now run this

362
00:36:04,400 --> 00:36:08,400
 for say 100 iterations, and let's see what kind of losses we're going to get.

363
00:36:08,400 --> 00:36:18,320
 So we started around 4.7. And now we're going to down to like 4.6, 4.5, etc. So the optimization

364
00:36:18,320 --> 00:36:25,120
 is definitely happening. But let's sort of try to increase number of iterations and only print at

365
00:36:25,120 --> 00:36:32,320
 the end, because we probably will not train for a hundred. Okay, so we're down to 3.6 roughly.

366
00:36:32,320 --> 00:36:36,560
 Roughly down to three.

367
00:36:36,560 --> 00:36:43,040
 This is the most janky optimization.

368
00:36:47,200 --> 00:36:54,080
 Okay, it's working. Let's just do 10,000. And then from here, we want to copy this.

369
00:36:54,080 --> 00:36:58,720
 And hopefully we're going to get something reasonable. And of course, it's not going to be

370
00:36:58,720 --> 00:37:03,600
 Shakespeare from a background model. But at least we see that the loss is improving. And

371
00:37:03,600 --> 00:37:09,360
 hopefully we're expecting something a bit more reasonable. Okay, so we're down at about 2.5

372
00:37:09,360 --> 00:37:14,880
 ish. Let's see what we get. Okay, dramatic improvement certainly on what we had here.

373
00:37:14,880 --> 00:37:20,880
 So let me just increase the number of tokens. Okay, so we see that we're starting to get something

374
00:37:20,880 --> 00:37:30,640
 at least like reasonable ish. Certainly not Shakespeare, but the model is making progress.

375
00:37:30,640 --> 00:37:35,440
 So that is the simplest possible model. So now what I'd like to do is,

376
00:37:37,600 --> 00:37:41,600
 obviously, this is a very simple model because the tokens are not talking to each other.

377
00:37:41,600 --> 00:37:46,240
 So given the previous context of whatever was generated, we're only looking at the very last

378
00:37:46,240 --> 00:37:50,960
 character to make the predictions about what comes next. So now these, now these tokens have

379
00:37:50,960 --> 00:37:55,840
 to start talking to each other and figuring out what is in the context so that they can make

380
00:37:55,840 --> 00:38:00,400
 better predictions for what comes next. And this is how we're going to kick off the transformer.

381
00:38:00,400 --> 00:38:04,320
 Okay, so next, I took the code that we developed in this Jupyter notebook and I converted it to

382
00:38:04,320 --> 00:38:10,000
 be a script. And I'm doing this because I just want to simplify our intermediate work into just

383
00:38:10,000 --> 00:38:15,280
 the final product that we have at this point. So in the top here, I put all the

384
00:38:15,280 --> 00:38:19,440
 parameters that we defined. I introduced a few and I'm going to speak to that in a little bit.

385
00:38:19,440 --> 00:38:25,920
 Otherwise, a lot of this should be recognizable. Reproducibility, read data, get the encoder and

386
00:38:25,920 --> 00:38:33,520
 the decoder, create the train and test splits, use the kind of like data loader that gets a batch

387
00:38:33,520 --> 00:38:39,360
 of the inputs and targets. This is new and I'll talk about it in a second. Now this is the

388
00:38:39,360 --> 00:38:44,160
 background language model that we developed and it can forward and give us a logits and loss and

389
00:38:44,160 --> 00:38:49,840
 it can generate. And then here we are creating the optimizer and this is the training loop.

390
00:38:49,840 --> 00:38:56,080
 So everything here should look pretty familiar. Now some of the small things that I added,

391
00:38:56,080 --> 00:39:02,400
 number one, I added the ability to run on a GPU if you have it. So if you have a GPU, then you can,

392
00:39:02,400 --> 00:39:06,480
 this will use CUDA instead of just CPU and everything will be a lot more faster.

393
00:39:06,480 --> 00:39:12,320
 Now when device becomes CUDA, then we need to make sure that when we load the data, we move it to

394
00:39:12,320 --> 00:39:20,080
 device. When we create the model, we want to move the model parameters to device. So as an example,

395
00:39:20,080 --> 00:39:25,280
 here we have the in an embedding table and it's got a dot weight inside it, which stores the

396
00:39:25,280 --> 00:39:30,800
 sort of lookup table. So that would be moved to the GPU so that all the calculations here

397
00:39:30,800 --> 00:39:35,360
 happen on the GPU and they can be a lot faster. And then finally here when I'm creating the

398
00:39:35,360 --> 00:39:39,280
 context that feeds it to generate, I have to make sure that I create on the device.

399
00:39:39,280 --> 00:39:45,760
 Number two, what I introduced is the fact that here in the training loop,

400
00:39:45,760 --> 00:39:53,920
 here I was just printing the loss dot item inside the training loop. But this is a very noisy

401
00:39:53,920 --> 00:39:59,200
 measurement of the current loss because every batch will be more or less lucky. And so what I

402
00:39:59,200 --> 00:40:06,320
 want to do usually is I have an estimate loss function. And the estimate loss basically then

403
00:40:06,320 --> 00:40:14,960
 goes up here. And it averages up the loss over multiple batches. So in particular, we're going

404
00:40:14,960 --> 00:40:19,840
 to iterate eval iter times. And we're going to basically get our loss. And then we're going to

405
00:40:19,840 --> 00:40:25,840
 get the average loss for both splits. And so this will be a lot less noisy. So here, what we call

406
00:40:25,840 --> 00:40:30,560
 the estimate loss, we're going to report the pre accurate train and validation loss.

407
00:40:30,560 --> 00:40:36,480
 Now, when we come back up, you'll notice a few things here. I'm setting the model to a valuation

408
00:40:36,480 --> 00:40:42,080
 phase. And down here, I'm resetting it back to training phase. Now right now for our model,

409
00:40:42,080 --> 00:40:47,040
 as is, this doesn't actually do anything. Because the only thing inside this model is this, and then

410
00:40:47,040 --> 00:40:54,400
 dot embedding. And this, this network would behave both would behave the same in both

411
00:40:54,400 --> 00:40:59,520
 evaluation mode and training mode. We have no dropout layers, we have no bathroom layers, etc.

412
00:40:59,520 --> 00:41:05,280
 But it is a good practice to think through what mode your neural network is in, because some layers

413
00:41:05,280 --> 00:41:12,160
 will have different behavior at inference time or training time. And there's also this context

414
00:41:12,160 --> 00:41:16,320
 manager torched up no grad. And this is just telling PyTorch that everything that happens inside

415
00:41:16,320 --> 00:41:22,320
 this function, we will not call dot backward on. And so PyTorch can be a lot more efficient with

416
00:41:22,320 --> 00:41:27,040
 its memory use, because it doesn't have to store all the intermediate variables, because we're

417
00:41:27,040 --> 00:41:31,680
 never going to call backward. And so it can, it can be a lot more memory efficient in that way.

418
00:41:31,680 --> 00:41:36,400
 So also a good practice to tell PyTorch when we don't intend to do back propagation.

419
00:41:36,400 --> 00:41:44,320
 So right now, this script is about 120 lines of code of, and that's kind of our starter code.

420
00:41:44,320 --> 00:41:49,760
 I'm calling it by gram.py, and I'm going to release it later. Now running this script

421
00:41:50,960 --> 00:41:56,480
 gives us output in the terminal, and it looks something like this. It basically, as I ran this

422
00:41:56,480 --> 00:42:01,360
 code, it was giving me the train loss and val loss. And we see that we convert to somewhere around

423
00:42:01,360 --> 00:42:06,800
 2.5 with the by-gram model. And then here's the sample that we produced at the end.

424
00:42:06,800 --> 00:42:12,640
 And so we have everything packaged up in the script, and we're in a good position now to iterate on

425
00:42:12,640 --> 00:42:18,000
 this. Okay, so we are almost ready to start writing our very first self-attention block

426
00:42:18,000 --> 00:42:24,800
 for processing these tokens. Now, before we actually get there, I want to get you used to

427
00:42:24,800 --> 00:42:29,920
 a mathematical trick that is used in the self-attention inside a transformer. And it's really just like

428
00:42:29,920 --> 00:42:35,600
 at the heart of an efficient implementation of self-attention. And so I want to work with this

429
00:42:35,600 --> 00:42:40,000
 toy example to just get used to this operation. And then it's going to make it much more clear

430
00:42:40,000 --> 00:42:48,320
 once we actually get to it in the script again. So let's create a b by t by c, where b, t, and c

431
00:42:48,320 --> 00:42:54,880
 are just 48 and two in the story example. And these are basically channels, and we have batches,

432
00:42:54,880 --> 00:43:00,400
 and we have the time component, and we have some information at each point in the sequence. So c.

433
00:43:00,400 --> 00:43:07,360
 Now, what we would like to do is we would like these tokens. So we have up to eight tokens here

434
00:43:07,360 --> 00:43:11,680
 in a batch, and these eight tokens are currently not talking to each other, and we would like them

435
00:43:11,680 --> 00:43:18,080
 to talk to each other. We'd like to couple them. And in particular, we don't we we want to couple

436
00:43:18,080 --> 00:43:23,680
 them in a very specific way. So the token, for example, at the fifth location, it should not

437
00:43:23,680 --> 00:43:29,120
 communicate with tokens in the sixth, seventh, and eighth location, because those are future

438
00:43:29,120 --> 00:43:34,080
 tokens in the sequence. The token on the fifth location should only talk to the one in the fourth,

439
00:43:34,080 --> 00:43:40,160
 third, second, and first. So it's only so information only flows from previous context to the current

440
00:43:40,160 --> 00:43:44,240
 timestamp. And we cannot get any information from the future, because we are about to try to

441
00:43:44,240 --> 00:43:52,320
 predict the future. So what is the easiest way from tokens to communicate? Okay, the easiest way I

442
00:43:52,320 --> 00:43:57,760
 would say is, okay, if we're up to if we're a fifth token, and I'd like to communicate with my past,

443
00:43:57,760 --> 00:44:04,480
 the simplest way we can do that is to just do a way is to just do an average of all the of

444
00:44:04,480 --> 00:44:09,360
 all the preceding elements. So for example, if I'm the fifth token, I would like to take the channels

445
00:44:09,360 --> 00:44:15,920
 that make up that our information at my step, but then also the channels from the four step,

446
00:44:15,920 --> 00:44:20,320
 third step, second step, and the first step, I'd like to average those up. And then that would

447
00:44:20,320 --> 00:44:26,240
 become sort of like a feature vector that summarizes me in the context of my history. Now, of course,

448
00:44:26,240 --> 00:44:31,280
 just doing a sum or like an average is an extremely weak form of interaction, like this communication

449
00:44:31,280 --> 00:44:36,160
 is extremely lossy. We've lost a ton of information about spatial arrangements of all those tokens.

450
00:44:36,160 --> 00:44:41,280
 But that's okay for now. We'll see how we can bring that information back later. For now,

451
00:44:41,280 --> 00:44:47,360
 what we would like to do is for every single batch element independently, for every teeth token

452
00:44:47,360 --> 00:44:54,640
 in that sequence, we'd like to now calculate the average of all the vectors in all the previous

453
00:44:54,640 --> 00:45:01,760
 tokens and also at this token. So let's write that out. I have a small snippet here. And instead

454
00:45:01,760 --> 00:45:07,760
 of just fumbling around, let me just copy paste it and talk to it. So in other words, we're going

455
00:45:07,760 --> 00:45:16,720
 to create X and BOW is short for bag of words, because bag of words is kind of like a term that

456
00:45:16,720 --> 00:45:21,120
 people use when you are just averaging up things. So this is just a bag of words. Basically, there's

457
00:45:21,120 --> 00:45:26,080
 a word stored on every one of these eight locations, and we're doing a bag of words, which is averaging.

458
00:45:26,080 --> 00:45:31,120
 So in the beginning, we're going to say that it's just initialized at zero. And then I'm doing

459
00:45:31,120 --> 00:45:35,840
 a for loop here. So we're not being efficient yet. That's coming. But for now, we're just iterating

460
00:45:35,840 --> 00:45:42,640
 over all the batch dimensions independently, iterating over time. And then the previous tokens

461
00:45:42,640 --> 00:45:50,000
 are at this batch, a dimension, and then everything up to and including the teeth token. Okay.

462
00:45:50,880 --> 00:45:59,600
 So when we slice out X in this way, X prep becomes of shape, how many elements there were in the past,

463
00:45:59,600 --> 00:46:04,080
 and then of course, C. So all the two dimensional information from these little tokens.

464
00:46:04,080 --> 00:46:12,640
 So that's the previous sort of chunk of tokens from my current sequence. And then I'm just doing

465
00:46:12,640 --> 00:46:19,040
 the average or the mean over the zero dimension. So I'm averaging out the time here. And I'm just

466
00:46:19,040 --> 00:46:24,240
 going to get a little C one dimensional vector, which I'm going to store in X back of words.

467
00:46:24,240 --> 00:46:29,360
 So I can run this. And this is not going to be very informative, because

468
00:46:29,360 --> 00:46:36,400
 let's see. So this is X of zero. So this is the zero batch element. And then Expo at zero.

469
00:46:36,400 --> 00:46:44,480
 Now, you see how the at the first location here, you see that the two are equal. And that's because

470
00:46:44,480 --> 00:46:50,560
 it's we're just doing an average of this one token. But here, this one is now an average of these two.

471
00:46:50,560 --> 00:47:01,120
 And now this one is an average of these three. And so on. So and this last one is the average

472
00:47:01,120 --> 00:47:06,320
 of all of these elements. So vertical average, just averaging up all the tokens, now gives this

473
00:47:06,320 --> 00:47:13,200
 outcome here. So this is all well and good. But this is very inefficient. Now the trick is that we

474
00:47:13,200 --> 00:47:18,560
 can be very, very efficient about doing this using matrix multiplication. So that's the mathematical

475
00:47:18,560 --> 00:47:23,760
 trick. And let me show you what I mean. Let's work with the toy example here. Let me run it and I'll

476
00:47:23,760 --> 00:47:31,200
 explain. I have a simple matrix here that is three by three of all ones, a matrix B of just random

477
00:47:31,200 --> 00:47:36,640
 numbers. And it's a three by two, and a matrix C, which will be three by three multiply three by two,

478
00:47:36,640 --> 00:47:44,160
 which will give out a three by two. So here we're just using a matrix multiplication. So a multiply

479
00:47:44,160 --> 00:47:54,640
 B gives us C. Okay, so how are these numbers in C achieved? Right, so this number in the top left

480
00:47:54,640 --> 00:48:02,080
 is the first row of a dot product with the first column of B. And since all the row of A right now

481
00:48:02,080 --> 00:48:08,320
 is all just once, then the dot product here with with this column of B is just going to do a sum

482
00:48:08,320 --> 00:48:15,600
 of these of this column. So two plus six plus six is 14. The element here in the output of C is

483
00:48:15,600 --> 00:48:21,920
 also the first column here, the first row of a multiply now with the second column of B. So seven

484
00:48:21,920 --> 00:48:27,840
 plus four plus five is 16. Now you see that there's repeating elements here. So this 14 again is

485
00:48:27,840 --> 00:48:32,320
 because this row is again all once, and it's multiplying the first column of B. So you get 14.

486
00:48:32,320 --> 00:48:39,120
 And this one is, and so on. So this last number here is the last row dot product last column.

487
00:48:39,120 --> 00:48:47,280
 Now the trick here is the following. This is just a boring number of, it's just a boring

488
00:48:47,280 --> 00:48:53,600
 array of all ones. But torch has this function called trail, which is short for a triangular,

489
00:48:55,280 --> 00:49:00,240
 something like that. And you can wrap it in torch that once, and it will just return the lower triangular

490
00:49:00,240 --> 00:49:08,080
 portion of this. Okay. So now it will basically zero out these guys here. So we just get the

491
00:49:08,080 --> 00:49:17,600
 lower triangular part. Well, what happens if we do that? So now we'll have a like this and B like

492
00:49:17,600 --> 00:49:23,040
 this. And now what are we getting here and C? Well, what is this number? Well, this is the first row

493
00:49:23,680 --> 00:49:30,960
 times the first column. And because this is zeros, these elements here are now ignored. So we just

494
00:49:30,960 --> 00:49:37,040
 get it to. And then this number here is the first row times the second column. And because these

495
00:49:37,040 --> 00:49:43,040
 are zeros, they get ignored. And it's just seven. The seven multiplies this one. But look what happened

496
00:49:43,040 --> 00:49:48,240
 here, because this is one and then zeros, we what ended up happening is we're just plucking out the

497
00:49:48,240 --> 00:49:58,080
 row of this row of B. And that's what we got. Now here, we have 110. So here, 110 dot product with

498
00:49:58,080 --> 00:50:02,560
 these two columns will now give us two plus six, which is eight, and seven plus four, which is 11.

499
00:50:02,560 --> 00:50:09,520
 And because this is one one one, we ended up with the addition of all of them. And so basically,

500
00:50:09,520 --> 00:50:16,640
 depending on how many ones and zeros we have here, we are basically doing a sum currently of the

501
00:50:16,640 --> 00:50:22,880
 variable number of these rows, and that gets deposited into C. So currently, we're doing sums,

502
00:50:22,880 --> 00:50:28,080
 because these are ones, but we can also do average, right? And you can start to see how we could do

503
00:50:28,080 --> 00:50:35,520
 average of the rows of B, sort of incremental fashion, because we don't have to, we can basically

504
00:50:35,520 --> 00:50:41,040
 normalize these rows, so that they sum to one, and then we're gonna get an average. So if we took a,

505
00:50:41,600 --> 00:50:54,240
 and then we did a equals a divide, torch dot sum in the, of a, in the one dimension. And then

506
00:50:54,240 --> 00:50:59,680
 let's keep them as true. So therefore, the broadcasting will work out. So if I rerun this,

507
00:50:59,680 --> 00:51:07,200
 you see now that these rows now sum to one. So this row is one, this row is 0.5, 0.5, 0. And here

508
00:51:07,200 --> 00:51:13,200
 we get one thirds. And now when we do a multiply B, what are we getting? Here we are just getting

509
00:51:13,200 --> 00:51:18,880
 the first row, first row. Here now we are getting the average of the first two rows.

510
00:51:18,880 --> 00:51:26,880
 Okay, so two and six average is four, and four and seven averages five and five. And on the bottom

511
00:51:26,880 --> 00:51:33,920
 here, we're now getting the average of these three rows. So the average of all of elements of B are

512
00:51:33,920 --> 00:51:41,440
 now deposited here. And so you can see that by manipulating these elements of this multiplying

513
00:51:41,440 --> 00:51:47,920
 matrix, and then multiplying it with any given matrix, we can do these averages in this incremental

514
00:51:47,920 --> 00:51:55,040
 fashion, because we just get, and we can manipulate that based on the elements of A. Okay, so that's

515
00:51:55,040 --> 00:51:59,680
 very convenient. So let's swing back up here and see how we can vectorize this and make it much

516
00:51:59,680 --> 00:52:06,800
 more efficient using what we've learned. So in particular, we are going to produce an array A,

517
00:52:06,800 --> 00:52:14,080
 but here I'm going to call it way short for weights. But this is our A. And this is how much of every

518
00:52:14,080 --> 00:52:19,040
 row we want to average up, and it's going to be an average because you can see that these rows sum

519
00:52:19,040 --> 00:52:28,560
 to one. So this is our A, and then our B in this example, of course, is X. So what's going to happen

520
00:52:28,560 --> 00:52:36,640
 here now is that we are going to have an expo to. And this expo to is going to be way multiplying

521
00:52:36,640 --> 00:52:45,280
 our X. So let's think this through way is T by T. And this is matrix multiplying in pytorch,

522
00:52:45,280 --> 00:52:53,440
 a B by T by C. And it's giving us the what shape. So pytorch will come here and it will see that

523
00:52:53,440 --> 00:52:58,960
 these shapes are not the same. So it will create a batch dimension here. And this is a batch

524
00:52:58,960 --> 00:53:04,400
 matrix multiply. And so it will apply this matrix multiplication in all the batch elements

525
00:53:04,400 --> 00:53:11,280
 in parallel and individually. And then for each batch element, there will be a T by T multiplying

526
00:53:11,280 --> 00:53:23,360
 T by C exactly as we had below. So this will now create B by T by C. And X bow two will now

527
00:53:23,360 --> 00:53:34,480
 become identical to expo. So we can see that torch dot all close of expo and expo two should be true.

528
00:53:34,480 --> 00:53:44,880
 Now, so this kind of like commisses us that these are in fact the same. So expo and expo two,

529
00:53:44,880 --> 00:53:51,520
 if I just print them, okay, we're not going to be able to, okay, we're not going to be able to

530
00:53:51,520 --> 00:53:57,840
 just stare it down. But well, let me try expo basically just at the zero element and expo

531
00:53:57,840 --> 00:54:02,880
 two at the zero element. So just the first batch. And we should see that this, that should be

532
00:54:02,880 --> 00:54:09,520
 identical, which they are. Right. So what happened here, the trick is we were able to use batch

533
00:54:09,520 --> 00:54:17,440
 matrix multiply to do this aggregation really. And it's a weighted aggregation. And the weights

534
00:54:17,440 --> 00:54:25,200
 are specified in this T by T array. And we're basically doing weighted sums. And these weighted

535
00:54:25,200 --> 00:54:32,400
 sums are according to the weights inside here, the take on sort of this triangular form. And so

536
00:54:32,400 --> 00:54:38,240
 that means that a token at the T dimension will only get sort of information from the

537
00:54:38,240 --> 00:54:44,080
 tokens preceding it. So that's exactly what we want. And finally, I would like to rewrite it in one

538
00:54:44,080 --> 00:54:50,240
 more way. And we're going to see why that's useful. So this is the third version. And it's also

539
00:54:50,240 --> 00:54:57,760
 identical to the first and second. But let me talk through it, it uses softmax. So trill here

540
00:54:57,760 --> 00:55:07,840
 is this matrix, lower triangular ones. Way begins as all zero. Okay, so if I just print way in the

541
00:55:07,840 --> 00:55:17,440
 beginning, it's all zero. Then I use masked fill. So what this is doing is weight that masked fill,

542
00:55:17,440 --> 00:55:22,080
 it's all zeros. And I'm saying, for all the elements where trill is equal to equal zero,

543
00:55:22,080 --> 00:55:28,080
 make them be negative infinity. So all the elements where trill is zero will become

544
00:55:28,080 --> 00:55:34,800
 negative infinity. Now, so this is what we get. And then the final one here is softmax.

545
00:55:37,120 --> 00:55:41,920
 So if I take a softmax along every single, so dim is negative one, so long every single row,

546
00:55:41,920 --> 00:55:52,640
 if I do a softmax, what is that going to do? Well, softmax is, it's also like a normalization

547
00:55:52,640 --> 00:56:00,160
 operation, right? And so spoiler alert, you get the exact same matrix. Let me bring back the softmax.

548
00:56:00,160 --> 00:56:05,920
 And recall that in softmax, we're going to exponentiate every single one of these. And then

549
00:56:05,920 --> 00:56:11,520
 we're going to divide by the sum. And so if we exponentiate every single element here, we're going to get a

550
00:56:11,520 --> 00:56:17,520
 one. And here we're going to get basically zero, zero, zero, zero, everywhere else. And then when

551
00:56:17,520 --> 00:56:23,840
 we normalize, we just get one. Here we're going to get one one, and then zeros. And then softmax

552
00:56:23,840 --> 00:56:31,840
 will again divide, and this will give us 0.5 and so on. And so this is also the same way to produce

553
00:56:31,840 --> 00:56:36,400
 this mask. Now the reason that this is a bit more interesting, and the reason we're going to end

554
00:56:36,400 --> 00:56:44,800
 up using it in self attention, is that these weights here begin with zero. And you can think of this

555
00:56:44,800 --> 00:56:51,280
 as like an interaction strength, or like an affinity. So basically, it's telling us how much of each

556
00:56:51,280 --> 00:56:58,640
 token from the past do we want to aggregate an average up. And then this line is saying,

557
00:56:59,280 --> 00:57:04,640
 tokens from the past cannot communicate by setting them to negative infinity. We're saying that we

558
00:57:04,640 --> 00:57:10,320
 will not aggregate anything from those tokens. And so basically, this then goes through softmax,

559
00:57:10,320 --> 00:57:13,520
 and through the weighted, and this is the aggregation through matrix multiplication.

560
00:57:13,520 --> 00:57:21,440
 And so what this is now is you can think of these as these zeros are currently just set by us to be

561
00:57:21,440 --> 00:57:28,240
 zero. But a quick preview is that these affinities between the tokens are not going to be just constant

562
00:57:28,240 --> 00:57:33,600
 at zero. They're going to be data dependent. These tokens are going to start looking at each other.

563
00:57:33,600 --> 00:57:38,640
 And some tokens will find other tokens more or less interesting. And depending on what their

564
00:57:38,640 --> 00:57:43,200
 values are, they're going to find each other interesting to different amounts. And I'm going to call

565
00:57:43,200 --> 00:57:48,240
 those affinities, I think. And then here we are saying the future cannot communicate with the past.

566
00:57:48,240 --> 00:57:54,080
 We're going to clamp them. And then when we normalize and some, we're going to aggregate

567
00:57:54,720 --> 00:57:59,280
 sort of their values, depending on how interestingly they find each other. And so that's the preview

568
00:57:59,280 --> 00:58:06,000
 for self attention. And basically, long story short from this entire section is that you can do

569
00:58:06,000 --> 00:58:13,680
 weighted aggregations of your past elements by having by using matrix multiplication of a lower

570
00:58:13,680 --> 00:58:19,040
 triangular fashion. And then the elements here in the lower triangular part are telling you how

571
00:58:19,040 --> 00:58:25,040
 much of each element fuses into this position. So we're going to use this trick now to develop

572
00:58:25,040 --> 00:58:29,040
 the self attention block. So first, let's get some quick preliminaries out of the way.

573
00:58:29,040 --> 00:58:34,000
 First, the thing I'm kind of bothered by is that you see how we're passing them vocab size into

574
00:58:34,000 --> 00:58:38,800
 the constructor. There's no need to do that because vocab size is already defined up top as a global

575
00:58:38,800 --> 00:58:44,640
 variable. So there's no need to pass this stuff around. Next, what I want to do is I don't want

576
00:58:44,640 --> 00:58:49,120
 to actually create, I want to create like a level of interaction here, where we don't directly go to

577
00:58:49,120 --> 00:58:54,560
 the embedding for the logits. But instead, we go through this intermediate phase, because we're

578
00:58:54,560 --> 00:59:01,440
 going to start making that bigger. So let me introduce a new variable and embed a short for

579
00:59:01,440 --> 00:59:10,080
 number of embedding dimensions. So an embed here will be say 32. That was the suggestion from

580
00:59:10,080 --> 00:59:16,080
 GitHub Copiled by the way. It also suggests 32, which is a good number. So this is an embedding

581
00:59:16,080 --> 00:59:23,040
 table and only 32 dimensional embeddings. So then here, this is not going to give us logits directly.

582
00:59:23,040 --> 00:59:27,520
 Instead, this is going to give us token embeddings. That's what I'm going to call it. And then to go

583
00:59:27,520 --> 00:59:33,120
 from the token embeddings to the logits, we're going to need a linear layer. So self.lmhead,

584
00:59:33,120 --> 00:59:38,800
 let's call it short for language modeling head, is an linear from an embed up to vocab size.

585
00:59:39,760 --> 00:59:44,080
 And then when we swing over here, we're actually going to get the logits by exactly what the

586
00:59:44,080 --> 00:59:50,320
 copilot says. Now we have to be careful here, because this C and this C are not equal.

587
00:59:50,320 --> 00:59:57,360
 This is an embed C and this is vocab size. So let's just say that an embed is equal to C.

588
00:59:57,360 --> 01:00:04,480
 And then this just creates one spurious layer of interaction through a linear layer. But this

589
01:00:04,480 --> 01:00:16,320
 should basically run. So we see that this runs and this currently looks kind of spurious,

590
01:00:16,320 --> 01:00:21,520
 but we're going to build on top of this. Now next up, so far, we've taken these in in the

591
01:00:21,520 --> 01:00:28,640
 seas and we've encoded them based on the identity of the tokens inside ID X. The next thing that

592
01:00:28,640 --> 01:00:33,360
 people very often do is that we're not just encoding the identity of these tokens, but also

593
01:00:33,360 --> 01:00:39,440
 their position. So we're going to have a second position embedding table here. So self that position

594
01:00:39,440 --> 01:00:45,760
 embedding table is an embedding of block size by an embed. And so each position from zero to

595
01:00:45,760 --> 01:00:51,680
 block size minus one will also get its own embedding vector. And then here, first let me decode

596
01:00:51,680 --> 01:00:57,760
 b by t from ID X dot shape. And then here, we're also going to have a positive bedding,

597
01:00:57,760 --> 01:01:02,560
 which is the positional embedding. And these are this is tortoise arrange. So this will be

598
01:01:02,560 --> 01:01:07,760
 basically just integers from zero to t minus one. And all of those integers from zero to t

599
01:01:07,760 --> 01:01:14,640
 minus one get embedded through the table to create a T by C. And then here, this gets renamed to

600
01:01:14,640 --> 01:01:20,640
 just say X and X will be the addition of the token embedding with the positional embeddings.

601
01:01:20,640 --> 01:01:27,840
 And here the broadcasting note will work out. So B by T by C plus T by C, this gets right aligned

602
01:01:27,840 --> 01:01:33,600
 a new dimension of one gets added, and it gets broadcasted across batch. So at this point,

603
01:01:33,600 --> 01:01:38,640
 X holds not just the token identities, but the positions at which these tokens occur.

604
01:01:38,640 --> 01:01:42,560
 And this is currently not that useful, because of course, we just have a simple

605
01:01:42,560 --> 01:01:46,000
 binary model. So it doesn't matter if you're on the fifth position, the second position,

606
01:01:46,000 --> 01:01:50,720
 or wherever, it's all translation invariant at this stage. So this information currently

607
01:01:50,720 --> 01:01:56,000
 wouldn't help. But as we work on the self attention block, we'll see that this starts to matter.

608
01:01:56,240 --> 01:02:03,840
 Okay, so now we get the crux of self attention. So this is probably the most important part of

609
01:02:03,840 --> 01:02:09,280
 this video to understand. We're going to implement a small self attention for a single individual

610
01:02:09,280 --> 01:02:14,400
 head as they're called. So we start off with where we were. So all of this code is familiar.

611
01:02:14,400 --> 01:02:20,080
 So right now I'm working with an example where I change the number of channels from two to 32. So

612
01:02:20,080 --> 01:02:26,560
 we have a four by eight arrangement of tokens. And each token and the information that each token

613
01:02:26,560 --> 01:02:32,640
 is currently 32 dimensional, but we just are working with random numbers. Now we saw here that

614
01:02:32,640 --> 01:02:40,480
 the code as we had it before does a simple weight, simple average of all the past tokens

615
01:02:40,480 --> 01:02:45,280
 and the current token. So it's just the previous information and current information is just being

616
01:02:45,280 --> 01:02:50,000
 mixed together in an average. And that's what this code currently achieves. And it does so

617
01:02:50,000 --> 01:02:56,720
 by creating this lower triangular structure, which allows us to mask out this way matrix that we

618
01:02:56,720 --> 01:03:03,920
 create. So we mask it out and then we normalize it. And currently, when we initialize the

619
01:03:03,920 --> 01:03:09,120
 affinities between all the different sort of tokens or nodes, I'm going to use those terms

620
01:03:09,120 --> 01:03:14,240
 interchangeably. So when we initialize the affinities between all the different tokens to be zero,

621
01:03:15,040 --> 01:03:22,320
 then we see that way gives us this structure where every single row has these uniform numbers.

622
01:03:22,320 --> 01:03:27,840
 And so that's what that's what then in this matrix multiply makes it so that we're doing a

623
01:03:27,840 --> 01:03:35,680
 simple average. Now, we don't actually want this to be all uniform, because different

624
01:03:35,680 --> 01:03:40,800
 tokens will find different other tokens more or less interesting. And we want that to be data

625
01:03:40,800 --> 01:03:46,400
 dependent. So for example, if I'm a vowel, then maybe I'm looking for consonants in my past. And

626
01:03:46,400 --> 01:03:51,520
 maybe I want to know what those consonants are. And I want that information to flow to me. And so

627
01:03:51,520 --> 01:03:56,480
 I want to now gather information from the past. But I want to do it in a data dependent way. And

628
01:03:56,480 --> 01:04:01,440
 this is the problem that self attention solves. Now the way self attention solves this is the

629
01:04:01,440 --> 01:04:07,840
 following. Every single node or every single token at each position will emit two vectors.

630
01:04:08,560 --> 01:04:16,560
 It will emit a query and it will emit a key. Now the query vector, roughly speaking, is

631
01:04:16,560 --> 01:04:21,280
 what am I looking for? And the key vector, roughly speaking, is what do I contain?

632
01:04:21,280 --> 01:04:29,440
 And then the way we get affinities between these tokens now in a sequence is we basically just

633
01:04:29,440 --> 01:04:36,080
 do a dot product between the keys and the queries. So my query dot products with all the keys of

634
01:04:36,080 --> 01:04:45,920
 all the other tokens. And that dot product now becomes way. And so if the key and the query are

635
01:04:45,920 --> 01:04:51,440
 sort of aligned, they will interact to a very high amount. And then I will get to learn more

636
01:04:51,440 --> 01:04:57,120
 about that specific token, as opposed to any other token in the sequence. So let's implement this now.

637
01:05:01,520 --> 01:05:09,440
 We're going to implement a single what's called head of self attention. So this is just one head.

638
01:05:09,440 --> 01:05:13,920
 There's a hyper parameter involved with these heads, which is the head size. And then here I'm

639
01:05:13,920 --> 01:05:19,280
 initializing linear modules, and I'm using bias equals false. So these are just going to apply

640
01:05:19,280 --> 01:05:28,160
 matrix multiply with some fixed weights. And now let me produce a key and Q k and Q by forwarding

641
01:05:28,160 --> 01:05:37,280
 these modules on X. So the size of this will now become B by T by 16, because that is the head size.

642
01:05:37,280 --> 01:05:49,040
 And the same here B by T by 16. So this being that size. So you see here that when I forward

643
01:05:49,040 --> 01:05:55,200
 this linear on top of my X, all the tokens in all the positions in the B by T arrangement,

644
01:05:55,200 --> 01:06:00,880
 all of them in parallel and independently produce a key and a query. So no communication has happened

645
01:06:00,880 --> 01:06:06,960
 yet. But the communication comes now, all the queries will dart product with all the keys.

646
01:06:06,960 --> 01:06:15,440
 So basically what we want is we want way now or the affinities between these to be query multiplying

647
01:06:15,440 --> 01:06:19,680
 key. But we have to be careful with we can't make sure it's multiplied this we actually need to

648
01:06:19,680 --> 01:06:26,960
 transpose k, but we have to be also careful because these are when you have the batch dimension. So

649
01:06:26,960 --> 01:06:32,720
 in particular, we want to transpose the last two dimensions, dimension negative one and dimension

650
01:06:32,720 --> 01:06:40,800
 negative two. So negative two, negative one. And so this matrix multiply now will basically do the

651
01:06:40,800 --> 01:06:54,640
 following B by T by 16. Matrix multiplies B by 16 by T to give us B by T by T. Right.

652
01:06:54,640 --> 01:07:02,080
 So for every row of B, we're not going to have a T square matrix given us the affinities. And

653
01:07:02,080 --> 01:07:07,840
 these are now the way. So they're not zeros. They are now coming from this dart product between

654
01:07:07,840 --> 01:07:14,320
 the keys and the queries. So this can now run, I can I can run this. And the weighted aggregation

655
01:07:14,320 --> 01:07:20,400
 now is a function in a data band and manner between the keys and queries of these nodes. So

656
01:07:20,400 --> 01:07:28,800
 just inspecting what happened here, the way takes on this form. And you see that before way was

657
01:07:28,800 --> 01:07:33,760
 just a constant. So it was applied in the same way to all the batch elements. But now every single

658
01:07:33,760 --> 01:07:39,440
 batch elements will have different sort of way, because every single batch element contains different

659
01:07:39,440 --> 01:07:45,280
 tokens at different positions. And so this is not a data dependent. So when we look at just the

660
01:07:45,280 --> 01:07:51,600
 zero row, for example, in the input, these are the weights that came out. And so you can see now

661
01:07:51,600 --> 01:07:57,600
 that they're not just exactly uniform. And in particular, as an example here for the last row,

662
01:07:57,600 --> 01:08:02,480
 this was the eighth token. And the eighth token knows what content it has and it knows at what

663
01:08:02,480 --> 01:08:09,280
 position it's in. And now the eight token based on that creates a query. Hey, I'm looking for this

664
01:08:09,280 --> 01:08:14,960
 kind of stuff. I'm a vowel, I'm on the eight position, I'm looking for any consonants at positions up to

665
01:08:14,960 --> 01:08:21,680
 four. And then all the nodes get to emit keys. And maybe one of the channels could be I am a

666
01:08:21,680 --> 01:08:27,920
 I am a consonant and I am in the position up to four. And that key would have a high number in that

667
01:08:27,920 --> 01:08:32,480
 specific channel. And that's how the query and the key when they dark product, they can find each other

668
01:08:32,480 --> 01:08:38,240
 and create a high affinity. And when they have a high affinity, like say, this token was pretty

669
01:08:38,240 --> 01:08:45,040
 interesting to to this eighth token. When they have a high affinity, then through the softmax,

670
01:08:45,040 --> 01:08:50,560
 I will end up aggregating a lot of its information into my position. And so I'll get to learn a lot

671
01:08:50,560 --> 01:09:00,000
 about it. Now, just this we're looking at way after this has already happened. Let me erase this

672
01:09:00,000 --> 01:09:04,480
 operation as well. So let me erase the masking and the softmax just to show you the under the hood

673
01:09:04,480 --> 01:09:10,480
 internals and how that works. So without the masking and the softmax way comes out like this,

674
01:09:10,480 --> 01:09:15,680
 right? This is the outputs of the dot products. And these are the raw outputs and they take on values

675
01:09:15,680 --> 01:09:22,000
 from negative, you know, two to positive two, etc. So that's the raw interactions and raw

676
01:09:22,000 --> 01:09:27,760
 affinities between all the nodes. But now if I'm a, if I'm a fifth node, I will not want to

677
01:09:27,760 --> 01:09:32,480
 aggregate anything from the sixth node seventh node and the eighth node. So actually, we use the

678
01:09:32,480 --> 01:09:40,160
 upper triangular masking. So those are not allowed to communicate. And now we actually want to have

679
01:09:40,160 --> 01:09:46,480
 a nice distribution. So we don't want to aggregate negative point one one of this node that's crazy.

680
01:09:46,480 --> 01:09:50,720
 So instead we exponentiate and normalize, and now we get a nice distribution that seems to want.

681
01:09:50,720 --> 01:09:55,600
 And this is telling us now in the data, depending on manner, how much of information to aggregate

682
01:09:55,600 --> 01:10:03,760
 from any of these tokens in the past. So that's way, and it's not zeros anymore, but it's calculated

683
01:10:03,760 --> 01:10:10,880
 in this way. Now there's one more part to a single self attention head. And that is that when we

684
01:10:10,880 --> 01:10:16,240
 do the aggregation, we don't actually aggregate the tokens exactly. We aggregate, we produce one

685
01:10:16,240 --> 01:10:23,040
 more value here. And we call that the value. So in the same way that we produce t and query,

686
01:10:23,040 --> 01:10:33,760
 we're also going to create a value. And then here, we don't aggregate x, we calculate a v,

687
01:10:33,760 --> 01:10:41,200
 which is just achieved by propagating this linear on top of x again. And then we output

688
01:10:41,200 --> 01:10:47,360
 way multiplied by v. So v is the elements that we aggregate, or the vector that we aggregate,

689
01:10:47,360 --> 01:10:53,280
 instead of the raw x. And now of course, this will make it so that the output here of this single

690
01:10:53,280 --> 01:10:59,920
 head will be 16 dimensional, because that is the head size. So you can think of x as kind of like

691
01:10:59,920 --> 01:11:05,200
 private information to this token, if you think about it that way. So x is kind of private to this

692
01:11:05,200 --> 01:11:12,080
 token. So I'm a fifth token at some, and I have some identity. And my information is kept in vector

693
01:11:12,080 --> 01:11:18,640
 x. And now for the purposes of the single head, here's what I'm interested in. Here's what I have.

694
01:11:18,640 --> 01:11:24,240
 And if you find me interesting, here's what I will communicate to you. And that's stored in v.

695
01:11:24,240 --> 01:11:29,920
 And so v is the thing that gets aggregated for the purposes of this single head between the

696
01:11:29,920 --> 01:11:37,120
 different nodes. And that's basically the self attention mechanism. This is, this is what it does.

697
01:11:38,000 --> 01:11:43,600
 There are a few notes that I would like to make about attention. Number one, attention is a

698
01:11:43,600 --> 01:11:48,080
 communication mechanism. You can really think about it as a communication mechanism, where you

699
01:11:48,080 --> 01:11:52,960
 have a number of nodes in a directed graph, where basically you have edges pointed between nodes

700
01:11:52,960 --> 01:11:59,520
 like this. And what happens is every node has some vector of information, and it gets to aggregate

701
01:11:59,520 --> 01:12:05,760
 information via a weighted sum from all the nodes that point to it. And this is done in a data

702
01:12:05,760 --> 01:12:10,080
 dependent manner. So depending on whatever data is actually stored at each node at any point in time.

703
01:12:10,080 --> 01:12:16,880
 Now, our graph doesn't look like this. Our graph has a different structure. We have eight nodes,

704
01:12:16,880 --> 01:12:23,040
 because the block size is eight, and there's always eight tokens. And the first node is only

705
01:12:23,040 --> 01:12:27,760
 pointed to by itself. The second node is pointed to by the first node and itself,

706
01:12:27,760 --> 01:12:32,720
 all the way up to the eighth node, which is pointed to by all the previous nodes and itself.

707
01:12:33,680 --> 01:12:38,720
 And so that's the structure that our directed graph has, or happens to have, in an autoregressive

708
01:12:38,720 --> 01:12:42,960
 sort of scenario like language modeling. But in principle, attention can be applied to any

709
01:12:42,960 --> 01:12:46,400
 arbitrary directed graph, and it's just a communication mechanism between the nodes.

710
01:12:46,400 --> 01:12:52,800
 The second note is that, notice that there's no notion of space. So attention simply acts over

711
01:12:52,800 --> 01:12:57,840
 like a set of vectors in this graph. And so by default, these nodes have no idea where they

712
01:12:57,840 --> 01:13:02,640
 are positioned in the space. And that's why we need to encode them positionally and sort of give

713
01:13:02,640 --> 01:13:07,840
 them some information that is anchored to a specific position so that they sort of know where they

714
01:13:07,840 --> 01:13:12,080
 are. And this is different than, for example, from convolution, because if you run, for example,

715
01:13:12,080 --> 01:13:17,520
 a convolution operation over some input, there is a very specific sort of layout of the information

716
01:13:17,520 --> 01:13:24,720
 in space and the convolutional filters sort of act in space. And so it's not like an attention.

717
01:13:24,720 --> 01:13:29,440
 An attention is just a set of vectors out there in space. They communicate. And if you want them

718
01:13:29,440 --> 01:13:33,840
 to have a notion of space, you need to specifically add it, which is what we've done when we

719
01:13:33,840 --> 01:13:40,160
 calculated the relative the positional encodings and added that information to the vectors.

720
01:13:40,160 --> 01:13:44,400
 The next thing that I hope is very clear is that the elements across the batch dimension,

721
01:13:44,400 --> 01:13:48,480
 which are independent examples, never talk to each other. They're always processed independently.

722
01:13:48,480 --> 01:13:52,880
 And this is a bashed matrix multiply that applies basically a matrix multiplication,

723
01:13:52,880 --> 01:13:57,280
 kind of imperil across the batch dimension. So maybe it would be more accurate to say that

724
01:13:57,280 --> 01:14:02,960
 in this analogy of a directed graph, we really have, because the batch size is four, we really have

725
01:14:02,960 --> 01:14:07,760
 four separate pools of eight nodes, and those eight nodes only talk to each other. But in total,

726
01:14:07,760 --> 01:14:13,520
 there's like 32 nodes that are being processed. But there's sort of four separate pools of eight,

727
01:14:13,520 --> 01:14:18,640
 you can look at it that way. The next note is that here in the case of language modeling,

728
01:14:18,640 --> 01:14:25,040
 we have this specific structure of directed graph where the future tokens will not communicate

729
01:14:25,040 --> 01:14:30,240
 to the past tokens. But this doesn't necessarily have to be the constraint in the general case.

730
01:14:30,240 --> 01:14:36,720
 And in fact, in many cases, you may want to have all of the nodes talk to each other fully. So as

731
01:14:36,720 --> 01:14:40,480
 an example, if you're doing sentiment analysis or something like that with a transformer, you might

732
01:14:40,480 --> 01:14:45,760
 have a number of tokens, and you may want to have them all talk to each other fully, because later,

733
01:14:45,760 --> 01:14:50,080
 you are predicting, for example, the sentiment of the sentence. And so it's okay for these

734
01:14:50,080 --> 01:14:55,920
 nodes to talk to each other. And so in those cases, you will use an encoder block of self attention.

735
01:14:55,920 --> 01:15:01,920
 And all it means that it's an encoder block is that you will delete this line of code,

736
01:15:01,920 --> 01:15:05,760
 allowing all the nodes to completely talk to each other. What we're implementing here is

737
01:15:05,760 --> 01:15:11,520
 sometimes called a decoder block. And it's called a decoder, because it is sort of like

738
01:15:11,520 --> 01:15:18,480
 decoding language. And it's got this autoregressive format, where you have to mask with the triangle

739
01:15:18,480 --> 01:15:24,240
 and matrix, so that nodes from the future never talk to the past, because they would give away

740
01:15:24,240 --> 01:15:29,840
 the answer. And so basically, an encoder blocks, you would delete this, allow all the nodes to talk.

741
01:15:29,840 --> 01:15:34,080
 In decoder blocks, this will always be present, so that you have this triangular structure.

742
01:15:34,080 --> 01:15:38,640
 But both are allowed and attention doesn't care. Attention supports arbitrary connectivity

743
01:15:38,640 --> 01:15:43,360
 between nodes. The next thing I wanted to comment on is you keep me, you keep hearing me say

744
01:15:43,360 --> 01:15:47,120
 attention, self attention, etc. There's actually also something called cross attention. What is

745
01:15:47,120 --> 01:15:56,080
 the difference? So basically, the reason this attention is self attention, is because the keys,

746
01:15:56,080 --> 01:16:02,720
 queries, and the values are all coming from the same source from X. So the same source X produces

747
01:16:02,720 --> 01:16:08,400
 keys, queries, and values. So these nodes are self attending. But in principle, attention is

748
01:16:08,400 --> 01:16:13,840
 much more general than that. So for example, an encoder decoder transformers, you can have a case

749
01:16:13,840 --> 01:16:18,800
 where the queries are produced from X. But the keys and the values come from a whole separate

750
01:16:18,800 --> 01:16:24,240
 external source, and sometimes from encoder blocks that encode some context that we'd like to

751
01:16:24,240 --> 01:16:28,880
 condition on. And so the keys and the values will actually come from a whole separate source.

752
01:16:28,880 --> 01:16:33,440
 Those are nodes on the side. And here we're just producing queries. And we're reading off

753
01:16:33,440 --> 01:16:40,320
 information from the side. So cross attention is used when there's a separate source of nodes,

754
01:16:40,320 --> 01:16:45,680
 we'd like to pull information from into our nodes. And it's self attention if we just have nodes

755
01:16:45,680 --> 01:16:50,480
 that would like to look at each other and talk to each other. So this attention here happens to be

756
01:16:50,480 --> 01:16:57,920
 self attention. But in principle, attention is a lot more general. Okay, and the last note at this

757
01:16:57,920 --> 01:17:02,960
 stage is if we come to the attention is only need paper here, we've already implemented attention.

758
01:17:02,960 --> 01:17:09,440
 So given query key and value, we've multiplied the query on a key, we've soft maxed it, and then

759
01:17:09,440 --> 01:17:13,680
 we are aggregating the values. There's one more thing that we're missing here, which is the dividing

760
01:17:13,680 --> 01:17:19,040
 by one over square root of the head size, the decay here is the head size. Why aren't they doing

761
01:17:19,040 --> 01:17:25,200
 this one is important. So they call it a scaled attention. And it's kind of like an important

762
01:17:25,200 --> 01:17:31,120
 normalization to basically have the problem is if you have unit Gaussian inputs, so zero mean unit

763
01:17:31,120 --> 01:17:36,880
 variance, k and q are unit Gaussian. And if you just do way naively, then you see that your way

764
01:17:36,880 --> 01:17:41,200
 actually will be the variance will be on the order of head size, which in our case is 16.

765
01:17:41,200 --> 01:17:47,040
 But if you multiply by one over head size square root, so this is square root, and this is one over,

766
01:17:47,040 --> 01:17:54,640
 then the variance of way will be one. So we'll be preserved. Now, why is this important? You'll

767
01:17:54,640 --> 01:18:01,200
 notice that way here will feed into softmax. And so it's really important, especially at

768
01:18:01,200 --> 01:18:07,760
 initialization, that way be fairly diffuse. So in our case here, we sort of locked out here,

769
01:18:07,760 --> 01:18:15,760
 and way at a fairly diffuse numbers here. So like this. Now the problem is that because of

770
01:18:15,760 --> 01:18:20,800
 softmax, if weight takes on very positive and very negative numbers inside it, softmax will

771
01:18:20,800 --> 01:18:28,560
 actually converge towards one hot vectors. And so I can illustrate that here. Say we are

772
01:18:28,560 --> 01:18:32,960
 applying softmax to a tensor of values that are very close to zero, then we're going to get a

773
01:18:32,960 --> 01:18:38,480
 diffuse thing out of softmax. But the moment I take the exact same thing and I start sharpening it,

774
01:18:38,480 --> 01:18:42,720
 making it bigger by multiplying these numbers by eight, for example, you'll see that the softmax

775
01:18:42,720 --> 01:18:47,760
 will start to sharpen. And in fact, it will sharpen towards the max. So it will sharpen towards

776
01:18:47,760 --> 01:18:52,880
 whatever number here is the highest. And so basically, we don't want these values to be too

777
01:18:52,880 --> 01:18:58,400
 extreme, especially at initialization. Otherwise, softmax will be way too peaky. And you're basically

778
01:18:58,400 --> 01:19:03,600
 aggregating information from like a single node, every node just aggregates information from a

779
01:19:03,600 --> 01:19:09,120
 single other node. That's not what we want, especially at initialization. And so the scaling is used

780
01:19:09,120 --> 01:19:14,400
 just to control the variance at initialization. Okay, so having said all that, let's now take our

781
01:19:14,400 --> 01:19:20,240
 self attention knowledge and let's take it for a spin. So here in the code, I created this head

782
01:19:20,240 --> 01:19:26,320
 module and implements a single head of self attention. So you give it a head size. And then here it

783
01:19:26,320 --> 01:19:30,960
 creates the key query and the value linear layers, typically people who use biases in these.

784
01:19:30,960 --> 01:19:35,360
 So those are the linear projections that we're going to apply to all of our nodes.

785
01:19:35,360 --> 01:19:41,760
 Now here, I'm creating this trill variable. Trill is not a parameter of the module. So in sort of

786
01:19:41,760 --> 01:19:46,880
 pytorch naming conventions, this is called a buffer. It's not a parameter. And you have to call it,

787
01:19:46,880 --> 01:19:50,640
 you have to assign it to the module using a register buffer. So that creates the trill,

788
01:19:51,600 --> 01:19:57,200
 the trying lower triangular matrix. And over given the input X, this should look very familiar now.

789
01:19:57,200 --> 01:20:03,520
 We calculate the keys, the queries, we calling pocket the attention scores in sideways. We normalize

790
01:20:03,520 --> 01:20:08,720
 it. So we're using scaled attention here. Then we make sure that sure doesn't communicate with the

791
01:20:08,720 --> 01:20:14,880
 past. So this makes it a decoder block. And then softmax and then aggregate the value and output.

792
01:20:14,880 --> 01:20:20,880
 Then here in the language model, I'm creating a head in the constructor and I'm calling it

793
01:20:20,880 --> 01:20:26,800
 self attention head. And the head size, I'm going to keep as the same and embed just for now.

794
01:20:26,800 --> 01:20:34,240
 And then here, once we've encoded the information with the token embeddings and the position embeddings,

795
01:20:34,240 --> 01:20:39,040
 we're simply going to feed it into the self attention head. And then the output of that is going to go

796
01:20:39,040 --> 01:20:46,320
 into the decoder language modeling head and create the logits. So this is sort of the simplest way

797
01:20:46,320 --> 01:20:52,720
 to plug in a self attention component into our network right now. I had to make one more change,

798
01:20:52,720 --> 01:21:00,240
 which is that here in the generate, we have to make sure that our ID X that we feed into the model.

799
01:21:00,240 --> 01:21:06,080
 Because now we're using positional embeddings, we can never have more than block size coming in,

800
01:21:06,080 --> 01:21:11,040
 because if ID X is more than block size, then our position embedding table is going to run out of

801
01:21:11,040 --> 01:21:16,080
 scope, because it only has embeddings for up to block size. And so therefore, I added some code

802
01:21:16,080 --> 01:21:23,520
 here to crop the context that we're going to feed into self, so that we never pass in more

803
01:21:23,520 --> 01:21:28,720
 block size elements. So those are the changes. And let's now train the network. Okay, so I also

804
01:21:28,720 --> 01:21:33,040
 came up to the script here, and I decreased the learning rate, because the self attention

805
01:21:33,040 --> 01:21:37,600
 can't tolerate very, very high learning rates. And then I also increased number of iterations

806
01:21:37,600 --> 01:21:41,600
 because the learning rate is lower. And then I trained it. And previously, we were only able to

807
01:21:41,600 --> 01:21:47,360
 get to up to 2.5. And now we are down to 2.4. So we definitely see a little bit of improvement

808
01:21:47,360 --> 01:21:53,760
 from 2.5 to 2.4 roughly. But the text is still not amazing. So clearly, the self attention had

809
01:21:53,760 --> 01:22:00,320
 is doing some useful communication. But we still have a long way to go. Okay, so now we've implemented

810
01:22:00,320 --> 01:22:05,120
 the scale dot product attention. Now next up, and the attention is all you need paper. There's

811
01:22:05,120 --> 01:22:09,760
 something called multi head attention. And what is multi head attention? It's just applying

812
01:22:09,760 --> 01:22:15,440
 multiple attentions in parallel and concatenating the results. So they have a little bit of diagram

813
01:22:15,440 --> 01:22:20,800
 here. I don't know if this is super clear. It's really just multiple attentions in parallel.

814
01:22:20,800 --> 01:22:27,600
 So let's implement that fairly straightforward. If we want a multi head attention, then we want

815
01:22:27,600 --> 01:22:33,280
 multiple heads of self attention running in parallel. So in PyTorch, we can do this by simply

816
01:22:33,280 --> 01:22:40,160
 creating multiple heads. So however, how many heads you want, and then what is the head size

817
01:22:40,160 --> 01:22:47,200
 of each? And then we run all of them in parallel into a list, and simply concatenate all of the

818
01:22:47,200 --> 01:22:53,520
 outputs. And we're concatenating over the channel dimension. So the way this looks now is we don't

819
01:22:53,520 --> 01:23:00,480
 have just a single attention that has a head size of 32, because remember, an embed is 32.

820
01:23:01,520 --> 01:23:08,080
 Instead of having one communication channel, we now have four communication channels in parallel.

821
01:23:08,080 --> 01:23:14,480
 And each one of these communication channels typically will be smaller correspondingly.

822
01:23:14,480 --> 01:23:19,440
 So because we have four communication channels, we want eight dimensional self attention. And so

823
01:23:19,440 --> 01:23:23,680
 from each communication channel, we're getting together eight dimensional vectors. And then we

824
01:23:23,680 --> 01:23:28,160
 have four of them. And that concatenates to give us 32, which is the original, an embed.

825
01:23:29,040 --> 01:23:33,120
 And so this is kind of similar to if you're familiar with convolutions, this is kind of like a group

826
01:23:33,120 --> 01:23:38,400
 convolution, because basically, instead of having one large convolution, we do convolution in groups.

827
01:23:38,400 --> 01:23:46,000
 And that's multi headed self attention. And so then here, we just use SA heads, self attention

828
01:23:46,000 --> 01:23:53,840
 heads instead. Now I actually ran it and scrolling down. I ran the same thing, and then we now get

829
01:23:53,840 --> 01:24:00,320
 it down to 2.28 roughly. And the upper distal, the generation is still not amazing. But clearly,

830
01:24:00,320 --> 01:24:06,240
 the validation loss is improving, because we were at 2.4 just now. And so it helps to have multiple

831
01:24:06,240 --> 01:24:11,120
 communication channels, because obviously these tokens have a lot to talk about. They want to

832
01:24:11,120 --> 01:24:14,800
 find the constants, the vowels, they want to find the vowels just from certain positions.

833
01:24:14,800 --> 01:24:20,320
 They want to find any kinds of different things. And so it helps to create multiple independent

834
01:24:20,320 --> 01:24:26,160
 channels of communication, gather lots of different types of data, and then decode the output. Now,

835
01:24:26,160 --> 01:24:30,480
 going back to the paper for a second, of course, I didn't explain this figure in full detail, but

836
01:24:30,480 --> 01:24:34,000
 we are starting to see some components of what we've already implemented. We have the positional

837
01:24:34,000 --> 01:24:39,120
 encodings, token encodings that add, we have the masked multi headed attention implemented.

838
01:24:39,120 --> 01:24:44,240
 Now, here's another multi headed attention, which is a cross attention to an encoder,

839
01:24:44,240 --> 01:24:48,240
 which we haven't we're not going to implement in this case. I'm going to come back to that later.

840
01:24:49,280 --> 01:24:53,840
 But I want you to notice that there's a feed forward part here, and then this is grouped into a block

841
01:24:53,840 --> 01:24:58,560
 that gets repeated again and again. Now, the feed forward part here is just a simple multi layer

842
01:24:58,560 --> 01:25:06,800
 projection. So the multi headed so here position wise feed forward networks is just a simple little

843
01:25:06,800 --> 01:25:12,560
 MLP. So I want to start basically in a similar fashion also adding computation into the network.

844
01:25:12,560 --> 01:25:18,880
 And this computation is on a per node level. So I've already implemented it, and you can see

845
01:25:18,880 --> 01:25:24,000
 the diff highlighted on the left here when I've added or changed things. Now, before we had the

846
01:25:24,000 --> 01:25:29,600
 multi headed self attention that did the communication, but we went way too fast to calculate the

847
01:25:29,600 --> 01:25:34,560
 logits. So the tokens looked at each other, but didn't really have a lot of time to think on

848
01:25:34,560 --> 01:25:40,960
 what they found from the other tokens. And so what I've implemented here is a little feed forward

849
01:25:40,960 --> 01:25:46,480
 single layer, and this little layer is just a linear followed by a row nonlinearity. And that's

850
01:25:46,480 --> 01:25:55,280
 that's it. So it's just a little layer. And then I call it feed forward, and embed. And then this

851
01:25:55,280 --> 01:26:00,080
 feed forward is just called sequentially right after the self attention. So we self attend,

852
01:26:00,080 --> 01:26:04,720
 then we feed forward. And you'll notice that the feed forward here, when it's applying linear,

853
01:26:04,720 --> 01:26:10,160
 this is on a per token level. All the tokens do this independently. So the self attention is the

854
01:26:10,160 --> 01:26:14,560
 communication. And then once they gathered all the data, now they need to think on that data

855
01:26:14,560 --> 01:26:19,440
 individually. And so that's what feed forward is doing. And that's why I've added it here.

856
01:26:19,440 --> 01:26:25,120
 Now when I train this, the validation loss actually continues to go down now to 2.24,

857
01:26:25,120 --> 01:26:30,480
 which is down from 2.28. The outputs still look kind of terrible, but at least we've improved

858
01:26:30,480 --> 01:26:38,000
 the situation. And so as a preview, we're going to now start to interspers the communication

859
01:26:38,000 --> 01:26:43,760
 with the computation. And that's also what the transformer does, when it has blocks that communicate

860
01:26:43,760 --> 01:26:50,640
 and then compute, and it groups them and replicates them. Okay, so let me show you what we like to do.

861
01:26:50,640 --> 01:26:55,360
 We'd like to do something like this. We have a block. And this block is basically this part here,

862
01:26:55,360 --> 01:27:01,360
 except for the cross attention. Now the block basically intersperses communication and the

863
01:27:01,360 --> 01:27:06,480
 computation. The computation, the communication is done using multi headed self attention. And

864
01:27:06,480 --> 01:27:10,880
 then the computation is done using a feed forward network on all the tokens independently.

865
01:27:12,480 --> 01:27:18,880
 Now what I've added here also is you'll notice this takes the number of embeddings in the

866
01:27:18,880 --> 01:27:22,560
 embedding dimension and number of heads that we would like, which is kind of like group sizing,

867
01:27:22,560 --> 01:27:27,680
 group convolution. And I'm saying that number of heads we'd like is four. And so because this is

868
01:27:27,680 --> 01:27:35,040
 32, we calculate that because this 32, the number of heads should be four, the head size should be

869
01:27:35,040 --> 01:27:40,000
 eight, so that everything sort of works out channel wise. So this is how the transformer

870
01:27:40,000 --> 01:27:46,160
 structures sort of the sizes typically. So the head size will become eight. And then this is

871
01:27:46,160 --> 01:27:50,800
 how we want to interspers them. And then here, I'm trying to create blocks, which is just a

872
01:27:50,800 --> 01:27:55,680
 sequential application of block block block. So that we're interspersing communication feed

873
01:27:55,680 --> 01:28:01,920
 forward many, many times. And then finally, we decode. Now actually try to run this. And the

874
01:28:01,920 --> 01:28:06,960
 problem is this doesn't actually give a very good answer. And we're very good result. And the

875
01:28:06,960 --> 01:28:11,520
 reason for that is we're starting to actually get like a pretty deep neural net. And deep neural

876
01:28:11,520 --> 01:28:15,440
 nets suffer from optimization issues. And I think that's what we're kind of like slightly

877
01:28:15,440 --> 01:28:21,360
 starting to run into. So we need one more idea that we can borrow from the transfer of paper to

878
01:28:21,360 --> 01:28:25,920
 resolve those difficulties. Now there are two optimizations that dramatically help with the

879
01:28:25,920 --> 01:28:30,720
 depth of these networks, and make sure that the networks remain optimizable. Let's talk about the

880
01:28:30,720 --> 01:28:36,880
 first one. The first one in this diagram is you see this arrow here. And then this arrow and this

881
01:28:36,880 --> 01:28:42,080
 arrow, those are skip connections, or sometimes called residual connections. They come from this

882
01:28:42,080 --> 01:28:48,960
 paper, the procedural learning from a direct mission from about 2015, that introduced the concept.

883
01:28:48,960 --> 01:28:55,920
 Now, these are basically what it means is you transform data, but then you have a skip connection

884
01:28:55,920 --> 01:29:02,240
 with addition from the previous features. Now the way I like to visualize it, that I prefer,

885
01:29:02,880 --> 01:29:08,720
 is the following. Here the computation happens from the top to bottom. And basically you have this

886
01:29:08,720 --> 01:29:14,800
 residual pathway, and you are free to fork off from the residual pathway, perform some computation,

887
01:29:14,800 --> 01:29:19,440
 and then project back to the residual pathway via addition. And so you go from the

888
01:29:19,440 --> 01:29:26,720
 inputs to the targets, only the plus and plus and plus. And the reason this is useful is because

889
01:29:26,720 --> 01:29:32,160
 during that propagation, remember from our micro grad video earlier, addition distributes

890
01:29:32,160 --> 01:29:39,600
 gradients equally to both of its branches, that that fat is the input. And so the supervision or

891
01:29:39,600 --> 01:29:46,160
 the gradients from the loss basically hop through every addition node all the way to the input,

892
01:29:46,160 --> 01:29:53,280
 and then also fork off into the residual blocks. But basically have this gradient superhighway

893
01:29:53,280 --> 01:29:58,480
 that goes directly from the supervision all the way to the input, unimpeded. And then these

894
01:29:58,480 --> 01:30:02,640
 residual blocks are usually initialized in the beginning, so they contribute very, very little,

895
01:30:02,640 --> 01:30:07,520
 if anything, to the residual pathway. They are initialized that way. So in the beginning,

896
01:30:07,520 --> 01:30:13,200
 they are almost kind of like not there. But then during the optimization, they come online over time,

897
01:30:13,200 --> 01:30:18,960
 and they start to contribute. But at least at the initialization, you can go from directly

898
01:30:18,960 --> 01:30:24,080
 supervision to the input gradient, this unimpeded and just flows, and then the blocks over time

899
01:30:24,640 --> 01:30:29,920
 kick in. And so that dramatically helps with the optimization. So let's implement this. So

900
01:30:29,920 --> 01:30:35,280
 coming back to our block here, basically what we want to do is we want to do x equals x plus

901
01:30:35,280 --> 01:30:43,520
 self attention and x equals x plus solve that feed forward. So this is x, and then we fork off

902
01:30:43,520 --> 01:30:47,920
 and do some communication and come back, and we fork off and we do some computation and come back.

903
01:30:47,920 --> 01:30:54,080
 So those are residual connections. And then swinging back up here, we also have to introduce

904
01:30:54,080 --> 01:31:02,960
 this projection. So an end that linear. And this is going to be from after we concatenate this,

905
01:31:02,960 --> 01:31:08,560
 this is the size and embed. So this is the output of the self tension itself. But then we actually

906
01:31:08,560 --> 01:31:15,360
 want the to apply the projection. And that's the result. So the projection is just a linear

907
01:31:15,360 --> 01:31:21,040
 transformation of the outcome of this layer. So that's the projection back into the residual pathway.

908
01:31:21,760 --> 01:31:26,480
 And then here in a feed forward, it's going to be the same thing. I could have a self that projection

909
01:31:26,480 --> 01:31:33,520
 here as well. But let me just simplify it. And let me couple it inside the same sequential

910
01:31:33,520 --> 01:31:39,600
 container. And so this is the projection layer going back into the residual pathway. And so

911
01:31:39,600 --> 01:31:45,200
 that's, well, that's it. So now we can train this. So I'm putting within one more small change.

912
01:31:45,840 --> 01:31:51,840
 When you look into the paper again, you see that the dimensionality of input and output is 512

913
01:31:51,840 --> 01:31:57,040
 for them. And they're saying that the inner layer here in the feed forward has dimensionality of 2048.

914
01:31:57,040 --> 01:32:01,760
 So there's a multiplier of four. And so the inner layer of the feed forward network

915
01:32:01,760 --> 01:32:06,960
 should be multiplied by four in terms of channel sizes. So I came here and I multiplied four times

916
01:32:06,960 --> 01:32:12,720
 embed here for the feed forward. And then from four times an embed coming back down to an embed

917
01:32:12,720 --> 01:32:17,440
 when we go back to the projection to the projection. So adding a bit of computation here and growing

918
01:32:17,440 --> 01:32:23,760
 that layer that is in the residual block on the side of the residual pathway. And then I train

919
01:32:23,760 --> 01:32:29,280
 this and we actually get down all the way to 2.08 validation loss. And we also see that the network

920
01:32:29,280 --> 01:32:33,200
 is starting to get big enough that our train loss is getting ahead of validation loss. So we

921
01:32:33,200 --> 01:32:41,520
 start to see like a little bit of overfitting. And our our generations here are still not amazing

922
01:32:41,520 --> 01:32:47,680
 but at least you see that we can see like is here this now grief sync. Like this starts to almost

923
01:32:47,680 --> 01:32:52,800
 look like English. So yeah, we're starting to really get there. Okay, and the second innovation

924
01:32:52,800 --> 01:32:57,680
 that is very helpful for optimizing very deep neural works is right here. So we have this addition

925
01:32:57,680 --> 01:33:02,800
 now that's the residual part, but this norm is referring to something called layer norm. So

926
01:33:02,800 --> 01:33:07,200
 layer norm is implemented in PyTorch. It's a paper that came out a while back here.

927
01:33:10,080 --> 01:33:16,080
 And layer norm is very, very similar to bashroom. So remember back to our make more series part 3.

928
01:33:16,080 --> 01:33:21,600
 We implemented bashroom realization and bashroom realization basically just made sure that

929
01:33:21,600 --> 01:33:30,800
 across the bash dimension, any individual neuron had unit Gaussian distribution. So it was zero

930
01:33:30,800 --> 01:33:37,280
 mean and unit standard deviation, one standard deviation output. So what I did here is I'm copy

931
01:33:37,280 --> 01:33:43,200
 pasting the bashroom 1D that we developed in our make more series. And see here we can initialize

932
01:33:43,200 --> 01:33:48,560
 for example this module and we can have a batch of 32, 100 dimensional vectors feeding through

933
01:33:48,560 --> 01:33:55,520
 the bashroom layer. So what this does is it guarantees that when we look at just the 0th column,

934
01:33:55,520 --> 01:34:02,880
 it's a 0 mean 1 standard deviation. So it's normalizing every single column of this input.

935
01:34:03,840 --> 01:34:08,880
 Now the rows are not going to be normalized by default because we're just normalizing columns.

936
01:34:08,880 --> 01:34:15,760
 So let's not implement layer norm. It's very complicated. Look, we come here, we change this

937
01:34:15,760 --> 01:34:23,040
 from 0 to 1. So we don't normalize the columns, we normalize the rows. And now we've implemented

938
01:34:23,040 --> 01:34:31,520
 layer norm. So now the columns are not going to be normalized, but the rows are going to be

939
01:34:31,520 --> 01:34:36,560
 normalized. For every individual example, it's 100 dimensional vector is normalized in this way.

940
01:34:36,560 --> 01:34:44,000
 And because our computation does not span across examples, we can delete all of this buffers stuff

941
01:34:44,000 --> 01:34:50,000
 because we can always apply this operation and don't need to maintain any running buffers.

942
01:34:50,000 --> 01:34:57,280
 So we don't need the buffers. We don't, there's no distinction between training and test time.

943
01:34:59,360 --> 01:35:04,720
 And we don't need these running buffers. We do keep gamma and beta. We don't need the momentum.

944
01:35:04,720 --> 01:35:12,320
 We don't care if it's training or not. And this is now a layer norm. And it normalizes the

945
01:35:12,320 --> 01:35:20,400
 rows instead of the columns. And this here is identical to basically this here. So let's now

946
01:35:20,400 --> 01:35:24,320
 implement layer norm in our transformer. Before I incorporate the layer norm, I just wanted to

947
01:35:24,320 --> 01:35:28,960
 note that as I said, very few details about the transformer have changed in the last five years.

948
01:35:28,960 --> 01:35:33,360
 But this is actually something that's likely the parts from the original paper. You see that the

949
01:35:33,360 --> 01:35:41,440
 add and norm is applied after the transformation. But in now it is a bit more basically common to

950
01:35:41,440 --> 01:35:46,000
 apply the layer norm before the transformation. So there's a reshuffling of the layer norms.

951
01:35:46,000 --> 01:35:50,240
 So this is called the pre norm formulation. And that the one that we're going to implement as

952
01:35:50,240 --> 01:35:54,800
 well. So select deviation from the original paper. Basically, we need to in layer norms,

953
01:35:54,800 --> 01:36:01,680
 layer norm one is an end dot layer norm. And we tell it how many words the embedding dimension.

954
01:36:01,680 --> 01:36:08,480
 And we need the second layer norm. And then here, the layer arms are applied immediately on x.

955
01:36:08,480 --> 01:36:16,080
 So self that layer norm one in applied on x and self that layer to apply to x before it goes

956
01:36:16,080 --> 01:36:22,320
 into self attention and feed forward. And the size of the layer norm here is an embeds of 32.

957
01:36:22,960 --> 01:36:28,160
 So when the layer norm is normalizing our features, it is the normalization here

958
01:36:28,160 --> 01:36:36,400
 happens. The mean and the variance are taken over 32 numbers. So the batch and the time act as batch

959
01:36:36,400 --> 01:36:42,800
 dimensions, both of them. So this is kind of like a per token transformation that just normalizes

960
01:36:42,800 --> 01:36:49,760
 the features and makes them a unit mean, unit Gaussian at initialization. But of course,

961
01:36:49,760 --> 01:36:55,680
 because these layer norms inside it have these gamma and beta trainable parameters. The layer

962
01:36:55,680 --> 01:37:02,160
 normal eventually create outputs that might not be unit Gaussian, but the optimization will determine

963
01:37:02,160 --> 01:37:08,560
 that. So for now, this is the this is incorporating the layer norms and let's train them up. Okay,

964
01:37:08,560 --> 01:37:14,080
 so I let it run. And we see that we get down to 2.06, which is better than the previous 2.08.

965
01:37:14,080 --> 01:37:18,960
 So a slight improvement by adding the layer norms. And I'd expect that they help even more if we

966
01:37:18,960 --> 01:37:23,920
 have bigger and deeper network. One more thing I forgot to add is that there should be a layer norm

967
01:37:23,920 --> 01:37:30,720
 here also typically, as at the end of the transformer and right before the final linear layer that

968
01:37:30,720 --> 01:37:36,960
 decodes into vocabulary. So I added that as well. So at this stage, we actually have a pretty complete

969
01:37:36,960 --> 01:37:42,080
 transformer coming to the original paper. And it's a decoder only transformer. I'll talk about that

970
01:37:42,080 --> 01:37:47,040
 in a second. But at this stage, the major pieces are in place. So we can try to scale this up and

971
01:37:47,040 --> 01:37:51,840
 see how well we can push this number. Now, in order to scale up the model, I had to perform some

972
01:37:51,840 --> 01:37:56,880
 cosmetic changes here to make it nicer. So I introduced this variable called n layer, which

973
01:37:56,880 --> 01:38:02,320
 just specifies how many layers of the blocks we're going to have. I create a bunch of blocks and we

974
01:38:02,320 --> 01:38:07,920
 have a new variable number of heads as well. I pulled out the layer norm here. And so this is

975
01:38:07,920 --> 01:38:14,400
 identical. Now one thing that I did briefly change is I added dropout. So dropout is something

976
01:38:14,400 --> 01:38:20,080
 that you can add right before the residual connection back, right before the connection back into the

977
01:38:20,080 --> 01:38:26,960
 residual pathway. So we can drop out that as the last layer here, we can drop out here at the end

978
01:38:26,960 --> 01:38:32,880
 of the multi headed attention as well. And we can also drop out here, when we calculate the

979
01:38:32,880 --> 01:38:38,640
 basically, affinities and after the softmax, we can drop out some of those. So we can

980
01:38:38,640 --> 01:38:46,320
 randomly prevent some of the nodes from communicating. And so dropout comes from this paper from 2014

981
01:38:46,320 --> 01:38:54,000
 or so. And basically it takes your neural mat. And it randomly, every forward backward pass,

982
01:38:54,000 --> 01:39:01,440
 shuts off some subset of neurons. So randomly drops them to zero and trains without them.

983
01:39:01,440 --> 01:39:06,800
 And what this does effectively is because the mask of what's being dropped out has changed

984
01:39:06,800 --> 01:39:12,320
 every single forward backward pass. It ends up kind of training an ensemble of sub networks.

985
01:39:12,320 --> 01:39:17,200
 And then at test time, everything is fully enabled and kind of all of those sub networks are merged

986
01:39:17,200 --> 01:39:21,840
 into a single ensemble, if you can, if you want to think about it that way. So I would read the paper

987
01:39:21,840 --> 01:39:26,640
 to get the full detail. For now, we're just going to stay on the level of this is a regularization

988
01:39:26,640 --> 01:39:31,120
 technique. And I added it because I'm about to scale up the model quite a bit. And I was concerned

989
01:39:31,120 --> 01:39:37,360
 about overfitting. So now when we scroll up to the top, we'll see that I changed a number of

990
01:39:37,360 --> 01:39:42,240
 hyper parameters here about our neural mat. So I made the best size be much larger, now 64.

991
01:39:42,240 --> 01:39:48,480
 I changed the block size to be 256. So previously was just eight, eight characters of context. Now

992
01:39:48,480 --> 01:39:55,920
 it is 256 characters of context to predict the 257th. I brought down the learning rate a little

993
01:39:55,920 --> 01:40:00,560
 bit because the neural mat is now much bigger. So I brought down the learning way. The embedding

994
01:40:00,560 --> 01:40:08,080
 dimension is not 384. And there are six heads. So 384 divide six means that every head is 64

995
01:40:08,080 --> 01:40:14,160
 dimensional as it as a standard. And then there are also going to be six layers of that. And the

996
01:40:14,160 --> 01:40:20,720
 dropout will be a point to so every forward backward pass 20% of all of these intermediate

997
01:40:20,720 --> 01:40:26,240
 calculations are disabled and dropped to zero. And then I already trained this and I ran it. So

998
01:40:27,120 --> 01:40:30,560
 drum roll, how does it perform? So let me just scroll up here.

999
01:40:30,560 --> 01:40:37,920
 We get a validation loss of 1.48, which is actually quite a bit of an improvement on what we had

1000
01:40:37,920 --> 01:40:43,760
 before, which I think was 2.07. So we went from 2.07 all the way down to 1.48 just by scaling up

1001
01:40:43,760 --> 01:40:48,560
 this neural mat with the code that we have. And this of course ran for a lot longer. This

1002
01:40:48,560 --> 01:40:53,760
 may be trained for I want to say about 15 minutes on my a 100 GPU. So that's a pretty good GPU.

1003
01:40:54,400 --> 01:40:59,040
 And if you don't have a GPU, you're not going to be able to reproduce this. On a CPU, this would be,

1004
01:40:59,040 --> 01:41:03,680
 I would not run this on a CPU or MacBook or something like that. You'll have to bring down

1005
01:41:03,680 --> 01:41:09,760
 the number of layers and the embedding dimension and so on. But in about 15 minutes, we can get

1006
01:41:09,760 --> 01:41:16,160
 this kind of a result. And I'm printing some of the Shakespeare here. But what I did also is I

1007
01:41:16,160 --> 01:41:21,440
 printed 10,000 characters, so a lot more and I wrote them to a file. And so here we see some of the

1008
01:41:21,440 --> 01:41:29,280
 outputs. So it's a lot more recognizable as the input text file. So the input text file just for

1009
01:41:29,280 --> 01:41:34,880
 reference looked like this. So there's always like someone speaking in this matter. And

1010
01:41:34,880 --> 01:41:41,760
 our predictions now take on that form. Except of course they're non-sensical when you actually

1011
01:41:41,760 --> 01:41:50,400
 read them. So it is every crimpy be house. Oh, those preparation. We give heed.

1012
01:41:51,120 --> 01:42:03,920
 Oh, oh, sent me you mighty lord. Anyway, so you can read through this. It's nonsensical, of course, but

1013
01:42:03,920 --> 01:42:09,440
 this is just a transformer trained on the character level for 1 million characters that come from

1014
01:42:09,440 --> 01:42:14,560
 Shakespeare. So there's sort of like blabbers on in Shakespeare like manner, but it doesn't of course

1015
01:42:14,560 --> 01:42:20,320
 make sense at this scale. But I think I think still a pretty good demonstration of what's possible.

1016
01:42:20,320 --> 01:42:29,040
 So now I think that kind of like concludes the programming section of this video. We basically

1017
01:42:29,040 --> 01:42:35,680
 kind of did a pretty good job in implementing this transformer, but the picture doesn't exactly

1018
01:42:35,680 --> 01:42:40,240
 match up to what we've done. So what's going on with all these additional parts here? So let me

1019
01:42:40,240 --> 01:42:45,680
 finish explaining this architecture and why it looks so funky. Basically what's happening here is

1020
01:42:45,680 --> 01:42:51,200
 what we implemented here is a decoder only transformer. So there's no component here. This

1021
01:42:51,200 --> 01:42:57,280
 part is called the encoder. And there's no cross attention block here. Our block only has a self

1022
01:42:57,280 --> 01:43:03,360
 attention and the feed forward. So it is missing this third in between piece here. This piece does

1023
01:43:03,360 --> 01:43:08,240
 cross attention. So we don't have it and we don't have the encoder. We just have the decoder. And

1024
01:43:08,240 --> 01:43:14,080
 the reason we have a decoder only is because we are just generating text and it's unconditioned

1025
01:43:14,080 --> 01:43:19,760
 on anything. We're just blabbering on according to a given data set. What makes it a decoder is

1026
01:43:19,760 --> 01:43:25,600
 that we are using the triangular mask in our transformer. So it has this autoregressive property

1027
01:43:25,600 --> 01:43:30,960
 where we can just go and sample from it. So the fact that it's using the triangular mask

1028
01:43:30,960 --> 01:43:37,360
 to mask out the attention makes it a decoder. And it can be used for language modeling. Now the

1029
01:43:37,360 --> 01:43:41,840
 reason that the original paper had an encoder decoder architecture is because it is a machine

1030
01:43:41,840 --> 01:43:48,320
 translation paper. So it is concerned with a different setting in particular. It expects some

1031
01:43:48,320 --> 01:43:54,720
 tokens that encode say for example French. And then it is expected to decode the translation in

1032
01:43:54,720 --> 01:44:01,840
 English. So you typically these here are special tokens. So you are expected to read in this and

1033
01:44:01,840 --> 01:44:06,640
 condition on it. And then you start off the generation with a special token called start.

1034
01:44:06,640 --> 01:44:11,680
 So this is a special new token that you introduce and always place in the beginning.

1035
01:44:11,680 --> 01:44:17,920
 And then the network is expected to output neural networks are awesome. And then a special

1036
01:44:17,920 --> 01:44:25,520
 end token to finish the generation. So this part here will be decoded exactly as we have done it.

1037
01:44:25,520 --> 01:44:31,280
 Neural networks are awesome. Will be identical to what we did. But unlike what we did, they want to

1038
01:44:31,280 --> 01:44:36,960
 condition the generation on some additional information. And in that case, this additional

1039
01:44:36,960 --> 01:44:42,880
 information is the French sentence that they should be translating. So what they do now is they

1040
01:44:42,880 --> 01:44:50,160
 bring the encoder. Now the encoder reads this part here. So we're all going to take the part of French

1041
01:44:50,160 --> 01:44:55,680
 and we're going to create tokens from it exactly as we've seen in our video. And we're going to

1042
01:44:55,680 --> 01:45:00,480
 put a transformer on it. But there's going to be no triangular mask. And so all the tokens are

1043
01:45:00,480 --> 01:45:05,040
 allowed to talk to each other as much as they want. And they're just encoding whatever's the

1044
01:45:05,040 --> 01:45:12,320
 content of this French sentence. Once they've encoded it, they've basically come out in the top

1045
01:45:12,320 --> 01:45:18,080
 here. And then what happens here is in our decoder, which does the language modeling,

1046
01:45:18,080 --> 01:45:24,480
 there's an additional connection here to the outputs of the encoder. And that is brought in

1047
01:45:24,480 --> 01:45:30,800
 through cross attention. So the queries are still generated from x. But now the keys and the values

1048
01:45:30,800 --> 01:45:36,400
 are coming from the side. The keys and the values are coming from the top generated by the nodes

1049
01:45:36,400 --> 01:45:42,640
 that came outside of the decoder. And those tops, the keys and the values there, the top of it,

1050
01:45:42,640 --> 01:45:48,880
 feed in on a side into every single block of the decoder. And so that's why there's an additional

1051
01:45:48,880 --> 01:45:54,880
 cross attention. And really what it's doing is it's conditioning the decoding, not just on the past

1052
01:45:54,880 --> 01:46:04,000
 of this current decoding, but also on having seen the full fully encoded French prompt sort of.

1053
01:46:04,000 --> 01:46:08,800
 And so it's an encoded decoder model, which is why we have those two transformers, an additional

1054
01:46:08,800 --> 01:46:13,680
 block, and so on. So we did not do this because we have no we have nothing to encode. There's no

1055
01:46:13,680 --> 01:46:18,240
 conditioning. We just have a text file and we just want to imitate it. And that's why we are using a

1056
01:46:18,240 --> 01:46:24,960
 decoder only transformer exactly as done in GPT. Okay, so now I wanted to do a very brief walkthrough

1057
01:46:24,960 --> 01:46:31,120
 of nano GPT, which you can find on my GitHub. And nano GPT is basically two files of interest.

1058
01:46:31,120 --> 01:46:36,320
 There's trained up by and modeled up by trained up by is all the boilerplate code for training the

1059
01:46:36,320 --> 01:46:42,560
 network. It is basically all the stuff that we had here is the training loop. It's just that it's a

1060
01:46:42,560 --> 01:46:47,120
 lot more complicated because we're saving and loading checkpoints and pre trained weights. And we are

1061
01:46:47,760 --> 01:46:51,520
 decaying the learning rate and compiling the model and using distributed training across

1062
01:46:51,520 --> 01:46:57,280
 multiple nodes or GPUs. So the training that pogates a little bit more hairy complicated.

1063
01:46:57,280 --> 01:47:03,520
 There's more options, etc. But the model that I should look very, very similar to what we've done

1064
01:47:03,520 --> 01:47:10,160
 here. In fact, the model is almost identical. So first, here we have the causal self attention

1065
01:47:10,160 --> 01:47:14,720
 block. And all of this should look very, very recognizable to you. We're producing queries,

1066
01:47:14,720 --> 01:47:20,800
 keys, values. We're doing dot products, we're masking, applying softmax, optionally dropping out.

1067
01:47:20,800 --> 01:47:26,480
 And here we are pulling the way the values. What is different here is that in our code,

1068
01:47:26,480 --> 01:47:34,000
 I have separated out the multi headed attention into just a single individual head. And then

1069
01:47:34,000 --> 01:47:39,600
 here I have multiple heads and I explicitly concatenate them. Whereas here, all of it is

1070
01:47:39,600 --> 01:47:44,960
 implemented in a batch manner inside a single causal self attention. And so we don't just have a B

1071
01:47:44,960 --> 01:47:50,400
 and a T and a C dimension. We also end up with a fourth dimension, which is the heads. And so it

1072
01:47:50,400 --> 01:47:56,480
 just gets a lot more sort of hairy because we have four dimensional array tensors now. But it is

1073
01:47:56,480 --> 01:48:01,600
 equivalent mathematically. So the exact same thing is happening as what we have. It's just a

1074
01:48:01,600 --> 01:48:05,120
 bit more efficient because all the heads are now treated as a batch dimension as well.

1075
01:48:06,640 --> 01:48:11,280
 Then we have the multiple layer perceptron. It's using the Galoon nonlinearity, which is defined

1076
01:48:11,280 --> 01:48:16,000
 here, instead of Ralu. And this is done just because opening I used it and I want to be able to load

1077
01:48:16,000 --> 01:48:21,600
 their checkpoints. The blocks of the transformer are identical, the communicate and the compute

1078
01:48:21,600 --> 01:48:27,520
 phase as we saw. And then the GPT will be identical. We have the position encodings, token encodings,

1079
01:48:27,520 --> 01:48:33,600
 the blocks, the layer norm at the end, the final linear layer. And this should look all very

1080
01:48:33,600 --> 01:48:37,920
 recognizable. And there's a bit more here, because I'm loading checkpoints and stuff like that.

1081
01:48:37,920 --> 01:48:42,400
 I'm separating out the parameters into those that should be weight-decade and those that shouldn't.

1082
01:48:42,400 --> 01:48:48,240
 But the generate function should also be very, very similar. So a few details are different,

1083
01:48:48,240 --> 01:48:53,200
 but you should definitely be able to look at this file and be able to understand a lot of the pieces

1084
01:48:53,200 --> 01:48:58,400
 now. So let's now bring things back to chat GPT. What would it look like if we wanted to train

1085
01:48:58,400 --> 01:49:03,440
 chat GPT ourselves? And how does it relate to what we learned today? Well, to train in chat GPT,

1086
01:49:03,440 --> 01:49:09,280
 there are roughly two stages. First is the pre-training stage and then the fine-tuning stage. In the

1087
01:49:09,280 --> 01:49:15,280
 pre-training stage, we are training on the large chunk of internet and just trying to get a first

1088
01:49:15,280 --> 01:49:21,440
 decoder-only transformer to babble text. So it's very, very similar to what we've done ourselves.

1089
01:49:22,320 --> 01:49:30,160
 Except we've done like a tiny little baby pre-training step. And so in our case, this is how you print

1090
01:49:30,160 --> 01:49:35,840
 a number of parameters. I printed it and it's about 10 million. So this transformer that I created here

1091
01:49:35,840 --> 01:49:43,200
 to create little Shakespeare transformer was about 10 million parameters. Our dataset is roughly

1092
01:49:43,200 --> 01:49:48,000
 one million characters, so roughly one million tokens. But you have to remember that opening

1093
01:49:48,000 --> 01:49:53,120
 eye uses different vocabulary. They're not on the character level. They use these sub-word

1094
01:49:53,120 --> 01:49:58,800
 chunks of words. And so they have a vocabulary of 50,000 roughly elements. And so their sequences

1095
01:49:58,800 --> 01:50:04,880
 are a bit more condensed. So our dataset, the Shakespeare dataset would be probably around 300,000

1096
01:50:04,880 --> 01:50:11,920
 tokens in the open-eye vocabulary roughly. So we trained about 10 million parameter model on

1097
01:50:11,920 --> 01:50:20,240
 roughly 300,000 tokens. Now when you go to the GPT-3 paper and you look at the transformers that they

1098
01:50:20,240 --> 01:50:25,760
 train, they train to number of transformers of different sizes. But the biggest transformer

1099
01:50:25,760 --> 01:50:32,320
 here has 175 billion parameters. So ours is again 10 million. They used this number of layers in a

1100
01:50:32,320 --> 01:50:38,160
 transformer. This is the end in bed. This is the number of heads. And this is the head size.

1101
01:50:39,040 --> 01:50:46,400
 And then this is the batch size. So ours was 65. And the learning rate is similar. Now when they

1102
01:50:46,400 --> 01:50:53,040
 train this transformer, they trained on 300 billion tokens. So again, remember ours is about 300,000.

1103
01:50:53,040 --> 01:50:58,800
 So this is about a million fold increase. And this number would not be even that large by today's

1104
01:50:58,800 --> 01:51:05,600
 standard. So you'd be going up one trillion and above. So they are training a significantly larger

1105
01:51:05,600 --> 01:51:12,400
 model on a good chunk of the internet. And that is the pre-training stage. But otherwise,

1106
01:51:12,400 --> 01:51:16,640
 these hyperparameters should be fairly recognizable to you. And the architecture is actually like

1107
01:51:16,640 --> 01:51:21,040
 nearly identical to what we implemented ourselves. But of course, it's a massive infrastructure

1108
01:51:21,040 --> 01:51:26,560
 challenge to train this. You're talking about typically thousands of GPUs having to, you know,

1109
01:51:26,560 --> 01:51:31,520
 talk to each other to train models of this size. So that's just the pre-training stage.

1110
01:51:31,520 --> 01:51:36,640
 Now, after you complete the pre-training stage, you don't get something that responds to your

1111
01:51:36,640 --> 01:51:43,120
 questions with answers. And it's not helpful and etc. You get a document complete. Right? So

1112
01:51:43,120 --> 01:51:47,920
 it babbles, but it doesn't babble Shakespeare in bables internet. It will create arbitrary

1113
01:51:47,920 --> 01:51:51,680
 news articles and documents and it will try to complete documents because that's what it's

1114
01:51:51,680 --> 01:51:56,160
 trained for. It's trying to complete the sequence. So when you give it a question, it would just

1115
01:51:56,160 --> 01:52:00,640
 potentially just give you more questions. It would follow with more questions. It will do

1116
01:52:00,640 --> 01:52:06,800
 whatever it looks like some close document would do in the training data on the internet. And so

1117
01:52:06,800 --> 01:52:11,280
 who knows, you're getting kind of like undefined behavior. It might basically answer with two

1118
01:52:11,280 --> 01:52:15,760
 questions with other questions. It might ignore your question. It might just try to complete some

1119
01:52:15,760 --> 01:52:22,000
 news article. It's totally unaligned, as we say. So the second fine tuning stage is to actually

1120
01:52:22,000 --> 01:52:28,960
 align it to be an assistant. And this is the second stage. And so this chat GPT blog post from

1121
01:52:28,960 --> 01:52:35,920
 OpenAI talks a little bit about how the stage is achieved. We basically, there's roughly three

1122
01:52:35,920 --> 01:52:41,360
 steps to this stage. So what they do here is they start to collect training data that looks

1123
01:52:41,360 --> 01:52:45,840
 specifically like what an assistant would do. So they have documents that have the format where

1124
01:52:45,840 --> 01:52:50,400
 the question is on top and then an answer is below. And they have a large number of these,

1125
01:52:50,400 --> 01:52:55,120
 but probably not on the order of the internet. This is probably on the order of maybe thousands

1126
01:52:55,120 --> 01:53:03,520
 of examples. And so they then fine tune the model to basically only focus on documents that look

1127
01:53:03,520 --> 01:53:08,160
 like that. And so you're starting to slowly align it. So it's going to expect a question at the top

1128
01:53:08,160 --> 01:53:13,920
 and it's going to expect to complete the answer. And these very, very large models are very sample

1129
01:53:13,920 --> 01:53:18,720
 efficient during their fine tuning. So this actually somehow works. But that's just step one.

1130
01:53:18,720 --> 01:53:23,760
 That's just fine tuning. So then they actually have more steps where, okay, the second step is you

1131
01:53:23,760 --> 01:53:28,720
 let the model respond and then different raters look at the different responses and rank them

1132
01:53:28,720 --> 01:53:33,920
 for their preferences to which one is better than the other. They use that to train the reward model.

1133
01:53:33,920 --> 01:53:39,840
 So they can predict basically using a different network how much of any candidate response

1134
01:53:39,840 --> 01:53:46,720
 would be desirable. And then once they have a reward model, they run PPO, which is a form of

1135
01:53:46,720 --> 01:53:53,280
 policy gradient reinforcement learning optimizer, to fine tune this sampling policy.

1136
01:53:54,000 --> 01:54:01,920
 So that the answers that the GPT now generates are expected to score a high reward according to

1137
01:54:01,920 --> 01:54:07,520
 the reward model. And so basically there's a whole lining stage here or fine tuning stage.

1138
01:54:07,520 --> 01:54:12,640
 It's got multiple steps in between there as well. And it takes the model from being a document

1139
01:54:12,640 --> 01:54:19,520
 completer to a question answer. And that's like a whole separate stage. A lot of this data is not

1140
01:54:19,520 --> 01:54:25,040
 available publicly. It is internal to open AI. And it's much harder to replicate this stage.

1141
01:54:25,040 --> 01:54:31,760
 And so that's roughly what would give you a chat GPT. And nano GPT focuses on the pre-training

1142
01:54:31,760 --> 01:54:38,080
 stage. Okay, and that's everything that I wanted to cover today. So we trained to summarize a

1143
01:54:38,080 --> 01:54:43,920
 decoder only transformer following this famous paper attention is all you need from 2017.

1144
01:54:44,800 --> 01:54:51,440
 And so that's basically a GPT. We trained it on a tiny Shakespeare and got sensible results.

1145
01:54:51,440 --> 01:55:00,560
 All of the training code is roughly 200 lines of code. I will be releasing this code base. So

1146
01:55:00,560 --> 01:55:05,280
 also it comes with all the Git log commits along the way as we built it up.

1147
01:55:05,280 --> 01:55:11,600
 In addition to this code, I'm going to release the notebook, of course, the Google collab.

1148
01:55:12,480 --> 01:55:18,000
 And I hope that gave you a sense for how you can train these models, like say GPT-3,

1149
01:55:18,000 --> 01:55:22,640
 that will be architecturally basically identical to what we have. But they are somewhere between

1150
01:55:22,640 --> 01:55:29,440
 10,000 and 1 million times bigger, depending on how you count. And so that's all I have for now.

1151
01:55:29,440 --> 01:55:34,400
 We did not talk about any of the fine-tuning stages that would typically go on top of this.

1152
01:55:34,400 --> 01:55:38,080
 So if you're interested in something that's not just language modeling, but you actually want to,

1153
01:55:38,080 --> 01:55:43,600
 you know, say, perform tasks, or you want them to be aligned in a specific way, or you want

1154
01:55:43,600 --> 01:55:48,400
 to detect sentiment or anything like that, basically anytime you don't want something that's just a

1155
01:55:48,400 --> 01:55:53,120
 document completer, you have to complete further stages of fine-tuning, which we did not cover.

1156
01:55:53,120 --> 01:55:58,080
 And that could be simple supervised fine-tuning, or it can be something more fancy like we see in

1157
01:55:58,080 --> 01:56:03,520
 chat GPT, where we actually train a reward model and then do rounds of PPO to align it with

1158
01:56:03,520 --> 01:56:07,360
 their respective reward model. So there's a lot more that can be done on top of it.

1159
01:56:07,360 --> 01:56:12,880
 I think for now we're starting to get to about two hours mark, so I'm going to kind of finish here.

1160
01:56:12,880 --> 01:56:19,680
 I hope you enjoyed the lecture, and yeah, go forth and transform. See you later.

