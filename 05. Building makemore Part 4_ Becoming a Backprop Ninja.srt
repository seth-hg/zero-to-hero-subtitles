1
00:00:00,000 --> 00:00:04,240
 Hi everyone. So today we are once again continuing our implementation of make more.

2
00:00:04,240 --> 00:00:10,720
 Now so far we've come up to here multilial perceptrons and our neural net looked like this

3
00:00:10,720 --> 00:00:14,880
 and we were implementing this over the last few lectures. Now I'm sure everyone is very excited

4
00:00:14,880 --> 00:00:19,360
 to go into recurrent neural networks and all of their variants and how they work and the diagrams

5
00:00:19,360 --> 00:00:23,360
 look cool and it's very exciting and interesting and we're going to get a better result. But unfortunately

6
00:00:23,360 --> 00:00:29,200
 I think we have to remain here for one more lecture and the reason for that is we've already

7
00:00:29,200 --> 00:00:33,280
 trained this multilial perceptron right and we are getting pretty good loss and I think we have a

8
00:00:33,280 --> 00:00:38,400
 pretty decent understanding of the architecture and how it works but the line of code here that I

9
00:00:38,400 --> 00:00:44,880
 take an issue with is here, lost up backward. That is we are taking a pytorch autograph and using

10
00:00:44,880 --> 00:00:49,760
 it to calculate all of our gradients along the way and I would like to remove the use of lost up

11
00:00:49,760 --> 00:00:54,960
 backward and I would like us to write our backward pass manually on the level of tensors and I think

12
00:00:54,960 --> 00:01:00,160
 that this is a very useful exercise for the following reasons. I actually have an entire blog post on

13
00:01:00,160 --> 00:01:06,960
 this topic but I like to call backpropagation a leaky abstraction and what I mean by that is

14
00:01:06,960 --> 00:01:11,760
 backpropagation doesn't just make your neural networks just work magically. It's not the case

15
00:01:11,760 --> 00:01:15,760
 that you can just stack up arbitrary LEGO blocks of differentiable functions and just cross your

16
00:01:15,760 --> 00:01:21,120
 fingers and back propagate and everything is great. Things don't just work automatically. It is a

17
00:01:21,120 --> 00:01:25,280
 leaky abstraction in the sense that you can shoot yourself in a foot if you do not understanding

18
00:01:25,280 --> 00:01:31,920
 its internals. It will magically not work or not work optimally and you will need to understand

19
00:01:31,920 --> 00:01:35,840
 how it works under the hood if you're hoping to debug it and if you are hoping to address it in

20
00:01:35,840 --> 00:01:42,000
 your neural nut. So this blog post here from a while ago goes into some of those examples.

21
00:01:42,000 --> 00:01:47,040
 So for example we've already covered them some of them already. For example the flat tails of

22
00:01:47,040 --> 00:01:53,200
 these functions and how you do not want to saturate them too much because your gradients will die.

23
00:01:53,200 --> 00:01:58,880
 The case of dead neurons which I've already covered as well. The case of exploding or vanishing

24
00:01:58,880 --> 00:02:04,320
 gradients in the case of a current neural networks which we are about to cover. And then also you

25
00:02:04,320 --> 00:02:10,240
 will often come across some examples in the wild. This is a snippet that I found in a random code

26
00:02:10,240 --> 00:02:15,280
 based on the internet where they actually have like a very subtle but pretty major bug in their

27
00:02:15,280 --> 00:02:20,240
 implementation. And the bug points at the fact that the author of this code does not actually

28
00:02:20,240 --> 00:02:24,640
 understand back propagation. So what they're trying to do here is they're trying to clip the loss

29
00:02:24,640 --> 00:02:28,640
 at a certain maximum value. But actually what they're trying to do is they're trying to clip

30
00:02:28,640 --> 00:02:33,200
 the gradients to have a maximum value instead of trying to clip the loss at a maximum value.

31
00:02:33,200 --> 00:02:40,320
 And indirectly they're basically causing some of the outliers to be actually ignored. Because

32
00:02:40,320 --> 00:02:46,320
 when you clip a loss of an outlier you are setting its gradient to zero. And so have a look through

33
00:02:46,320 --> 00:02:50,480
 this and read through it. But there's basically a bunch of subtle issues that you're going to

34
00:02:50,480 --> 00:02:54,800
 avoid if you actually know what you're doing. And that's why I don't think it's the case that

35
00:02:54,800 --> 00:02:59,360
 because PyTorch or other frameworks offer autograd it is okay for us to ignore how it works.

36
00:02:59,360 --> 00:03:06,880
 Now we've actually already covered autograd and we wrote micrograd. But micrograd was an autograd

37
00:03:06,880 --> 00:03:11,280
 engine only on the level of individual scalars. So the atoms were single individual numbers.

38
00:03:11,280 --> 00:03:15,840
 And you know I don't think it's enough and I'd like us to basically think about back propagation

39
00:03:15,840 --> 00:03:21,760
 on level of tensors as well. And so in a summary I think it's a good exercise. I think it is very

40
00:03:21,760 --> 00:03:26,560
 very valuable. You're going to become better at debugging neural networks and making sure that

41
00:03:26,560 --> 00:03:30,160
 you understand what you're doing. It is going to make everything fully explicit so you're not

42
00:03:30,160 --> 00:03:34,480
 going to be nervous about what is hidden away from you. And basically in general we're going to

43
00:03:34,480 --> 00:03:40,640
 emerge stronger. And so let's get into it. A bit of a fun historical note here is that today

44
00:03:40,640 --> 00:03:45,120
 writing your backward pass by hand and manually is not recommended and no one does it except for

45
00:03:45,120 --> 00:03:50,080
 the purposes of exercise. But about 10 years ago in deep learning this was fairly standard and in

46
00:03:50,080 --> 00:03:54,720
 fact pervasive. So at the time everyone used to write their backward pass by hand manually,

47
00:03:54,720 --> 00:03:59,600
 including myself. And it's just what you would do. So we used to write backward pass by hand and

48
00:03:59,600 --> 00:04:05,840
 now everyone just called lost a backward. We've lost something. I wanted to give you a few examples

49
00:04:05,840 --> 00:04:13,360
 of this. So here's a 2006 paper from Jeff Hinton and Russell Select Enough in science that was

50
00:04:13,360 --> 00:04:18,640
 influential at the time. And this was training some architectures called restricted bolts and

51
00:04:18,640 --> 00:04:25,600
 machines. And basically it's an auto encoder trained here. And this is from roughly 2010.

52
00:04:25,600 --> 00:04:30,480
 I had a library for training restricted bolts and machines. And this was at the time written in

53
00:04:30,480 --> 00:04:35,600
 MATLAB. So Python was not used for deep learning pervasively. It was all MATLAB. And MATLAB was

54
00:04:35,600 --> 00:04:41,440
 this scientific computing package that everyone would use. So we would write MATLAB, which is

55
00:04:41,440 --> 00:04:47,040
 barely a programming language in a big as well. But it had a very convenient tensor class. And it

56
00:04:47,040 --> 00:04:51,120
 was this computing environment and you would run here, it would all run on the CPU of course,

57
00:04:51,120 --> 00:04:55,120
 but you would have very nice plots to go with it and a built-in debugger. And it was pretty nice.

58
00:04:55,760 --> 00:05:01,520
 Now the code in this package in 2010 that I wrote for fitting researchable machines,

59
00:05:01,520 --> 00:05:06,160
 to a large extent is recognizable. But I wanted to show you how you would,

60
00:05:06,160 --> 00:05:11,920
 well, I'm creating the data and the XY batches. I'm initializing the neural nut. So it's got weights

61
00:05:11,920 --> 00:05:16,480
 and biases just like we're used to. And then this is the training loop where we actually do the

62
00:05:16,480 --> 00:05:21,280
 forward pass. And then here, at this time, they didn't even necessarily use back propagation to

63
00:05:21,280 --> 00:05:27,360
 train neural networks. So this in particular implements contrastive divergence, which estimates

64
00:05:27,360 --> 00:05:32,480
 a gradient. And then here we take that gradient and use it for a parameter update along lines

65
00:05:32,480 --> 00:05:38,560
 that we're used to. Yeah, here. But you can see that basically people are meddling with these

66
00:05:38,560 --> 00:05:43,920
 gradients directly and inline and themselves. It wasn't that common to use an autograd engine.

67
00:05:43,920 --> 00:05:49,200
 Here's one more example from a paper of mine from 2014 called the fragment embeddings.

68
00:05:49,920 --> 00:05:54,960
 And here what I was doing is I was aligning images and text. And so it's kind of like a clip if

69
00:05:54,960 --> 00:05:59,360
 you're familiar with it. But instead of working on the level of entire images and entire sentences,

70
00:05:59,360 --> 00:06:03,920
 it was working on the level of individual objects and little pieces of sentences. And I was embedding

71
00:06:03,920 --> 00:06:09,280
 them and then calculating a very much like a clip like loss. And I dug up the code from 2014

72
00:06:09,280 --> 00:06:15,680
 of how I implemented this. And it was already in numpy and Python. And here I'm implementing the

73
00:06:15,680 --> 00:06:20,880
 cost function. And it was standards to implement not just the cost, but also the backward pass

74
00:06:20,880 --> 00:06:26,400
 manually. So here I'm calculating the image embeddings, sentence embeddings, the last function

75
00:06:26,400 --> 00:06:32,000
 I calculate this course. This is the last function. And then once I have the last function,

76
00:06:32,000 --> 00:06:36,880
 I do the backward pass right here. So I backward through the loss function and through the neural

77
00:06:36,880 --> 00:06:42,640
 net and I append regularization. So everything was done by hand manually. And you were just right

78
00:06:42,640 --> 00:06:46,960
 out the backward pass. And then you would use a gradient checker to make sure that your numerical

79
00:06:46,960 --> 00:06:51,280
 estimate of the gradient agrees with the one you calculated during back propagation. So this

80
00:06:51,280 --> 00:06:55,520
 was very standard for a long time. But today, of course, it is standard to use an autograph engine.

81
00:06:55,520 --> 00:07:00,560
 But it was definitely useful. And I think people sort of understood how these neural networks work

82
00:07:00,560 --> 00:07:04,720
 on a very intuitive level. And so I think it's a good exercise again. And this is where we want to

83
00:07:04,720 --> 00:07:09,040
 be. Okay, so just as a reminder from our previous lecture, this is the Jupyter Notebook that we

84
00:07:09,040 --> 00:07:14,320
 implemented at the time. And we're going to keep everything the same. So we're still going to have

85
00:07:14,320 --> 00:07:18,880
 a two layer multialing perception with a batch normalization layer. So the forward pass will be

86
00:07:18,880 --> 00:07:23,280
 basically identical to this lecture. But here we're going to get rid of loss.backward. And instead,

87
00:07:23,280 --> 00:07:27,760
 we're going to write the backward pass manually. Now here's the starter code for this lecture.

88
00:07:27,760 --> 00:07:34,400
 We are becoming a back prop ninja in this notebook. And the first few cells here are identical to

89
00:07:34,400 --> 00:07:39,680
 what we are used to. So we are doing some imports loading the data set and processing the data set.

90
00:07:39,680 --> 00:07:44,640
 None of this changed. Now here, I'm introducing a utility function that we're going to use later

91
00:07:44,640 --> 00:07:48,640
 to compare the gradients. So in particular, we are going to have the gradients that we estimate

92
00:07:48,640 --> 00:07:53,840
 manually ourselves. And we're going to have gradients that PyTorch calculates. And we're going to be

93
00:07:53,840 --> 00:07:59,520
 checking for correctness, assuming of course that PyTorch is correct. Then here, we have the

94
00:07:59,520 --> 00:08:04,720
 initialization that we are quite used to. So we have our embedding table for the characters,

95
00:08:04,720 --> 00:08:09,600
 the first layer, second layer, and a batch normalization in between. And here's where we create all the

96
00:08:09,600 --> 00:08:15,280
 parameters. Now you will note that I changed the initialization a little bit to be small numbers.

97
00:08:15,280 --> 00:08:19,840
 So normally you would set the biases to be all zero. Here I am setting them to be small random

98
00:08:19,840 --> 00:08:25,200
 numbers. And I'm doing this because if your variables are initialized to exactly zero,

99
00:08:25,200 --> 00:08:29,120
 sometimes what can happen is that can mask an incorrect implementation of a gradient.

100
00:08:29,120 --> 00:08:34,640
 Because when everything is zero, it sort of simplifies and gives you a much simpler expression

101
00:08:34,640 --> 00:08:39,200
 of the gradient than you would otherwise get. And so by making it small numbers, I'm trying to

102
00:08:39,200 --> 00:08:46,640
 unmask those potential errors in these calculations. You also notice that I'm using B1 in the first

103
00:08:46,640 --> 00:08:52,480
 layer. I'm using a bias despite batch normalization right afterwards. So this would typically not be

104
00:08:52,480 --> 00:08:56,400
 what you do because we talked about the fact that you don't need a bias. But I'm doing this here

105
00:08:56,400 --> 00:09:00,720
 just for fun, because we're going to have a gradient with respect to it. And we can check that we are

106
00:09:00,720 --> 00:09:06,800
 still calculating it correctly, even though this bias is asparious. So here I'm calculating a single

107
00:09:06,800 --> 00:09:12,320
 batch. And then here I am doing a forward pass. Now you'll notice that the forward pass is significantly

108
00:09:12,320 --> 00:09:19,040
 expanded from what we are used to. Here the forward pass was just here. Now the reason that the forward

109
00:09:19,040 --> 00:09:24,560
 pass is longer is for two reasons. Number one here, we just had an F dot cross entropy. But here I

110
00:09:24,560 --> 00:09:30,960
 am bringing back a explicit implementation the loss function. And number two, I've broken up the

111
00:09:30,960 --> 00:09:37,040
 implementation into manageable chunks. So we have a lot, a lot more intermediate tensors along the way

112
00:09:37,040 --> 00:09:41,440
 in the forward pass. And that's because we are about to go backwards and calculate the gradients

113
00:09:41,440 --> 00:09:48,240
 in this back propagation from the bottom to the top. So we're going to go upwards. And just like

114
00:09:48,240 --> 00:09:52,400
 we have, for example, the lock props tensor in a forward pass, in a backward pass, we're going to

115
00:09:52,400 --> 00:09:56,480
 have a D lock props, which is going to store the derivative of the loss with respect to the lock

116
00:09:56,480 --> 00:10:02,240
 props tensor. And so we're going to be pretending D to every one of these tensors and calculating it

117
00:10:02,240 --> 00:10:08,000
 along the way of this back propagation. So as an example, we have a B and raw here, we're going to

118
00:10:08,000 --> 00:10:14,240
 be calculating a DB and raw. So here I'm telling PyTorch that we want to retain the grad of all

119
00:10:14,240 --> 00:10:19,520
 these intermediate values, because here in exercise one, we're going to calculate the backward pass.

120
00:10:19,520 --> 00:10:25,440
 So we're going to calculate all these D variables and use the CMP function I've introduced above

121
00:10:25,440 --> 00:10:29,920
 to check our correctness with respect to what PyTorch is telling us. This is going to be

122
00:10:29,920 --> 00:10:35,920
 exercise one where we sort of back propagate through this entire graph. Now, just to give you a very

123
00:10:35,920 --> 00:10:41,440
 quick preview of what's going to happen in exercise two and below, here we have fully broken up the

124
00:10:41,440 --> 00:10:47,040
 loss and back propagated through it manually in all the little atomic pieces that make it up.

125
00:10:47,040 --> 00:10:52,160
 But here we're going to collapse the loss into a single cross entropy call. And instead,

126
00:10:52,160 --> 00:10:58,960
 we're going to analytically derive using math and paper and pencil, the gradient of the loss with

127
00:10:58,960 --> 00:11:03,600
 respect to the logits. And instead of back propagating through all of its little chunks one at a time,

128
00:11:03,600 --> 00:11:07,440
 we're just going to analytically drive what that gradient is. And we're going to implement that,

129
00:11:07,440 --> 00:11:12,560
 which is much more efficient, as we'll see in a bit. Then we're going to do the exact same thing

130
00:11:12,560 --> 00:11:17,600
 for batch normalization. So instead of breaking up batch norm into all the tiny components,

131
00:11:17,600 --> 00:11:23,360
 we're going to use a pen and paper and mathematics and calculus to derive the gradient through the

132
00:11:23,360 --> 00:11:27,360
 batch, the batch norm layer. So we're going to calculate the backward pass through batch

133
00:11:27,360 --> 00:11:31,440
 norm layer in a much more efficient expression, instead of back propagating through all of its

134
00:11:31,440 --> 00:11:37,360
 little pieces independently. So it's going to be exercise three. And then exercise four,

135
00:11:37,360 --> 00:11:42,320
 we're going to put it all together. And this is the full code of training, this two layer MLP.

136
00:11:42,320 --> 00:11:46,320
 And we're going to basically insert our manual backdrop. We're going to take out

137
00:11:46,320 --> 00:11:52,480
 loss the backward. And you will basically see that you can get all the same results using

138
00:11:52,480 --> 00:11:58,960
 fully your own code. And the only thing we're using from PyTorch is the torch.tensor to make

139
00:11:58,960 --> 00:12:03,520
 the calculations efficient. But otherwise, you will understand fully what needs to forward and

140
00:12:03,520 --> 00:12:08,880
 backward in your own and train it. And I think that'll be awesome. So let's get to it. Okay, so I ran

141
00:12:08,880 --> 00:12:14,320
 all the cells of this notebook all the way up to here. And I'm going to erase this. And I'm going

142
00:12:14,320 --> 00:12:18,960
 to start implementing backward pass, starting with the lock problems. So we want to understand

143
00:12:18,960 --> 00:12:23,440
 what should go here to calculate the gradient of the loss with respect to all the elements of the

144
00:12:23,440 --> 00:12:28,480
 lockprops tensor. Now, I'm going to give away the answer here. But I want to put a quick note here

145
00:12:28,480 --> 00:12:34,160
 that I think would be most pedagogically useful for you is to actually go into the description of

146
00:12:34,160 --> 00:12:38,640
 this video and find the link to this Jupyter notebook. You can find it both on GitHub, but you can

147
00:12:38,640 --> 00:12:42,640
 also find Google collab with it. So you don't have to install anything, you'll just go to a website

148
00:12:42,640 --> 00:12:48,960
 on Google collab. And you can try to implement these derivatives or gradients yourself. And then

149
00:12:48,960 --> 00:12:54,320
 if you are not able to come to my video and see me do it. And so work in tandem and try to first

150
00:12:54,320 --> 00:12:59,120
 yourself and then see me give away the answer. And I think that would be most valuable to you. And

151
00:12:59,120 --> 00:13:03,920
 that's how I recommend you go through this lecture. So we are starting here with the lockprops. Now,

152
00:13:03,920 --> 00:13:10,240
 the lockprops will hold the derivative of the loss with respect to all the elements of lockprops.

153
00:13:10,240 --> 00:13:18,560
 What is inside lockprops? The shape of this is 32 by 27. So it's not going to surprise you that

154
00:13:18,560 --> 00:13:23,440
 the lockprops should also be an array of size 32 by 27. Because we want the derivative of the loss

155
00:13:23,440 --> 00:13:27,760
 with respect to all of its elements. So the sizes of those are always going to be equal.

156
00:13:27,760 --> 00:13:38,000
 Now, how does lockprops influence the loss? Okay, loss is negative lockprops indexed

157
00:13:38,000 --> 00:13:46,560
 with range of n and yb and then the mean of that. Now, just as a reminder, yb is just basically an

158
00:13:46,560 --> 00:13:53,920
 array of all the correct indices. So what we're doing here is we're taking the lockprops array of

159
00:13:53,920 --> 00:14:03,120
 size 32 by 27. Right. And then we are going in every single row. And in each row, we are plugging

160
00:14:03,120 --> 00:14:08,640
 plugging out the index eight and then 14 and 15 and so on. So we're going down the rows,

161
00:14:08,640 --> 00:14:14,160
 that's the iterator range of n. And then we are always plugging out the index of the column

162
00:14:14,160 --> 00:14:20,240
 specified by this tensor yb. So in the zero throw, we are taking the eighth column. In the first row,

163
00:14:20,240 --> 00:14:27,040
 we're taking the 14th column, etc. And so lockprops at this plucks out all those

164
00:14:27,040 --> 00:14:34,320
 lock probabilities of the correct next character in a sequence. So that's what that does. And the

165
00:14:34,320 --> 00:14:41,520
 shape of this or the size of it is of course 32, because our batch size is 32. So these elements

166
00:14:41,520 --> 00:14:48,800
 get plucked out and then their mean and the negative of that becomes loss. So I always like to work

167
00:14:48,800 --> 00:14:55,520
 with simpler examples to understand the numerical form of derivative. What's going on here is once

168
00:14:55,520 --> 00:15:02,080
 we've plucked out these examples, we're taking the mean and then the negative. So the loss basically,

169
00:15:02,080 --> 00:15:08,960
 if I can write it this way is the negative of say a plus b plus c. And the mean of those three

170
00:15:08,960 --> 00:15:13,360
 numbers would be say negative with divide three. That would be how we achieve the mean of three

171
00:15:13,360 --> 00:15:19,440
 numbers a, b, c, although we actually have 32 numbers here. And so what is basically the loss

172
00:15:19,440 --> 00:15:25,840
 by say like dA, right? Well, if we simplify this expression mathematically, this is negative 1

173
00:15:25,840 --> 00:15:33,760
 over 3 of a and negative plus negative 1 over 3 of b plus negative 1 over 3 of c. And so what is

174
00:15:33,760 --> 00:15:39,360
 d loss by dA? It's just negative 1 over 3. And so you can see that if we don't just have a b and c,

175
00:15:39,360 --> 00:15:46,160
 but we have 32 numbers, then d loss by d, you know, every one of those numbers is going to be 1 over

176
00:15:46,160 --> 00:15:57,680
 n more generally, because n is the size of the batch 32 in this case. So d loss by d lockprobs

177
00:15:57,680 --> 00:16:04,720
 is negative 1 over n in all these places. Now, what about the other elements inside lockprobs?

178
00:16:04,720 --> 00:16:11,040
 Because lockprobs is large array. You see that lockprobs are checked is 32 by 27, but only 32 of

179
00:16:11,040 --> 00:16:17,040
 them participate in the loss calculation. So what's the derivative of all the other most of the

180
00:16:17,040 --> 00:16:22,800
 elements that do not get plucked out here? Well, their loss intuitively is zero. Sorry,

181
00:16:22,800 --> 00:16:26,720
 their gradient intuitively is zero. And that's because they did not participate in the loss.

182
00:16:26,720 --> 00:16:32,960
 So most of these numbers inside this tensor does not feed into the loss. And so if we were to change

183
00:16:32,960 --> 00:16:38,560
 these numbers, then the loss doesn't change, which is the equivalent of saying that the

184
00:16:38,560 --> 00:16:44,800
 rate of the loss with respect to them is zero. They don't impact it. So here's a way to implement

185
00:16:44,800 --> 00:16:51,040
 this derivative then. We start out with torched at zeros of shape 32 by 27, or let's just say,

186
00:16:51,600 --> 00:16:55,840
 instead of doing this, because we don't want to hardcode numbers, let's do torched at zeros

187
00:16:55,840 --> 00:17:00,960
 like lockprobs. So basically, this is going to create an array of zeros exactly in the shape of

188
00:17:00,960 --> 00:17:07,840
 lockprobs. And then we need to set the derivative of negative 1 over n inside exactly these locations.

189
00:17:07,840 --> 00:17:12,880
 So here's what we can do. The lockprobs indexed in the identical way

190
00:17:12,880 --> 00:17:17,600
 will be just set to negative 1 over 0 divide n,

191
00:17:19,760 --> 00:17:26,320
 right, just like we derived here. So now let me erase all of these reasoning. And then this is

192
00:17:26,320 --> 00:17:32,240
 the candidate derivative for the lockprobs. Let's uncomment the first line and check that this is correct.

193
00:17:32,240 --> 00:17:42,000
 Okay, so CMP ran and let's go back to CMP. And you see that what it's doing is it's calculating if

194
00:17:42,000 --> 00:17:48,400
 the calculated value by us, which is dt, is exactly equal to t dot grad as calculated by

195
00:17:48,400 --> 00:17:53,120
 pytorch. And then this is making sure that all the elements are exactly equal,

196
00:17:53,120 --> 00:17:58,000
 and then converting this to a single Boolean value, because we don't want to Boolean tensor,

197
00:17:58,000 --> 00:18:03,280
 we just want to Boolean value. And then here, we are making sure that, okay, if they're not

198
00:18:03,280 --> 00:18:07,040
 exactly equal, maybe they are approximately equal because of some floating point issues,

199
00:18:07,040 --> 00:18:12,480
 but they're very, very close. So here we are using torched at all close, which has a little bit of a

200
00:18:12,480 --> 00:18:17,760
 wiggle available, because sometimes you can get very, very close. But if you use a slightly

201
00:18:17,760 --> 00:18:23,760
 different calculation, because of floating point arithmetic, you can get a slightly different result.

202
00:18:23,760 --> 00:18:28,480
 So this is checking if you get an approximately close result. And then here we are checking the

203
00:18:28,480 --> 00:18:35,120
 maximum, basically the value that has the highest difference, and what is the difference, and the

204
00:18:35,120 --> 00:18:40,000
 absolute value difference between those two. And so we are printing whether we have an exact equality,

205
00:18:40,000 --> 00:18:47,760
 an approximate equality, and what is the largest difference. And so here, we see that we actually

206
00:18:47,760 --> 00:18:52,720
 have exact equality. And so therefore, of course, we also have an approximate equality. And the

207
00:18:52,720 --> 00:18:59,440
 maximum difference is exactly zero. So basically, our delock props is exactly equal to what pytorch

208
00:18:59,440 --> 00:19:05,520
 calculated to be log props dot grad in its back propagation. So so far, we're doing pretty well.

209
00:19:06,160 --> 00:19:11,360
 Okay, so let's now continue our back propagation. We have that log props depends on props through

210
00:19:11,360 --> 00:19:18,720
 a log. So all the elements of props are being element wise applied log two. Now if we want deep

211
00:19:18,720 --> 00:19:25,120
 props, then remember your micro graph training, we have like a log node, it takes in props and

212
00:19:25,120 --> 00:19:31,440
 creates log props. And deep props will be the local derivative of that individual operation log

213
00:19:32,080 --> 00:19:36,800
 times the derivative loss with respect to its output, which in this case is delock props.

214
00:19:36,800 --> 00:19:42,160
 So what is the local derivative of this operation? Well, we are taking log element wise,

215
00:19:42,160 --> 00:19:46,720
 and we can come here and we can see, well, from all five's your friend, that d by dx of log of x

216
00:19:46,720 --> 00:19:54,320
 is just simply one over x. So therefore, in this case, x is problems. So we have d by dx is one over

217
00:19:54,320 --> 00:19:59,040
 x, which is one of our problems. And then this is the local derivative. And then times, we want to

218
00:19:59,040 --> 00:20:05,680
 chain it. So this is chain rule, time to do log props. Then let me uncomment this and let me run

219
00:20:05,680 --> 00:20:11,680
 the cell in place. And we see that the derivative of props as we calculated here is exactly correct.

220
00:20:11,680 --> 00:20:18,480
 And so notice here how this works, props that are props is going to be inverted and then

221
00:20:18,480 --> 00:20:24,640
 element wise, multiplied here. So if your props is very, very close to one, that means your network

222
00:20:24,640 --> 00:20:29,280
 is currently predicting the character correctly, then this will become one over one and delock

223
00:20:29,280 --> 00:20:35,200
 props is just passed through. But if your probabilities are incorrectly assigned, so if the correct

224
00:20:35,200 --> 00:20:43,680
 character here is getting a very low probability, then 1.0 dividing by it will boost this and then

225
00:20:43,680 --> 00:20:48,480
 multiply by the all props. So basically, what this line is doing intuitively is it's taking the

226
00:20:48,480 --> 00:20:53,200
 examples that have a very low probability currently assigned, and it's boosting their gradient.

227
00:20:54,080 --> 00:21:01,520
 You can again look at it that way. Next up is count some imp. So we want the river of this.

228
00:21:01,520 --> 00:21:06,960
 Now let me just pause here and kind of introduce what's happening here in general, because I know

229
00:21:06,960 --> 00:21:11,360
 it's a little bit confusing. We have the logis that come out of the neural net. Here what I'm doing

230
00:21:11,360 --> 00:21:17,440
 is I'm finding the maximum in each row, and I'm subtracting it for the purpose of numerical stability.

231
00:21:17,440 --> 00:21:22,000
 And we talked about how if you do not do this, you run these numerical issues of some of the logits

232
00:21:22,000 --> 00:21:29,120
 take on two large values, because we end up exponentiating them. So this is done just for safety numerically.

233
00:21:29,120 --> 00:21:34,400
 Then here's the exponentiation of all the sort of like logits to create our counts.

234
00:21:34,400 --> 00:21:40,880
 And then we want to take the sum of these counts and normalize so that all the props sum to one.

235
00:21:40,880 --> 00:21:46,560
 Now here, instead of using one over count sum, I use raise to the power of negative one.

236
00:21:46,560 --> 00:21:49,840
 Mathematically, they are identical. I just found that there's something wrong with the

237
00:21:49,840 --> 00:21:55,600
 pytorch implementation of the backward pass of division. And it gives like a real result,

238
00:21:55,600 --> 00:22:01,440
 but that doesn't happen for star star negative one. So I'm using this formula instead. But basically,

239
00:22:01,440 --> 00:22:05,760
 all that's happening here is we got the logits, we want to exponentiate all of them, and we want

240
00:22:05,760 --> 00:22:11,200
 to normalize the counts to create our probabilities. It's just that it's happening across multiple lines.

241
00:22:12,560 --> 00:22:22,240
 So now here, we want to first take the derivative, we want to back propagate into counts a minute,

242
00:22:22,240 --> 00:22:29,280
 and then into counts as well. So what should be the count sum? Now we actually have to be careful

243
00:22:29,280 --> 00:22:34,800
 here, because we have to scrutinize and be careful with the shapes. So counts that shape,

244
00:22:35,520 --> 00:22:43,360
 and then counts some in that shape are different. So in particular, counts as 32 by 27, but this

245
00:22:43,360 --> 00:22:49,280
 counts some in is 32 by one. And so in this multiplication here, we also have an implicit

246
00:22:49,280 --> 00:22:54,720
 broadcasting that pytorch will do, because it needs to take this column tensor of 32 numbers

247
00:22:54,720 --> 00:23:00,000
 and replicate it horizontally 27 times to align these two tensors so it can do an element

248
00:23:00,000 --> 00:23:05,120
 twice multiply. So really what this looks like is the following using a toy example again.

249
00:23:05,120 --> 00:23:11,040
 What we really have here is just props is counts times consummables. So it's a equals a times b,

250
00:23:11,040 --> 00:23:17,760
 but a is three by three, and b is just three by one, a column tensor. And so pytorch internally

251
00:23:17,760 --> 00:23:23,760
 replicated this elements of b, and it did that across all the columns. So for example, b1,

252
00:23:23,760 --> 00:23:28,320
 which is a first element of b, would be replicated here across all the columns in this multiplication.

253
00:23:29,360 --> 00:23:33,040
 And now we're trying to back propagate through this operation to count some in.

254
00:23:33,040 --> 00:23:38,880
 So when we are calculating this derivative, it's important to realize that these two,

255
00:23:38,880 --> 00:23:45,600
 this looks like a single operation, but actually it's two operations applied sequentially. The first

256
00:23:45,600 --> 00:23:51,200
 operation that pytorch did is it took this column tensor and replicated it across all the,

257
00:23:51,200 --> 00:23:56,800
 across all the columns basically 27 times. So that's the first operation, it's a replication.

258
00:23:56,800 --> 00:24:01,680
 And then the second operation is the multiplication. So let's first back wrap through the multiplication.

259
00:24:01,680 --> 00:24:06,960
 If these two arrays were of the same size, and we just have a and b,

260
00:24:06,960 --> 00:24:12,480
 both of them three by three, then how do we, how do we back propagate through a multiplication?

261
00:24:12,480 --> 00:24:17,760
 So if you just have scalars and not tensors, then if you have c equals a times b, then what is

262
00:24:17,760 --> 00:24:23,840
 the root of the of c with respect to b? Well, it's just a. And so that's the local derivative.

263
00:24:24,640 --> 00:24:30,080
 So here in our case, I'm doing the multiplication and back propagate through just the multiplication

264
00:24:30,080 --> 00:24:36,640
 itself, which is element wise, is going to be the local derivative, which in this case is simply

265
00:24:36,640 --> 00:24:43,200
 counts, because counts is the a. So it's the local derivative, and then times because the chain rule

266
00:24:43,200 --> 00:24:50,880
 deprops. So this here is the derivative or the gradient, but with respect to replicated b.

267
00:24:52,080 --> 00:24:57,120
 But we don't have a replicated b, we just have a single b column. So how do we now back propagate

268
00:24:57,120 --> 00:25:03,520
 through the replication? And intuitively, this b one is the same variable, and it's just reused

269
00:25:03,520 --> 00:25:09,680
 multiple times. And so you can look at it as being equivalent to a case wave encountered in

270
00:25:09,680 --> 00:25:14,800
 micrograd. And so here, I'm just pulling out a random graph we used in micrograd. We had an

271
00:25:14,800 --> 00:25:21,920
 example where a single node has its output feeding into two branches of basically the graph.

272
00:25:22,000 --> 00:25:26,320
 Until the last function. And we're talking about how the correct thing to do in the backward pass

273
00:25:26,320 --> 00:25:32,320
 is we need to sum all the gradients that arrive at any one note. So across these different branches,

274
00:25:32,320 --> 00:25:39,040
 the gradients would sum. So if a note is used multiple times, the gradients for all of its uses

275
00:25:39,040 --> 00:25:44,800
 sum during back propagation. So here b one is used multiple times in all these columns. And

276
00:25:44,800 --> 00:25:51,760
 therefore, the right thing to do here is to sum horizontally across all the rows. So to sum

277
00:25:51,760 --> 00:25:58,720
 in dimension one, but we want to retain this dimension so that the so that counts them in,

278
00:25:58,720 --> 00:26:03,680
 and its gradient are going to be exactly the same shape. So we want to make sure that we keep them

279
00:26:03,680 --> 00:26:09,840
 is true. So we don't lose this dimension. And this will make the counts on him be exactly shape 32

280
00:26:09,840 --> 00:26:17,360
 by one. So revealing this comparison as well and running this, we see that we get an exact match.

281
00:26:18,320 --> 00:26:26,960
 So this derivative is exactly correct. And let me erase this. Now let's also back propagate into

282
00:26:26,960 --> 00:26:32,240
 counts, which is the other variable here to create props. So from props to count some

283
00:26:32,240 --> 00:26:37,120
 info, we just did that, let's go into counts as well. So the counts will be

284
00:26:39,840 --> 00:26:49,520
 decouces are a. So dc by d a is just b. So therefore it's count some info. And then times chain rule

285
00:26:49,520 --> 00:27:01,840
 d props. Now, count some info is 32 by one. D props is 32 by 27. So those will broadcast fine

286
00:27:01,840 --> 00:27:08,480
 and will give us decouces. There's no additional summation required here. There will be a broadcasting

287
00:27:08,480 --> 00:27:13,360
 that happens in this multiply here, because count some info needs to be replicated again

288
00:27:13,360 --> 00:27:20,080
 to correctly multiply d props. But that's going to give the correct result. So as far as this single

289
00:27:20,080 --> 00:27:26,160
 operation is concerned, so we've backed up here from props to counts, but we can't actually check

290
00:27:26,160 --> 00:27:33,040
 the derivative of counts. I have it much later on. And the reason for that is because count some info

291
00:27:33,040 --> 00:27:37,920
 depends on counts. And so there's a second branch here that we have to finish, because count some

292
00:27:37,920 --> 00:27:43,200
 info back propagates into count some and count some will back propagate into counts. And so counts

293
00:27:43,200 --> 00:27:48,160
 is a node that is being used twice. It's used right here into props, and it goes through this other

294
00:27:48,160 --> 00:27:53,440
 branch through count some info. So even though we've calculated the first contribution of it,

295
00:27:53,440 --> 00:27:58,320
 we still have to calculate the second contribution of it later. Okay, so we're continuing with this

296
00:27:58,320 --> 00:28:02,720
 branch. We have the derivative for count some info. Now we want the derivative counts some.

297
00:28:03,440 --> 00:28:08,960
 So decount some equals, what is the local derivative of this operation? So this is basically an element

298
00:28:08,960 --> 00:28:14,960
 wise one over counts some. So count some race to the power of negative one is the same as

299
00:28:14,960 --> 00:28:21,440
 one over count some. If we go to Wolfram alpha, we see that x is negative one d by d by d x of it

300
00:28:21,440 --> 00:28:26,880
 is basically negative x to the negative two, right? One negative one over square is the same as

301
00:28:26,880 --> 00:28:34,400
 negative x to the negative two. So decount some here will be local derivative is going to be negative

302
00:28:34,400 --> 00:28:43,120
 counts some to the negative two. That's the local derivative times chain rule, which is

303
00:28:43,120 --> 00:28:51,680
 decount some info. So that's the count some. Let's uncomment this and check that I am correct.

304
00:28:52,320 --> 00:28:58,720
 Okay, so we have perfect equality. And there's no sketching us going on here with any shapes

305
00:28:58,720 --> 00:29:03,440
 because these are of the same shape. Okay, next up we want to back propagate through this line.

306
00:29:03,440 --> 00:29:10,000
 We have that count some is counts that some along the rows. So I wrote out some help here.

307
00:29:10,000 --> 00:29:15,920
 We have to keep in mind that counts of course is 32 by 27 and count some is 32 by one.

308
00:29:16,560 --> 00:29:23,360
 So in this back propagation, we need to take this column of the rudeness and transform it into a

309
00:29:23,360 --> 00:29:29,360
 array of derivatives to the original array. So what is this operation doing? We're taking in

310
00:29:29,360 --> 00:29:33,840
 some kind of an input like say a three by three matrix A and we are summing up the rows into

311
00:29:33,840 --> 00:29:41,840
 column tensor B, B1 B2 B3. That is basically this. So now we have the derivatives of the loss

312
00:29:41,840 --> 00:29:48,160
 with respect to B, all the elements of B. And now we want to derivative loss with respect to all these

313
00:29:48,160 --> 00:29:54,240
 little a's. So how do the B's depend on the a's is basically what we're after? What is the local

314
00:29:54,240 --> 00:30:00,640
 derivative of this operation? Well, we can see here that B1 only depends on these elements here.

315
00:30:00,640 --> 00:30:06,400
 The derivative of B1 with respect to all of these elements down here is zero. But for these

316
00:30:06,400 --> 00:30:14,720
 elements here like a 1 1 a 1 2 etc, the local derivative is one. So Db1 by Da 1 1, for example,

317
00:30:14,720 --> 00:30:20,640
 is one. So it's one one and one. So when we have the derivative of the loss with respect to B1,

318
00:30:20,640 --> 00:30:26,960
 the local derivative of B1 with respect to these inputs is zeros here, but it's one on these guys.

319
00:30:26,960 --> 00:30:36,080
 So in the chain rule, we have the local derivative times sort of the derivative of B1. And so because

320
00:30:36,080 --> 00:30:41,120
 the local derivative is one on these three elements, the local derivative of multiplying the derivative

321
00:30:41,120 --> 00:30:47,360
 of B1 will just be the derivative of B1. And so you can look at it as a router. Basically,

322
00:30:47,360 --> 00:30:52,640
 an addition is a router of gradient. Whatever gradient comes from above, it just gets routed

323
00:30:52,640 --> 00:30:58,080
 equally to all the elements that participate in that addition. So in this case, the derivative of B1

324
00:30:58,080 --> 00:31:04,640
 will just flow equally to the derivative of a 1 1 a 1 2 and a 1 3. So if we have a derivative of

325
00:31:04,640 --> 00:31:11,120
 all the elements of B, and in this column tensor, which is D counts sum that we've calculated just now,

326
00:31:11,120 --> 00:31:18,320
 we basically see that what that amounts to is all of these are now flowing to all these elements of A,

327
00:31:18,320 --> 00:31:24,640
 and they're doing that horizontally. So basically what we want is we want to take the D counts sum

328
00:31:24,640 --> 00:31:31,440
 of size 32 by 1, and we just want to replicate it 27 times horizontally to create 32 by 27 array.

329
00:31:32,240 --> 00:31:36,000
 So there's many ways to implement this operation. You could of course just replicate the tensor.

330
00:31:36,000 --> 00:31:42,560
 But I think maybe one clean one is that D counts is simply torch dot once like

331
00:31:42,560 --> 00:31:52,160
 so just an two dimensional arrays of once in the shape of counts. So 32 by 27 times D counts sum.

332
00:31:52,160 --> 00:31:58,000
 So this way we're letting the broadcasting here basically implement the replication. You can look

333
00:31:58,000 --> 00:32:05,120
 at that way. But then we have to also be careful because D counts was all already calculated.

334
00:32:05,120 --> 00:32:10,080
 We calculated earlier here, and that was just the first branch and we're now finishing the second

335
00:32:10,080 --> 00:32:15,360
 branch. So we need to make sure that these gradients add so plus equals. And then here,

336
00:32:15,360 --> 00:32:23,680
 let's comment out the comparison and let's make sure crossing fingers that we have the correct

337
00:32:23,680 --> 00:32:29,440
 result. So pytorch agrees with us on this gradient as well. Okay, hopefully we're getting a hang

338
00:32:29,440 --> 00:32:35,440
 of this now counts as an element wise exp of norm logits. So now we want D norm logits.

339
00:32:35,440 --> 00:32:40,160
 And because it's an element has operation everything is very simple. What is the local

340
00:32:40,160 --> 00:32:45,840
 derivative of e to the x? It's famously just e to the x. So this is the local derivative.

341
00:32:45,840 --> 00:32:52,640
 That is the local derivative. Now we already calculated it and it's inside counts. So we

342
00:32:52,640 --> 00:32:58,480
 made as well potentially just reuse counts. That is the local derivative times D counts.

343
00:32:58,480 --> 00:33:05,520
 Funny as that looks, the counts time D counts is iterative on the normal logits.

344
00:33:05,520 --> 00:33:10,160
 And now let's erase this and let's verify and it looks good.

345
00:33:10,160 --> 00:33:19,840
 So that's norm logits. Okay, so we are here on this line now, D norm logits. We have that and

346
00:33:19,840 --> 00:33:24,480
 we're trying to calculate the logits and the logit maxes. So back propagating through this line.

347
00:33:24,480 --> 00:33:30,080
 Now we have to be careful here because the shapes again are not the same. And so there's an implicit

348
00:33:30,080 --> 00:33:36,320
 broadcasting happening here. So normal digits has this shape 32 by 27. Logits does as well.

349
00:33:36,320 --> 00:33:44,000
 But logit maxes is only 32 by one. So there's a broadcasting here in the minus. Now here I tried

350
00:33:44,000 --> 00:33:49,920
 to sort of write out a two example again. We basically have that this is our C equals A minus B.

351
00:33:49,920 --> 00:33:54,320
 And we see that because of the shape, these are three by three, but this one is just a column.

352
00:33:54,320 --> 00:34:00,880
 And so for example, every element of C, we have to look at how it came to be. And every element of

353
00:34:00,880 --> 00:34:09,600
 C is just the corresponding element of A minus basically that associated B. So it's very clear

354
00:34:09,600 --> 00:34:16,640
 now that the derivatives of every one of these C's with respect to their inputs are one for the

355
00:34:16,640 --> 00:34:26,080
 corresponding A. And it's a negative one for the corresponding B. And so therefore, the derivatives

356
00:34:26,080 --> 00:34:32,800
 on the C will flow equally to the corresponding A's. And then also to the corresponding B's.

357
00:34:32,800 --> 00:34:37,760
 But then in addition to that, the B's are broadcast. So we'll have to do the additional sum just like

358
00:34:37,760 --> 00:34:43,760
 we did before. And of course, derivatives for B's will undergo a minus because the local derivative

359
00:34:43,760 --> 00:34:51,760
 here is negative one. So DC 32 by D, B three is negative one. So let's just implement that.

360
00:34:51,760 --> 00:35:01,920
 Basically, D logits will be exactly copying the derivative on normal digits. So D logits equals

361
00:35:01,920 --> 00:35:07,360
 D normal logits, and I'll do a dot clone for safety. So we're just making a copy. And then we have

362
00:35:07,360 --> 00:35:14,880
 that D logit maxis will be the negative of D normal logits, because of the negative sign.

363
00:35:14,880 --> 00:35:23,200
 And then we have to be careful because logit maxis is a column. And so just like we saw before,

364
00:35:23,200 --> 00:35:30,480
 because we keep replicating the same elements across all the columns, then in the backward pass,

365
00:35:30,480 --> 00:35:36,160
 because we keep reusing this, these are all just like separate branches of use of that one variable.

366
00:35:36,160 --> 00:35:41,520
 And so therefore, we have to do a sum along one would keep them equals true, so that we don't

367
00:35:41,520 --> 00:35:47,120
 destroy this dimension. And then the logit maxis will be the same shape. Now we have to be careful

368
00:35:47,120 --> 00:35:53,280
 because this D logits is not the final D logits. And that's because not only do we get gradient

369
00:35:53,280 --> 00:35:58,720
 signal into logits through here, but the logit maxis is a function of logits. And that's a second

370
00:35:58,720 --> 00:36:04,160
 branch into logits. So this is not yet our final derivative for logits. We will come back later for

371
00:36:04,160 --> 00:36:09,600
 the second branch. For now, the logit maxis is the final derivative. So let me uncomment this

372
00:36:09,600 --> 00:36:17,120
 CMP here. And let's just run this and logit maxis if my torch agrees with us. So that was the

373
00:36:17,120 --> 00:36:23,840
 derivative into through this line. Now, before we move on, I want to pause here briefly, and I want

374
00:36:23,840 --> 00:36:28,800
 to look at these logit maxis and especially their gradients. We've talked previously in the previous

375
00:36:28,800 --> 00:36:33,760
 route lecture that the only reason we're doing this is for the numerical stability of the soft max

376
00:36:33,760 --> 00:36:38,880
 that we are implementing here. And we talked about how if you take these logits for any one of these

377
00:36:38,880 --> 00:36:45,120
 examples, so one row of this logits tensor, if you add or subtract any value equally to all the

378
00:36:45,120 --> 00:36:50,800
 elements, then the value of the problems will be unchanged. You're not changing the soft max.

379
00:36:50,800 --> 00:36:55,760
 The only thing that this is doing is it's making sure that X doesn't overflow. And the reason we're

380
00:36:55,760 --> 00:37:01,840
 using a max is because then we are guaranteed that each row of logits, the highest number is 0. And so

381
00:37:01,840 --> 00:37:11,040
 this will be safe. And so basically what that has repercussions. If it is the case that changing

382
00:37:11,040 --> 00:37:16,320
 logit maxis does not change the props and therefore there's not changed the loss, then the gradient

383
00:37:16,320 --> 00:37:22,480
 on logit maxis should be zero, right? Because saying those two things is the same. So indeed,

384
00:37:22,480 --> 00:37:26,960
 we hope that this is very, very small numbers. Indeed, we hope this is zero. Now, because of

385
00:37:26,960 --> 00:37:32,720
 floating point sort of wonkiness, this doesn't count exactly zero, only in some of the rows it does.

386
00:37:32,720 --> 00:37:37,840
 But we get extremely small values like one in negative nine or 10. And so this is telling us

387
00:37:37,840 --> 00:37:43,760
 that the values of logit maxis are not impacting the loss as they shouldn't. It feels kind of

388
00:37:43,760 --> 00:37:50,160
 weird to back propagate through this branch, honestly, because if you have any implementation of like

389
00:37:50,160 --> 00:37:54,480
 F dot cross entropy and pytorch and you you block together all these elements and you're not doing

390
00:37:54,480 --> 00:37:59,840
 the back propagation piece by piece, then you would probably assume that the derivative through here

391
00:37:59,840 --> 00:38:08,240
 is exactly zero. So you would be sort of skipping this branch because it's only done for numerical

392
00:38:08,240 --> 00:38:13,120
 stability. But it's interesting to see that even if you break up everything into the full atoms and

393
00:38:13,120 --> 00:38:17,520
 you still do the computation as you'd like with respect to numerical stability, the correct thing

394
00:38:17,520 --> 00:38:23,360
 happens and you still get a very, very small gradients here, basically reflecting the fact that the

395
00:38:23,360 --> 00:38:28,800
 values of these do not matter with respect to the final loss. Okay, so let's now continue

396
00:38:28,800 --> 00:38:33,360
 back propagation through this line here. We've just calculated the logit maxis and now we want to

397
00:38:33,360 --> 00:38:38,800
 back prop into logits through this second branch. Now here, of course, we took logits and we took

398
00:38:38,800 --> 00:38:44,560
 the max along all the rows. And then we looked at its values here. Now the way this works is that

399
00:38:44,560 --> 00:38:52,960
 in pytorch, this thing here, the max returns both the values and it returns the indices that we

400
00:38:52,960 --> 00:38:58,320
 wish those values to call them the maximum value. Now in the forward pass, we only used values

401
00:38:58,320 --> 00:39:03,120
 because that's all we needed. But in the backward pass, it's extremely useful to know about where

402
00:39:03,120 --> 00:39:08,240
 those maximum values occurred. And we have the indices at which they occurred. And this will,

403
00:39:08,240 --> 00:39:13,040
 of course, helps us to help us do the back propagation. Because what should the backward pass be here

404
00:39:13,040 --> 00:39:18,240
 in this case? We have the logit tensor, which is 32 by 27. And in each row, we find the maximum

405
00:39:18,240 --> 00:39:23,600
 value. And then that value gets plucked out into logit maxis. And so intuitively,

406
00:39:23,600 --> 00:39:33,600
 basically the derivative flowing through here then should be one times the local derivative is one

407
00:39:33,600 --> 00:39:39,760
 for the appropriate entry that was plucked out. And then times the global derivative of the logit

408
00:39:39,760 --> 00:39:44,400
 maxis. So really what we're doing here, if you think through it, is we need to take the delogit

409
00:39:44,400 --> 00:39:52,000
 maxis, and we need to scatter it to the correct positions in these logits from where the maximum

410
00:39:52,000 --> 00:39:59,120
 values came. And so I came up with one line of code, sort of that does that, let me just

411
00:39:59,120 --> 00:40:03,680
 erase a bunch of stuff here. So the line of, you could do it kind of very similar to what we done

412
00:40:03,680 --> 00:40:10,240
 here, where we create a zeros, and then we populate the correct elements. So we use the indices here,

413
00:40:10,240 --> 00:40:18,240
 and we would set them to be one. But you can also use one hat. So f dot one hat. And then I'm taking

414
00:40:18,240 --> 00:40:24,880
 the logit max over the first dimension dot indices. And I'm telling PyTorch that the

415
00:40:24,880 --> 00:40:32,240
 dimension of every one of these tensors should be 27. And so what this is going to do

416
00:40:34,000 --> 00:40:40,960
 is, okay, I apologize, this is crazy. Be guilty that I am show of this. It's really just an array

417
00:40:40,960 --> 00:40:46,320
 of where the maxis came from in each row. And that element is one, and the old, the other elements

418
00:40:46,320 --> 00:40:52,560
 are zero. So it's a one-hat vector in each row. And these indices are now populating a single one

419
00:40:52,560 --> 00:40:57,520
 in the proper place. And then what I'm doing here is I'm multiplying by the logit maxis.

420
00:40:58,080 --> 00:41:06,160
 And keep in mind that this is a column of 32 by one. And so when I'm doing this times the

421
00:41:06,160 --> 00:41:11,840
 logit maxis, the logit maxis will broadcast, and that column will get replicated. And then the

422
00:41:11,840 --> 00:41:17,520
 element wise multiply will ensure that each of these just gets routed to whichever one of these

423
00:41:17,520 --> 00:41:25,360
 bits is turned on. And so that's another way to implement this kind of an operation. And both of

424
00:41:25,360 --> 00:41:30,640
 these can be used. I just thought I would show an equivalent way to do it. And I'm using plus equals

425
00:41:30,640 --> 00:41:35,680
 because we already calculated the logits here. And this is now the second branch. So let's

426
00:41:35,680 --> 00:41:42,960
 look at logits and make sure that this is correct. And we see that we have exactly the correct answer.

427
00:41:42,960 --> 00:41:49,840
 Next up, we want to continue with logits here. That is an outcome of a matrix multiplication

428
00:41:49,840 --> 00:41:56,960
 and a bias offset in this linear layer. So I've printed out the shapes of all these intermediate

429
00:41:56,960 --> 00:42:05,200
 tensors. We see that logits is of course 32 by 27, as we've just seen. Then the age here is 32 by 64.

430
00:42:05,200 --> 00:42:10,880
 So these are 64 dimensional hidden states. And then this w matrix projects those 64 dimensional

431
00:42:10,880 --> 00:42:17,040
 vectors into 27 dimensions. And then there's a 27 dimensional offset, which is a one dimensional

432
00:42:17,040 --> 00:42:24,720
 vector. Now we should note that this plus here actually broadcasts, because h multiplied by w two

433
00:42:24,720 --> 00:42:31,440
 will give us a 32 by 27. And so then this plus b two is a 27 dimensional vector here.

434
00:42:31,440 --> 00:42:36,320
 Now in the rules of broadcasting, what's going to happen with this bias vector is that this one

435
00:42:36,320 --> 00:42:43,200
 dimensional vector of 27 will get aligned with an padded dimension of one on the left. And it will

436
00:42:43,200 --> 00:42:49,040
 basically become a row vector. And then it will get replicated vertically 32 times to make it 32

437
00:42:49,040 --> 00:42:56,640
 by 27. And then there's an element wise multiply. Now, the question is how do we back propagate from

438
00:42:56,640 --> 00:43:03,120
 logits to the hidden states, the weight matrix w two and the bias b two. And you might think that

439
00:43:03,120 --> 00:43:09,600
 we need to go to some matrix calculus. And then we have to look up the derivative for a matrix

440
00:43:09,600 --> 00:43:13,200
 multiplication. But actually you don't have to do any of that. And you can go back to first

441
00:43:13,200 --> 00:43:18,640
 principles and derive this yourself on a piece of paper. And specifically what I like to do and I

442
00:43:18,640 --> 00:43:24,640
 what I find works well for me is you find a specific small example that you then fully write out. And

443
00:43:24,640 --> 00:43:29,280
 then in process of analyzing how that individual small example works, you will understand a broader

444
00:43:29,280 --> 00:43:35,280
 pattern and you'll be able to generalize and write out the full general formula for how these

445
00:43:35,280 --> 00:43:40,400
 derivatives flow in an expression like this. So let's try that out. So pardon the low budget

446
00:43:40,400 --> 00:43:44,960
 production here, but what I've done here is I'm writing it out on a piece of paper. Really,

447
00:43:44,960 --> 00:43:52,240
 what we are interested in is we have a multiply B plus C. And that creates a D. And we have the

448
00:43:52,240 --> 00:43:55,600
 derivative of the loss with respect to D. And we'd like to know what the derivative of the loss is

449
00:43:55,600 --> 00:44:01,200
 with respect to a B and C. Now these here are a little two dimensional examples of a matrix

450
00:44:01,200 --> 00:44:08,400
 multiplication, two by two times a two by two, plus a two, a vector of just two elements, C one and C

451
00:44:08,400 --> 00:44:16,640
 two gives me a two by two. Now notice here that I have a bias vector here called C. And the bias

452
00:44:16,640 --> 00:44:21,840
 vector C one and C two. But as I described over here, that bias vector will become a row vector in

453
00:44:21,840 --> 00:44:26,320
 the broadcasting and will replicate vertically. So that's what's happening here as well. C one C

454
00:44:26,320 --> 00:44:31,760
 two is replicated vertically. And we see how we have two rows of C one C two as a result.

455
00:44:31,760 --> 00:44:39,040
 So now when I say write it out, I just mean like this, basically break up this matrix multiplication

456
00:44:39,040 --> 00:44:44,560
 into the actual thing that that's going on under the hood. So as a result of matrix multiplication

457
00:44:44,560 --> 00:44:49,760
 and how it works, D one one is the result of a dot product between the first row of A and the

458
00:44:49,760 --> 00:44:59,440
 first column of B. So a one B one one plus a one two B two one plus C one. And so on so forth for

459
00:44:59,440 --> 00:45:03,920
 all the other elements of D. And once you actually write it out, it becomes obvious this is just a

460
00:45:03,920 --> 00:45:10,560
 bunch of multiplies and ads. And we know from micro grad how to differentiate multiplies and ads.

461
00:45:10,560 --> 00:45:15,680
 And so this is not scary anymore. It's not just a matrix multiplication. It's just tedious

462
00:45:15,680 --> 00:45:21,360
 unfortunately. But this is completely tractable. We have DL by D for all of these. And we want

463
00:45:21,360 --> 00:45:26,800
 DL by all these little other variables. So how do we achieve that? And how do we actually get the

464
00:45:26,800 --> 00:45:32,640
 gradients? Okay, so the low budget production continues here. So let's for example derive the

465
00:45:32,640 --> 00:45:38,640
 derivative of the loss with respect to a one one. We see here that a one one occurs twice in our

466
00:45:38,640 --> 00:45:44,240
 simple expression right here right here. And influence is D one one and D one two. So this is so what

467
00:45:44,240 --> 00:45:52,640
 is DL by D a one one? Well, it's DL by D one one times the local derivative of D one one,

468
00:45:52,640 --> 00:45:58,080
 which in this case is just B one one, because that's what's multiplying a one one here. So,

469
00:45:58,080 --> 00:46:04,000
 and likewise here the local derivative of D one two with respect to a one one is just B one two.

470
00:46:04,000 --> 00:46:10,240
 And so B one two will in the chain rule therefore multiply D L by D one two. And then because a one

471
00:46:10,240 --> 00:46:17,280
 one is used both to produce D one one and D one two, we need to add up the contributions of both

472
00:46:17,280 --> 00:46:22,560
 of those sort of chains that are running in parallel. And that's why we get a plus just adding

473
00:46:22,560 --> 00:46:29,200
 up those two, those two contributions. And that gives us DL by D a one one. We can do the exact

474
00:46:29,200 --> 00:46:35,280
 same analysis for the other one for all the other elements of a. And when you simply write it out,

475
00:46:35,280 --> 00:46:42,800
 it's just super simple taking ingredients on, you know, expressions like this. You find that

476
00:46:42,800 --> 00:46:51,200
 this matrix DL by D a that we're after, right, if we just arrange all of them in the same shape as

477
00:46:51,200 --> 00:46:59,040
 A takes. So A is just two by two matrix. So DL by D a here will be also just the same shape

478
00:47:00,240 --> 00:47:06,400
 tensor with the derivatives now. So DL by D a one one, etc. And we see that actually we can

479
00:47:06,400 --> 00:47:13,600
 express what we've written out here as a matrix multiply. And so it just so happens that DL by

480
00:47:13,600 --> 00:47:18,400
 that all of these formulas that we've derived here by taking gradients can actually be expressed

481
00:47:18,400 --> 00:47:22,960
 as a matrix multiplication. And in particular, we see that it is the matrix multiplication of these

482
00:47:22,960 --> 00:47:33,520
 two matrices. So it is the DL by D. And then matrix multiplying B, but B transpose actually. So you

483
00:47:33,520 --> 00:47:40,400
 see that B two one and B one two have changed place. Whereas before we had of course B one one,

484
00:47:40,400 --> 00:47:48,080
 B one two, B two one, B two two. So you see that this other matrix B is transposed. And so basically

485
00:47:48,080 --> 00:47:53,040
 what we have on story short, just by doing very simple reasoning here, by breaking up the expression

486
00:47:53,040 --> 00:48:00,720
 in the case of a very simple example, is that DL by D a is which is this is simply equal to DL by

487
00:48:00,720 --> 00:48:08,960
 Dd matrix multiplied with B transpose. So that is what we have so far. Now we also want the derivative

488
00:48:08,960 --> 00:48:17,120
 with respect to B and C. Now, for B, I'm not actually doing the full derivation because honestly it's

489
00:48:18,000 --> 00:48:23,440
 not deep. It's just annoying. It's exhausting. You can actually do this analysis yourself.

490
00:48:23,440 --> 00:48:27,200
 You'll also find that if you take this these expressions and you differentiate with respect

491
00:48:27,200 --> 00:48:33,360
 to B instead of A, you will find that DL by Db is also a matrix multiplication. In this case,

492
00:48:33,360 --> 00:48:38,320
 you have to take the matrix A and transpose it. And matrix multiply that with DL by Dd.

493
00:48:38,320 --> 00:48:46,560
 And that's what gives you a DL by Db. And then here for the offsets C one and C two,

494
00:48:46,560 --> 00:48:51,280
 if you again just differentiate with respect to C one, you will find an expression like this.

495
00:48:51,280 --> 00:48:58,960
 And C two, an expression like this. And basically you'll find that DL by Dc is simply because they're

496
00:48:58,960 --> 00:49:06,240
 just offsetting these expressions. You just have to take the DL by Dd matrix of the derivatives of D

497
00:49:06,240 --> 00:49:12,480
 and you just have to sum across the columns. And that gives you the derivative for C.

498
00:49:13,440 --> 00:49:20,880
 So long story short, the backward pass of a matrix multiply is a matrix multiply. And instead of

499
00:49:20,880 --> 00:49:26,720
 just like we had D equals A times B plus C in a scalar case, we sort of like arrive at

500
00:49:26,720 --> 00:49:31,760
 something very, very similar, but now with a matrix multiplication instead of a scalar multiplication.

501
00:49:31,760 --> 00:49:42,240
 So the derivative of D with respect to A is DL by Dd matrix multiply B transpose. And here it's

502
00:49:42,240 --> 00:49:48,480
 a transpose multiply DL by Dd. But in both cases, it's a matrix multiplication with the derivative

503
00:49:48,480 --> 00:49:57,440
 and the other term in the multiplication. And for C it is a sum. Now I'll tell you a secret.

504
00:49:57,440 --> 00:50:01,600
 I can never remember the formulas that we just arrived for back propagating information

505
00:50:01,600 --> 00:50:06,080
 multiplication. And I can back propagate through these expressions just fine. And the reason this

506
00:50:06,080 --> 00:50:11,760
 works is because the dimensions have to work out. So let me give you an example. Say I want to

507
00:50:11,760 --> 00:50:19,600
 create DH, then what should DH be number one? I have to know that the shape of DH must be the same

508
00:50:19,600 --> 00:50:25,120
 as the shape of H. And the shape of H is three two by 64. And then the other piece of information I

509
00:50:25,120 --> 00:50:34,880
 know is that DH must be some kind of matrix multiplication of D logits with W two. And D logits is 32 by 27.

510
00:50:35,440 --> 00:50:43,520
 And W two is 64 by 27. There is only a single way to make the shape work out. In this case,

511
00:50:43,520 --> 00:50:49,600
 and it is indeed the correct result. In particular here, H needs to be 32 by 64. The only way to

512
00:50:49,600 --> 00:50:56,640
 achieve that is to take a D logits and matrix multiply it with, you see how I have to take W

513
00:50:56,640 --> 00:51:02,080
 two, but I have to transpose it to make the dimensions work out. So W to transpose. And it's

514
00:51:02,080 --> 00:51:06,720
 the only way to make these two matrix multiply those two pieces to make the shapes work out.

515
00:51:06,720 --> 00:51:13,280
 And that turns out to be the core formula. So if we come here, we want DH, which is DA. And we see

516
00:51:13,280 --> 00:51:21,920
 that DA is DL by DD matrix multiply B transpose. So that's D logits multiply and B is W two. So

517
00:51:21,920 --> 00:51:26,880
 W two transpose, which is exactly what we have here. So there's no need to remember these formulas.

518
00:51:27,520 --> 00:51:36,080
 Similarly, now if I want D W two, well, I know that it must be a matrix multiplication of D logits

519
00:51:36,080 --> 00:51:41,280
 and H. And maybe there's a few transpose, but there's one transpose in there as well. And I don't

520
00:51:41,280 --> 00:51:46,000
 know which way it is. So I have to come to W two. And I see that it's shape is 64 by 27.

521
00:51:46,000 --> 00:51:54,080
 And that has to come from some interest multiplication of these two. And so to get a 64 by 27, I need to

522
00:51:54,080 --> 00:52:03,680
 take H, I need to transpose it. And then I need to matrix multiply it. So that will become 64 by 32.

523
00:52:03,680 --> 00:52:08,800
 And then I need to make sure small by the 32 by 27. And that's going to give me a 64 by 27.

524
00:52:08,800 --> 00:52:12,960
 So I need to make sure small by this with the logits that shape, just like that. That's the only

525
00:52:12,960 --> 00:52:18,560
 way to make the dimensions work out and just use matrix multiplication. And if we come here,

526
00:52:18,560 --> 00:52:24,720
 we see that that's exactly what's here. So a transpose A for us is H, multiply with the logits.

527
00:52:24,720 --> 00:52:35,920
 So that's W two. And then DB two is just the vertical sum. And actually, in the same way,

528
00:52:35,920 --> 00:52:40,160
 there's only one way to make the shapes work out. I don't have to remember that it's a vertical sum

529
00:52:40,160 --> 00:52:45,280
 along the zero axis, because that's the only way that this makes sense, because B two shape is 27.

530
00:52:45,840 --> 00:52:56,080
 So in order to get a D logits here, it's 32 by 27. So knowing that it's just some over D logits in some

531
00:52:56,080 --> 00:53:04,800
 direction, that direction must be zero, because I need to eliminate this dimension. So it's this.

532
00:53:04,800 --> 00:53:11,600
 So this is, so that's kind of like the hacky way. Let me copy paste and delete that. And let me

533
00:53:11,600 --> 00:53:17,760
 swing over here. And this is our backward pass for the linear layer, hopefully. So now let's

534
00:53:17,760 --> 00:53:25,600
 uncomment these three, and we're checking that we got all the three derivatives correct, and run.

535
00:53:25,600 --> 00:53:33,600
 And we see that H, W two and B two are all exactly correct. So we back propagate it through a linear

536
00:53:33,600 --> 00:53:41,520
 layer. Now next up, we have derivative for the H already. And we need to back propagate through

537
00:53:41,520 --> 00:53:48,720
 10 H into H preact. So we want to derive D H preact. And here we have to back propagate through a

538
00:53:48,720 --> 00:53:53,600
 10 H. And we've already done this in micro grad. And we remember that 10 H is a very simple backward

539
00:53:53,600 --> 00:53:59,120
 formula. Now, unfortunately, if I just put in D by DX of 10 H of X into both from alpha, it lets

540
00:53:59,120 --> 00:54:04,320
 us down. It tells us that it's a hyperbolic secant function squared of X. It's not exactly

541
00:54:04,320 --> 00:54:10,400
 helpful. But luckily, Google image search does not let us down. And it gives us the simpler formula.

542
00:54:10,400 --> 00:54:15,760
 And in particular, if you have that A is equal to 10 H of Z, then D A by D Z,

543
00:54:15,760 --> 00:54:22,080
 back propagating through 10 H, is just one minus a square. And take note that one minus a square,

544
00:54:22,080 --> 00:54:29,520
 A here is the output of the 10 H, not the input to the 10 H Z. So the D A by D Z is here formulated

545
00:54:29,520 --> 00:54:34,320
 in terms of the output of that 10 H. And here also in Google image search, we have the full

546
00:54:34,320 --> 00:54:39,520
 derivation. If you want to actually take the actual definition of 10 H and work through the math to

547
00:54:39,520 --> 00:54:46,080
 figure out one minus 10 to the square of Z. So one minus a square is the local derivative.

548
00:54:46,080 --> 00:54:55,360
 In our case, that is one minus the output of 10 H square, which here is H. So it's H square.

549
00:54:55,360 --> 00:55:01,840
 And that is the local derivative. And then times the chain rule, DH. So that is going to be our

550
00:55:01,840 --> 00:55:08,880
 candidate implementation. So if we come here and then uncomment this, let's hope for the best.

551
00:55:08,880 --> 00:55:16,000
 And we have the right answer. Okay, next up, we have BHP Act, and we want to back propagate into

552
00:55:16,000 --> 00:55:21,760
 the gain, the B and raw and the B and bias. So here, this is the Bashtron parameters, B and

553
00:55:21,760 --> 00:55:26,800
 gain in bias inside the Bashtron that take the B and raw that is exact unit Gaussian,

554
00:55:26,800 --> 00:55:33,360
 and they scale it and shift it. And these are the parameters of the Bashtron. Now, here, we have

555
00:55:33,360 --> 00:55:37,440
 a multiplication, but it's worth noting that this multiply is very, very different from this

556
00:55:37,440 --> 00:55:43,280
 matrix multiply here. Matrix multiply our dark products between rows and columns of these matrices

557
00:55:43,280 --> 00:55:48,560
 involved. This is an element twice multiply. So things are quite a bit simpler. Now, we do have

558
00:55:48,560 --> 00:55:53,440
 to be careful with some of the broadcasting happening in this line of code, though. So you see how B and

559
00:55:53,440 --> 00:56:00,720
 gain and B and bias are one by 64, but HP react and B and raw are 32 by 64.

560
00:56:00,720 --> 00:56:05,920
 So we have to be careful with that and make sure that all the shapes work out fine and that the

561
00:56:05,920 --> 00:56:10,480
 broadcasting is correctly back propagated. So in particular, let's start with the B and gain.

562
00:56:10,480 --> 00:56:18,320
 So D, B and gain should be. And here, this is again, element twice multiply. And whenever we have

563
00:56:18,320 --> 00:56:24,080
 A times B equals C, we saw that the local derivative here is just if this is a local derivative is

564
00:56:24,080 --> 00:56:30,400
 just the B, the other one. So the local derivative is just B and raw, and then times chain rule.

565
00:56:30,400 --> 00:56:40,560
 So D, H, P, act. So this is the candidate gradient. Now again, we have to be careful because B and

566
00:56:40,560 --> 00:56:50,960
 gain is of size one by 64. But this here would be 32 by 64. And so the correct thing to do in this

567
00:56:50,960 --> 00:56:56,960
 case, of course, is that B and gain here is a rule vector of 64 numbers. It gets replicated vertically

568
00:56:56,960 --> 00:57:02,960
 in this operation. And so therefore, the correcting to do is to sum because it's being replicated.

569
00:57:02,960 --> 00:57:09,040
 And therefore, all the gradients in each of the rows that are now flowing backwards need to sum

570
00:57:09,040 --> 00:57:17,120
 up to that same tensor B and gain. So if to sum across all the zero, all the examples, basically,

571
00:57:17,120 --> 00:57:22,880
 which is the direction which this gets replicated. And now we have to be also careful because we

572
00:57:22,880 --> 00:57:30,240
 be in game is of shape one by 64. So in fact, I need to keep them as true. Otherwise, I will just

573
00:57:30,240 --> 00:57:37,040
 get 64. Now I don't actually really remember why the B and gain and the B and bias, I made them

574
00:57:37,040 --> 00:57:45,280
 be one by 64. But the biases be one and B two, I just made them be one dimensional vectors.

575
00:57:45,280 --> 00:57:51,520
 They're not two dimensional tensors. So I can't recall exactly why I left the gain and the bias

576
00:57:51,520 --> 00:57:55,440
 as two dimensional. But it doesn't really matter as long as you are consistent and you're keeping

577
00:57:55,440 --> 00:57:59,600
 it the same. So in this case, we want to keep the dimension so that the tensor shapes work.

578
00:58:01,360 --> 00:58:13,680
 Next up, we have B and raw. So D B and raw will be B and gain multiplying D H preact. That's our

579
00:58:13,680 --> 00:58:22,480
 chain rule. Now what about the dimensions of this? We have to be careful, right? So D H preact is

580
00:58:22,480 --> 00:58:30,960
 32 by 64. B and gain is one by 64. So we'll just get replicated and to create this multiplication,

581
00:58:30,960 --> 00:58:35,120
 which is the correct thing, because in a forward pass, it also gets replicated in just the same way.

582
00:58:35,120 --> 00:58:40,160
 So in fact, we don't need the brackets here, we're done. And the shapes are already correct.

583
00:58:40,160 --> 00:58:47,200
 And finally, for the bias, very similar, this bias here is very, very similar to the bias we saw in

584
00:58:47,200 --> 00:58:53,280
 the linear layer. And we see that the gradients from H preact will simply flow into the biases

585
00:58:53,280 --> 00:58:58,960
 and add up, because these are just these are just offsets. And so basically, we want this to be D

586
00:58:58,960 --> 00:59:04,960
 H preact, but it needs to sum along the right dimension. And in this case, similar to the gain,

587
00:59:04,960 --> 00:59:09,840
 we need to sum across the zero dimension, the examples, because of the way that the bias gets

588
00:59:09,840 --> 00:59:16,640
 replicated very quickly. And we also want to have keep them as true. And so this will basically take

589
00:59:16,640 --> 00:59:23,360
 this and sum it up and give us a one by 64. So this is the candidate implementation, it makes all

590
00:59:23,360 --> 00:59:30,560
 the shapes work. Let me bring it up down here. And then let me uncomment these three lines

591
00:59:30,560 --> 00:59:37,040
 to check that we are getting the correct result for all the three tensors. And indeed, we see that

592
00:59:37,040 --> 00:59:42,720
 all of that got back propagated correctly. So now we get to the batch norm layer. We see how here

593
00:59:42,720 --> 00:59:48,160
 being gained and being biased are the parameters. So the back propagation ends. But being raw now

594
00:59:48,160 --> 00:59:53,760
 is the output of the standardization. So here, what I'm doing, of course, is I'm breaking up the

595
00:59:53,760 --> 00:59:58,320
 batch norm into manageable pieces. So we can back propagate through each line individually. But

596
00:59:58,320 --> 01:00:06,880
 basically what's happening is B and mean I is the sum. So this is the B and mean I, I apologize for

597
01:00:06,880 --> 01:00:15,040
 the variable naming. B and diff is X minus mu. B and diff two is X minus mu squared here inside

598
01:00:15,040 --> 01:00:23,360
 the variance. B and var is the variance. So sigma square, this is B and var. And it's basically the

599
01:00:23,360 --> 01:00:31,040
 sum of squares. So this is the X minus mu squared, and then the sum. Now you'll notice one departure

600
01:00:31,040 --> 01:00:38,720
 here. Here it is normalized as one over M, which is number of examples. Here I'm normalizing as one

601
01:00:38,720 --> 01:00:43,680
 over N minus one instead of N. And this is deliberate. And I'll come back to that in a bit when we are

602
01:00:43,680 --> 01:00:49,600
 at this line. It is something called the bestial correction. But this is how I want it in our case.

603
01:00:49,600 --> 01:00:57,840
 B and var in then becomes basically B and var plus epsilon. Epsilon is one negative five. And

604
01:00:57,840 --> 01:01:03,680
 then it's one over square root is the same as raising to the power of negative point five.

605
01:01:03,680 --> 01:01:08,080
 Right? Because point five is squared. And then negative makes it one over square root.

606
01:01:08,800 --> 01:01:15,600
 So B and var M is a one over this denominator here. And then we can see that B and raw, which is the

607
01:01:15,600 --> 01:01:23,840
 X hat here, is equal to the B and diff, the numerator, multiplied by the B and var in.

608
01:01:23,840 --> 01:01:29,600
 And this line here that creates pre H pre act was the last piece we've already back propagated

609
01:01:29,600 --> 01:01:36,080
 through it. So now what we want to do is we are here and we have B and raw. And we have to

610
01:01:36,080 --> 01:01:42,720
 first back propagate into B and diff and B and var in. So now we're here and we have D B and raw.

611
01:01:42,720 --> 01:01:50,000
 And we need to back propagate through this line. Now I've written out the shapes here and indeed

612
01:01:50,000 --> 01:01:55,840
 B and var in is a shape one by 64. So there is a broadcasting happening here that we have to be

613
01:01:55,840 --> 01:02:00,240
 careful with. But it is just an element wise simple multiplication by now we should be pretty

614
01:02:00,240 --> 01:02:07,440
 comfortable with that to get D B and diff. We know that this is just B and var in multiplied with

615
01:02:07,440 --> 01:02:18,400
 D B and raw. And conversely, to get D B and var in, we need to take B and diff and multiply

616
01:02:18,400 --> 01:02:26,400
 that by D B and raw. So this is the candidate. But of course, we need to make sure that broadcasting

617
01:02:26,400 --> 01:02:34,080
 is obeyed. So in particular, B and var and multiplying with D B and raw will be okay and give us 32 by

618
01:02:34,080 --> 01:02:44,640
 64 as we expect. But D B and var in would be taking a 32 by 64, multiplying it by 32 by 64.

619
01:02:44,640 --> 01:02:52,880
 So this is 32 by 64. But of course, D B, this B and var in is only one by 64. So the second line

620
01:02:52,880 --> 01:02:59,600
 here needs a sum across the examples. And because there's this dimension here, we need to make sure

621
01:02:59,600 --> 01:03:07,520
 that keep them history. So this is the candidate. Let's erase this and let's swing down here

622
01:03:07,520 --> 01:03:17,760
 and implement it. And the let's comment out D B and var in and D B and diff. Now, we'll actually

623
01:03:17,760 --> 01:03:24,720
 notice that D B and diff, by the way, is going to be incorrect. So when I run this, B and

624
01:03:24,720 --> 01:03:30,960
 B and var in this correct, B and diff is not correct. And this is actually expected, because

625
01:03:30,960 --> 01:03:37,040
 we're not done with B and diff. So in particular, when we slide here, we see here that B and raw is

626
01:03:37,040 --> 01:03:42,320
 a function of B and diff. But actually B and var of is a function of B and var, which is a function

627
01:03:42,320 --> 01:03:49,840
 of B and D, which is a function of B and diff. So it comes here. So B, D and diff, these variable

628
01:03:49,840 --> 01:03:55,280
 names are crazy. I'm sorry, it branches out into two branches. And we've only done one branch of it.

629
01:03:55,280 --> 01:03:59,120
 We have to continue our back propagation and eventually come back to B and diff. And then we'll

630
01:03:59,120 --> 01:04:04,160
 be able to do a plus equals and get the actual current gradient. For now, it is good to verify

631
01:04:04,160 --> 01:04:09,440
 that CBMP also works. It doesn't just lie to us and tell us that everything is always correct.

632
01:04:09,440 --> 01:04:14,880
 It can in fact detect when your gradient is not correct. So it's that's good to see as well.

633
01:04:14,880 --> 01:04:18,320
 Okay, so now we have the derivative here, and we're trying to back propagate through this line.

634
01:04:18,320 --> 01:04:23,680
 And because we're raising to a power of negative point five, I brought up the power rule. And we

635
01:04:23,680 --> 01:04:29,600
 see that basically we have that the B and var will now be we bring down the exponent. So negative point

636
01:04:29,600 --> 01:04:37,760
 five times x, which is this. And now race to the power of negative point five minus one, which is

637
01:04:37,760 --> 01:04:43,760
 a negative one point five. Now, we would have to also apply a small chain rule here in our head,

638
01:04:43,760 --> 01:04:49,920
 because we need to take further derivative of B and var with respect to this expression here

639
01:04:49,920 --> 01:04:54,400
 inside the bracket. But because this is an element wise operation, and everything is fairly simple,

640
01:04:54,400 --> 01:05:00,160
 that's just one. And so there's nothing to do there. So this is the local derivative. And then

641
01:05:00,160 --> 01:05:04,480
 times the global derivative to create the chain rule. This is just times the B and var.

642
01:05:05,520 --> 01:05:11,920
 So this is our candidate. Let me bring this down. And uncommon to the check.

643
01:05:11,920 --> 01:05:19,520
 And we see that we have the correct result. Now, before we back propagate through the next line,

644
01:05:19,520 --> 01:05:22,640
 I want to briefly talk about the note here, where I'm using the vessels correction,

645
01:05:22,640 --> 01:05:28,640
 dividing by n minus one, instead of dividing by n, when I normalize here, the sum of squares.

646
01:05:28,640 --> 01:05:34,080
 Now, you'll notice that this is the departure from the paper, which uses one over n instead,

647
01:05:34,080 --> 01:05:41,440
 not one over n minus one. There m is rn. And so it turns out that there are two ways of estimating

648
01:05:41,440 --> 01:05:48,400
 variance of an array. One is the biased estimate, which is one over n. And the other one is the

649
01:05:48,400 --> 01:05:53,040
 unbiased estimate, which is one over n minus one. Now, confusingly, in the paper, this is

650
01:05:53,040 --> 01:05:57,680
 not very clearly described. And also it's a detail that kind of matters, I think.

651
01:05:57,680 --> 01:06:03,600
 They are using the biased version train time. But later, when they are talking about the inference,

652
01:06:04,080 --> 01:06:09,360
 they are mentioning that when they do the inference, they are using the unbiased estimate,

653
01:06:09,360 --> 01:06:18,000
 which is the n minus one version in basically for inference. And to calibrate the running mean

654
01:06:18,000 --> 01:06:23,280
 and running variance, basically. And so they actually introduce a train test mismatch,

655
01:06:23,280 --> 01:06:28,240
 where in training, they use the biased version. And in the test time, they use the unbiased version.

656
01:06:28,240 --> 01:06:33,840
 I find this extremely confusing. You can read more about the Bessel's correction and why

657
01:06:33,840 --> 01:06:38,320
 dividing by n minus one gives you a better estimate of the variance. In a case where you have population

658
01:06:38,320 --> 01:06:44,960
 size, or samples for a population, there are very small. And that is indeed the case for us,

659
01:06:44,960 --> 01:06:50,160
 because we are dealing with mini batches. And these mini matches are a small sample of a larger

660
01:06:50,160 --> 01:06:55,520
 population, which is the entire training set. And so it just turns out that if you just estimate

661
01:06:55,520 --> 01:07:00,720
 it using one over n, that actually almost always underestimates the variance. And it is a biased

662
01:07:00,720 --> 01:07:05,360
 estimator. And it is advised that you use the unbiased version and divide by n minus one. And

663
01:07:05,360 --> 01:07:09,680
 you can go through this article here that I liked that actually describes the full reasoning. And

664
01:07:09,680 --> 01:07:15,760
 I'll link it in the video description. Now, when you calculate the torture variance, you'll notice

665
01:07:15,760 --> 01:07:19,680
 that they take the unbiased flag, whether or not you want to divide by n or n minus one.

666
01:07:20,400 --> 01:07:27,120
 Confusingly, they do not mention what the default is for unbiased. But I believe unbiased by default

667
01:07:27,120 --> 01:07:34,320
 is true. I'm not sure why the docs here don't cite that. Now, in the batch number, 1D, the

668
01:07:34,320 --> 01:07:39,280
 documentation again is kind of wrong and confusing. It says that the standard deviation is calculated

669
01:07:39,280 --> 01:07:44,160
 via the biased estimator. But this is actually not exactly right. And people have pointed out that

670
01:07:44,160 --> 01:07:50,000
 it is not right in a number of issues since then, because actually the rabbit hole is deeper. And

671
01:07:50,400 --> 01:07:55,360
 they follow the paper exactly. And they use the biased version for training. But when they're

672
01:07:55,360 --> 01:08:00,000
 estimating the running standard deviation, we are using the unbiased version. So again,

673
01:08:00,000 --> 01:08:06,080
 there's the train test mismatch. So long story short, I'm not a fan of train test discrepancies.

674
01:08:06,080 --> 01:08:11,920
 I basically kind of consider the fact that we use the biased version, the training time, and

675
01:08:11,920 --> 01:08:16,160
 the unbiased test time, I basically consider this to be a bug. And I don't think that there's a good

676
01:08:16,160 --> 01:08:20,800
 reason for that. It's not really, they don't really go into the detail of the reasoning behind it

677
01:08:20,800 --> 01:08:26,480
 in this paper. So that's why I basically prefer to use the bestness correction in my own work.

678
01:08:26,480 --> 01:08:30,560
 Unfortunately, Batchnerone does not take a keyword argument that tells you whether or not

679
01:08:30,560 --> 01:08:35,840
 you want to use the unbiased version or the biased version in both training tests. And so

680
01:08:35,840 --> 01:08:40,240
 therefore anyone using batch normalization, basically in my view has a bit of a bug in the code.

681
01:08:42,080 --> 01:08:47,600
 And this turns out to be much less of a problem if your batch mini batch sizes are a bit larger.

682
01:08:47,600 --> 01:08:52,800
 But still, I just find it kind of unpodable. So maybe someone can explain why this is okay.

683
01:08:52,800 --> 01:08:58,400
 But for now, I prefer to use the unbiased version consistently both during training and at test time.

684
01:08:58,400 --> 01:09:03,360
 And that's why I'm using one over n minus one here. Okay, so let's now actually back propagate

685
01:09:03,360 --> 01:09:09,520
 through this line. So the first thing that I always like to do is I like to scrutinize the

686
01:09:09,520 --> 01:09:15,200
 shapes first. So in particular here, looking at the shapes of what's involved, I see that Bnvar

687
01:09:15,200 --> 01:09:23,840
 shape is one by 64. So it's a row vector and Bn if two dot shape is 32 by 64. So clearly here,

688
01:09:23,840 --> 01:09:32,800
 we're doing a sum over the zero axis to squash the first dimension of the shapes here using a sum.

689
01:09:32,800 --> 01:09:37,520
 So that right away actually hints to me that there will be some kind of a replication or

690
01:09:37,520 --> 01:09:42,080
 broadcasting in the backward pass. And maybe you're noticing the pattern here, but basically,

691
01:09:42,080 --> 01:09:47,840
 anytime you have a sum in the forward pass, that turns into a replication or broadcasting in the

692
01:09:47,840 --> 01:09:53,600
 backward pass along the same dimension. And conversely, when we have a replication or a

693
01:09:53,600 --> 01:09:59,520
 broadcasting in the forward pass, that indicates a variable reuse. And so in the backward pass,

694
01:09:59,520 --> 01:10:04,720
 that turns into a sum over the exact same dimension. And so hopefully you're noticing that duality

695
01:10:04,720 --> 01:10:08,160
 that those two are kind of like the opposite of each other in the forward and backward pass.

696
01:10:08,160 --> 01:10:13,200
 Now, once we understand the shapes, the next thing I like to do always is I like to look at a

697
01:10:13,200 --> 01:10:18,880
 two example in my head to sort of just like understand roughly how the variable dependencies

698
01:10:18,880 --> 01:10:25,520
 go in the mathematical formula. So here, we have a two dimensional array at the end of two,

699
01:10:25,520 --> 01:10:30,960
 which we are scaling by a constant. And then we are summing vertically over the columns.

700
01:10:31,520 --> 01:10:35,600
 So if we have a two by two matrix A, and then we sum over the columns and scale,

701
01:10:35,600 --> 01:10:42,240
 we would get a row vector B1 B2. And B1 depends on A in this way, where it's just some that are

702
01:10:42,240 --> 01:10:50,320
 scaled of A and B2 in this way, where it's the second column, sum and scale. And so looking at

703
01:10:50,320 --> 01:10:55,520
 this basically, what we want to do now is we have the derivatives on B1 and B2, and we want to

704
01:10:55,520 --> 01:10:59,760
 back propagate them into aase. And so it's clear that just differentiating in your head,

705
01:11:00,480 --> 01:11:08,080
 the local derivative here is 1 over n minus 1 times 1 for each one of these a's. And

706
01:11:08,080 --> 01:11:15,600
 basically, the derivative of B1 has to flow through the columns of A scaled by 1 over n minus 1.

707
01:11:15,600 --> 01:11:22,400
 And that's roughly what's happening here. So intuitively, the derivative flow tells us that

708
01:11:22,400 --> 01:11:29,680
 dB and F2 will be the local derivative of this operation. And there are many ways to do this,

709
01:11:29,680 --> 01:11:35,920
 by the way, but I like to do something like this. Torched out one slide of B and F2. So I'll create

710
01:11:35,920 --> 01:11:42,880
 a large array to the measure of ones. And then I will scale it. So 1.0 divided by n minus 1.

711
01:11:42,880 --> 01:11:50,080
 So this is a array of 1 over n minus 1. And that's sort of like the local derivative.

712
01:11:50,080 --> 01:11:55,840
 And now for the chain rule, I will simply just multiply it by dB and R.

713
01:11:58,480 --> 01:12:04,480
 And notice here what's going to happen. This is 32 by 64. And this is just 1 by 64. So I'm letting

714
01:12:04,480 --> 01:12:10,960
 the broadcasting do the replication, because internally in PyTorch, basically dB and R, which

715
01:12:10,960 --> 01:12:18,160
 is 1 by 64 row vector, well, in this multiplication, get copied vertically until the two are of the

716
01:12:18,160 --> 01:12:23,200
 same shape. And then there will be an element wise multiply. And so that the broadcasting is

717
01:12:23,200 --> 01:12:28,000
 basically doing the replication. And I will end up with the derivatives of dB and F2.

718
01:12:28,960 --> 01:12:35,920
 Here. So this is the kentexolution. Let's bring it down here. Let's uncomment this line,

719
01:12:35,920 --> 01:12:41,200
 where we check it. And let's hope for the best. And indeed, we see that this is the

720
01:12:41,200 --> 01:12:47,840
 correct formula. Next up, let's differentiate here into B and F. So here we have that B and F is

721
01:12:47,840 --> 01:12:53,120
 element wise squared to create B and F2. So this is a relatively simple derivative,

722
01:12:53,120 --> 01:12:57,680
 because it's a simple element wise operation. So it's kind of like the scalar case. And we have

723
01:12:57,680 --> 01:13:04,880
 that dB and F should be, if this is x squared, then derivative of this is 2x. Right? So it's simply

724
01:13:04,880 --> 01:13:11,440
 2 times B and F, that's the local derivative. And then times chain rule. And the shape of these

725
01:13:11,440 --> 01:13:18,000
 is the same, they are of the same shape. So times this. So that's the backward pass for this variable.

726
01:13:18,000 --> 01:13:23,040
 Let me bring them down here. And now we have to be careful, because we already calculated dB and

727
01:13:23,040 --> 01:13:30,080
 diff, right? So this is just the end of the other, you know, other branch coming back to B and F,

728
01:13:30,080 --> 01:13:36,880
 because B and F will already back propagate it to way over here, from B and raw. So we now

729
01:13:36,880 --> 01:13:42,080
 completed the second branch. And so that's why I have to do plus equals. And if you recall,

730
01:13:42,080 --> 01:13:46,320
 we had an incorrect derivative for B and F before. And I'm hoping that once we

731
01:13:46,320 --> 01:13:54,480
 append this last missing piece, we have the exact correctness. So let's run. And B and diff now

732
01:13:54,480 --> 01:14:00,240
 actually shows the exact correct derivative. So that's comforting. Okay, so let's now back

733
01:14:00,240 --> 01:14:05,760
 propagate through this line here. The first thing we do, of course, is we check the shapes.

734
01:14:05,760 --> 01:14:11,760
 And I wrote them out here. And basically the shape of this is 32 by 64. Hbbn is the same shape.

735
01:14:12,480 --> 01:14:18,800
 But B and me ny is a row vector one by 64. So this minus here will actually do broadcasting.

736
01:14:18,800 --> 01:14:23,440
 And so we have to be careful with that. And as a hint to us, again, because of the duality,

737
01:14:23,440 --> 01:14:28,560
 a broadcasting in the forward pass means a variable reuse. And therefore there will be a sum in the

738
01:14:28,560 --> 01:14:36,320
 backward pass. So let's write out the backward pass here now. Back propagate into the H pre Bn.

739
01:14:36,320 --> 01:14:41,440
 Because this is these are the same shape, then the local derivative for each one of the elements

740
01:14:41,440 --> 01:14:47,360
 here is just one for the corresponding element in here. So basically what this means is that the

741
01:14:47,360 --> 01:14:53,040
 gradient just simply copies is just a variable assignment, its quality. So I'm just going to

742
01:14:53,040 --> 01:15:00,800
 clone this tensor, just for safety to create an exact copy of D B and diff. And then here to

743
01:15:00,800 --> 01:15:08,400
 back propagate into this one, what I'm inclined to do here is D Bn, me ny will basically be

744
01:15:10,240 --> 01:15:18,400
 what is the local derivative? Well, it's negative torch dot one, like of the shape of B and diff.

745
01:15:18,400 --> 01:15:29,520
 And then times the derivative here, D Bn diff.

746
01:15:29,520 --> 01:15:38,720
 And this here is the back propagation for the replicated B and me ny. So I still have to

747
01:15:38,720 --> 01:15:44,880
 back propagate through the replication and broadcasting. And I do that by doing a sum. So I'm going to

748
01:15:44,880 --> 01:15:50,240
 take this whole thing and I'm going to do a sum over the zero dimension, which was the replication.

749
01:15:50,240 --> 01:15:58,800
 So if you scrutinize this, by the way, you'll notice that this is the same shape as that. And so

750
01:15:58,800 --> 01:16:02,560
 what I'm doing, what I'm doing here doesn't actually make that much sense because it's just a

751
01:16:04,000 --> 01:16:10,960
 array of ones multiplying D P and diff. So in fact, I can just do this. And there's a equivalent.

752
01:16:10,960 --> 01:16:19,840
 So this is the candidate backward pass. Let me copy it here. And then let me comment out this one

753
01:16:19,840 --> 01:16:31,520
 and this one. Enter. And it's wrong. Damn. Actually, sorry, this is supposed to be wrong.

754
01:16:31,520 --> 01:16:37,400
 And it's supposed to be wrong because we are back propagating from a B and diff into H

755
01:16:37,400 --> 01:16:44,080
 pre Bn. And but we're not done because B and me ny depends on H pre Bn. And there will be a second

756
01:16:44,080 --> 01:16:48,560
 portion of that derivative coming from this second branch. So we're not done yet. And we expect it

757
01:16:48,560 --> 01:16:54,400
 to be incorrect. So there you go. So let's not back propagate from B and me ny into H pre Bn.

758
01:16:57,280 --> 01:17:02,320
 And so here again, we have to be careful because there's a broadcasting along or there's a sum

759
01:17:02,320 --> 01:17:06,880
 along the zero dimension. So this will turn into broadcasting in the backward pass now.

760
01:17:06,880 --> 01:17:10,960
 And I'm going to go a little bit faster on this line because it is very similar to the line that

761
01:17:10,960 --> 01:17:21,280
 we had before and multiple eyes in the past. In fact, so D H pre Bn will be the gradient will

762
01:17:21,280 --> 01:17:28,480
 be scaled by one over n. And then basically this gradient here, D Bn, my mean, I is going to be

763
01:17:28,480 --> 01:17:34,000
 scaled by one over n. And then it's going to flow across all the columns and deposit itself into

764
01:17:34,000 --> 01:17:41,120
 D H pre Bn. So what we want is this thing scaled by one over n. We'll put the constant up front here.

765
01:17:45,280 --> 01:17:52,720
 So scaled on the gradient. And now we need to replicate it across all the across all the rows

766
01:17:52,720 --> 01:18:01,840
 here. So we I like to do that by torch dot ones like off basically H pre Bn.

767
01:18:03,840 --> 01:18:18,400
 And I will let broadcasting do the work of replication. So like that. So this is D H pre

768
01:18:18,400 --> 01:18:22,560
 Pn. And hopefully we can plus equals that.

769
01:18:22,560 --> 01:18:32,720
 So this here is broadcasting. And then this is the scaling. So this should be correct.

770
01:18:33,600 --> 01:18:39,120
 Okay. So that completes the back propagation of the bathroom layer. And we are now here.

771
01:18:39,120 --> 01:18:43,920
 Let's back propagate through the linear layer one here. Now, because everything is getting a

772
01:18:43,920 --> 01:18:48,560
 little vertically crazy, I copy pasted the line here. And let's just back propagate through this

773
01:18:48,560 --> 01:18:54,800
 one line. So first, of course, we inspect the shapes and we see that this is 3,2, by 64.

774
01:18:55,440 --> 01:19:06,160
 MCAT is 32 by 30. W1 is 30 30 by 64. And B1 is just 64. So as I mentioned, back propagating

775
01:19:06,160 --> 01:19:11,840
 through linear layers is fairly easy just by matching the shapes. So let's do that. We have that

776
01:19:11,840 --> 01:19:21,600
 D MCAT should be some matrix multiplication of D H pre Bn with W1 and one transpose thrown in there.

777
01:19:22,400 --> 01:19:36,480
 So to make MCAT be 32 by 30, I need to take D H pre Bn 32 by 64 and multiply it by W1.transpose.

778
01:19:36,480 --> 01:19:49,680
 To get D w1, I need to end up with 30 by 64. So to get that, I need to take MCAT transpose

779
01:19:51,280 --> 01:19:59,920
 and multiply that by D H pre Bn. And finally, to get D B1,

780
01:19:59,920 --> 01:20:08,320
 this is a addition. And we saw that basically, I need to just sum the elements in D H pre Bn

781
01:20:08,320 --> 01:20:14,240
 along some dimension. And to make the dimensions work out, I need to sum along the 0th axis here

782
01:20:14,240 --> 01:20:20,800
 to eliminate this dimension. And we do not keep them. So that we want to just get a single

783
01:20:20,800 --> 01:20:28,480
 1 dimensional lecture of 64. So these are the claimed derivatives. Let me put that here and

784
01:20:28,480 --> 01:20:36,400
 let me uncomment three lines and cross our fingers. Everything is great. Okay, so we now continue

785
01:20:36,400 --> 01:20:42,400
 almost there. We have the derivative of MCAT and we want to back propagate into M.

786
01:20:42,400 --> 01:20:49,360
 So I again copied this line over here. So this is the forward pass. And then this is the shapes.

787
01:20:49,920 --> 01:20:56,560
 So remember that the shape here was 32 by 30. And the original shape of M was 32 by 3 by 10.

788
01:20:56,560 --> 01:21:01,440
 So this layer in the forward pass, as you recall, did the concatenation of these three

789
01:21:01,440 --> 01:21:07,920
 10 dimensional character vectors. And so now we just want to undo that. So this is actually

790
01:21:07,920 --> 01:21:12,480
 relatively straightforward operation, because the backward pass of the, what is the view?

791
01:21:12,480 --> 01:21:17,760
 View is just a re-print representation of the array. It's just a logical form of how you interpret

792
01:21:17,760 --> 01:21:23,920
 the array. So let's just reinterpret it to be what it was before. So in other words, the M is not

793
01:21:23,920 --> 01:21:35,920
 32 by 30. It is basically the MCAT. But if you view it as the original shape, so just M.shape.

794
01:21:35,920 --> 01:21:42,400
 You can pass in tuples into view. And so this should just be, okay,

795
01:21:44,640 --> 01:21:50,240
 we just re-represent that view. And then we uncomment this line here. And hopefully,

796
01:21:50,240 --> 01:21:56,960
 yeah, so the derivative of M is correct. So in this case, we just have to re-represent the shape

797
01:21:56,960 --> 01:22:01,760
 of those derivatives into the original view. So now we are at the final line. And the only thing

798
01:22:01,760 --> 01:22:08,560
 that's left to back propagate through is this indexing operation here, M is C at XB. So as I did

799
01:22:08,560 --> 01:22:12,640
 before, I copy pasted this line here. And let's look at the shapes of everything that's involved

800
01:22:12,640 --> 01:22:21,920
 and remind ourselves how this worked. So M.shape was 32 by 3 by 10. So it's 32 examples. And then we

801
01:22:21,920 --> 01:22:28,000
 have three characters. Each one of them has a 10 dimensional embedding. And this was achieved

802
01:22:28,000 --> 01:22:33,760
 by taking the lookup table C, which have 27 possible characters, each of them 10 dimensional.

803
01:22:33,760 --> 01:22:42,480
 And we looked up at the rows that were specified inside this tensor XB. So XB is 32 by 3.

804
01:22:43,040 --> 01:22:47,840
 And it's basically giving us for each example the identity or the index of which character

805
01:22:47,840 --> 01:22:56,320
 is part of that example. And so here I'm showing the first five rows of three of this tensor XB.

806
01:22:56,320 --> 01:23:01,520
 And so we can see that for example, here it was the first example in this batch is that the

807
01:23:01,520 --> 01:23:05,440
 first character and the first character and the fourth character comes into the neural net.

808
01:23:05,440 --> 01:23:10,800
 And then we want to predict the next character in a sequence after the character is 114.

809
01:23:12,240 --> 01:23:18,160
 So basically what's happening here is there are integers inside XB. And each one of these

810
01:23:18,160 --> 01:23:25,120
 integers is specifying which row of C we want to pluck out. Right. And then we arrange those

811
01:23:25,120 --> 01:23:29,920
 rows that we've plucked out into three, two by three by 10 tensor. And we just package them

812
01:23:29,920 --> 01:23:35,200
 in, we just package them into this tensor. And now what's happening is that we have D.

813
01:23:36,240 --> 01:23:42,160
 So for every one of these basically plucked out rows, we have their gradients now,

814
01:23:42,160 --> 01:23:48,000
 but they're arranged inside this 32 by three by 10 tensor. So all we have to do now is we just

815
01:23:48,000 --> 01:23:53,760
 need to route this gradient backwards through this assignment. So we need to find which row of C

816
01:23:53,760 --> 01:24:01,280
 that every one of these tendimational embeddings come from. And then we need to deposit them into D.C.

817
01:24:03,040 --> 01:24:09,440
 So we just need to undo the indexing. And of course, if any of these rows of C was used multiple times,

818
01:24:09,440 --> 01:24:13,360
 which almost certainly is the case, like the row one and one was used multiple times,

819
01:24:13,360 --> 01:24:18,880
 then we have to remember that the gradients that arrive there have to add. So for each occurrence,

820
01:24:18,880 --> 01:24:24,080
 we have to have an addition. So let's now write this out. And I don't actually know of like a

821
01:24:24,080 --> 01:24:29,760
 much better way to do this than a for loop unfortunately in Python. So maybe someone can come up with a

822
01:24:29,760 --> 01:24:35,360
 factorized efficient operation. But for now, let's just use for loops. So let me create a torched

823
01:24:35,360 --> 01:24:45,360
 out zeros like C to initialize just 27 by 10 tensor of all zeros. And then honestly 4k in range

824
01:24:45,360 --> 01:24:52,320
 xb dot shape at zero. Maybe someone has a better way to do this, but for J in range,

825
01:24:52,960 --> 01:25:01,600
 xb dot shape at one. This is going to iterate over all the all the elements of xb, all these

826
01:25:01,600 --> 01:25:10,160
 integers. And then let's get the index at this position. So the index is basically xb at kj.

827
01:25:10,160 --> 01:25:18,560
 So that an example of that is 11 or 14 and so on. And now in the forward pass, we took,

828
01:25:19,520 --> 01:25:31,680
 we basically took the row of C at index, and we deposited it into m at k aj. That's what happened.

829
01:25:31,680 --> 01:25:35,360
 That's where they are packaged. So now we need to go backwards and we just need to route

830
01:25:35,360 --> 01:25:44,640
 dm at the position kj. We now have these derivatives for each position and it's 10

831
01:25:44,640 --> 01:25:53,760
 dimensional. And you just need to go into the correct row of C. So dC rather at ix is this,

832
01:25:53,760 --> 01:25:58,880
 but plus equals, because there could be multiple occurrences, like the same row could have been

833
01:25:58,880 --> 01:26:05,680
 used many, many times. And so all of those derivatives will just go backwards through the indexing

834
01:26:05,680 --> 01:26:14,080
 and they will add. So this is my candidate solution. Let's copy it here.

835
01:26:14,080 --> 01:26:24,480
 Let's uncomment this and cross our fingers. Hey, so that's it. We've back propagated through

836
01:26:24,480 --> 01:26:32,480
 this entire beast. So there we go. Totally makes sense. So now we come to exercise two.

837
01:26:33,120 --> 01:26:36,640
 It basically turns out that in this first exercise, we were doing way too much work.

838
01:26:36,640 --> 01:26:40,880
 We were back propagating way too much. And it was all good practice and so on,

839
01:26:40,880 --> 01:26:44,880
 but it's not what you would do in practice. And the reason for that is, for example,

840
01:26:44,880 --> 01:26:49,760
 here I separated out this loss calculation over multiple lines and I broke it up all,

841
01:26:49,760 --> 01:26:54,480
 all two, like its smallest atomic pieces and we back propagated through all of those individually.

842
01:26:54,480 --> 01:26:58,400
 But it turns out that if you just look at the mathematical expression for the loss,

843
01:26:59,920 --> 01:27:05,120
 then actually you can do the differentiation on pen and paper and a lot of terms cancel and

844
01:27:05,120 --> 01:27:09,600
 simplify. And the mathematical expression you end up with can be significantly shorter and

845
01:27:09,600 --> 01:27:13,200
 easier to implement than back propagating through all the little pieces of everything you've done.

846
01:27:13,200 --> 01:27:18,480
 So before we had this complicated forward pass, going from logits to the loss,

847
01:27:18,480 --> 01:27:23,920
 but in PyTorch, everything can just be glued together into a single call f dot cross entropy.

848
01:27:23,920 --> 01:27:28,480
 You just pass in logits and the labels and you get the exact same loss as I verify here.

849
01:27:28,480 --> 01:27:33,760
 So our previous loss and the fast loss coming from the chunk of operations as a single mathematical

850
01:27:33,760 --> 01:27:39,840
 expression is the same, but it's much, much faster and forward pass. It's also much, much faster

851
01:27:39,840 --> 01:27:43,760
 and backward pass. And the reason for that is if you just look at the mathematical form of this

852
01:27:43,760 --> 01:27:48,320
 and differentiate again, you will end up with a very small and short expression. So that's what

853
01:27:48,320 --> 01:27:54,000
 we want to do here. We want to in a single operation or in a single go, or like very quickly,

854
01:27:54,000 --> 01:27:59,680
 go directly into delogits. And we need to implement delogits as a function of

855
01:27:59,680 --> 01:28:06,640
 logits and YBs. But it will be significantly shorter than whatever we did here, where to get

856
01:28:06,640 --> 01:28:12,320
 to delogits, we had to go all the way here. So all of this work can be skipped in a much,

857
01:28:12,320 --> 01:28:18,240
 much simpler mathematical expression that you can implement here. So you can give it a shot

858
01:28:18,240 --> 01:28:23,200
 yourself, basically look at what exactly is the mathematical expression of loss

859
01:28:23,200 --> 01:28:29,760
 and differentiate with respect to the logits. So let me show you a hint. You can of course try

860
01:28:29,760 --> 01:28:34,240
 it fully yourself. But if not, I can give you some hint of how to get started mathematically.

861
01:28:34,240 --> 01:28:41,120
 So basically what's happening here is we have logits, then there's the softmax that takes the

862
01:28:41,120 --> 01:28:46,640
 logits and gives you probabilities. Then we are using the identity of the correct next character

863
01:28:46,640 --> 01:28:52,640
 to pluck out a row of probabilities, take the negative log of it to get our negative log probability.

864
01:28:53,120 --> 01:28:58,240
 And then we average up all the log probabilities or negative log probabilities to get our loss.

865
01:28:58,240 --> 01:29:05,360
 So basically what we have is for a single individual example, rather, we have that loss is equal to

866
01:29:05,360 --> 01:29:11,520
 negative log probability, where P here is kind of like thought of as a vector of all the probabilities.

867
01:29:11,520 --> 01:29:19,920
 So at the yth position, where y is the label. And we have that P here, of course, is the softmax.

868
01:29:20,480 --> 01:29:27,840
 So the i th component of P of this probability vector is just the softmax function. So raising

869
01:29:27,840 --> 01:29:34,320
 all the logits basically to the power of E and normalizing. So everything comes to one.

870
01:29:34,320 --> 01:29:40,400
 Now if you write out P of y here, you can just write out the softmax. And then basically what

871
01:29:40,400 --> 01:29:46,000
 we're interested in is we're interested in the derivative of the loss with respect to the i th

872
01:29:46,000 --> 01:29:53,920
 logit. And so basically it's a d by dli of this expression here, where we have l indexed with

873
01:29:53,920 --> 01:29:59,040
 the specific label y. And on the bottom, we have a sum over j of e to the lj and the negative

874
01:29:59,040 --> 01:30:03,760
 log of all that. So potentially give it a shot pen and paper and see if you can actually derive

875
01:30:03,760 --> 01:30:09,680
 the expression for the loss by dli. And then we're going to implement it here. Okay, so I am going

876
01:30:09,680 --> 01:30:14,880
 to give away the result here. So this is some of the math I did to derive the gradients

877
01:30:15,760 --> 01:30:20,320
 analytically. And so we see here that I'm just applying the rules of calculus from your first or

878
01:30:20,320 --> 01:30:24,800
 second year of bachelor's degree if you took it. And we see that the expression is actually

879
01:30:24,800 --> 01:30:29,680
 simplified quite a bit. You have to separate out the analysis in the case where the i th index

880
01:30:29,680 --> 01:30:34,720
 that you're interested in inside logits is either equal to the label or it's not equal to the label.

881
01:30:34,720 --> 01:30:39,440
 And then the expression is simplify and cancel in a slightly different way. And what we end up with

882
01:30:39,440 --> 01:30:45,200
 is something very, very simple. We either end up with basically p at i, where p is again this

883
01:30:45,200 --> 01:30:52,080
 vector of probabilities after a softmax or p at i minus one, where we just simply subtract one.

884
01:30:52,080 --> 01:30:57,520
 But in any case, we just need to calculate the softmax p and then in the correct dimension,

885
01:30:57,520 --> 01:31:02,880
 we need to subtract one. And that's the gradient, the form that it takes analytically. So let's

886
01:31:02,880 --> 01:31:06,960
 implement this basically. And we have to keep in mind that this is only done for a single example.

887
01:31:06,960 --> 01:31:12,400
 But here we are working with batches of examples. So we have to be careful of that. And then the

888
01:31:12,400 --> 01:31:17,680
 loss for a batch is the average loss over all the examples. So in other words, it's the example

889
01:31:17,680 --> 01:31:22,960
 for all the individual examples is the loss for each individual example summed up and then divided

890
01:31:22,960 --> 01:31:29,120
 by n. And we have to back propagate through that as well and be careful with it. So d logits is

891
01:31:29,120 --> 01:31:36,560
 going to be f dot softmax. Plattorch has a softmax function that you can call. And we want to apply

892
01:31:36,560 --> 01:31:43,200
 the softmax on the logits. And we want to go in the dimension that is one. So basically, we want to

893
01:31:43,200 --> 01:31:49,520
 do the softmax along the rows of these logits. Then at the correct positions, we need to subtract

894
01:31:49,520 --> 01:31:57,760
 a one. So d logits at iterating over all the rows and indexing into the columns provided by the

895
01:31:57,760 --> 01:32:05,600
 correct labels inside yb. We need to subtract one. And then finally, it's the average loss that is

896
01:32:05,600 --> 01:32:11,440
 the loss. And in the average, there's a one over n of all the losses added up. And so we need to

897
01:32:11,440 --> 01:32:17,520
 also back propagate through that division. So the gradient has to be scaled down by n as well,

898
01:32:17,520 --> 01:32:24,000
 because of the mean. But this otherwise should be the result. So now if we verify this,

899
01:32:24,000 --> 01:32:30,480
 we see that we don't get an exact match. But at the same time, the maximum difference from

900
01:32:31,040 --> 01:32:38,400
 logits from PyTorch and our d logits here is on the order of 5e negative nine. So it's a tiny,

901
01:32:38,400 --> 01:32:44,320
 tiny number. So because of floating point of emptiness, we don't get the exact bitwise result.

902
01:32:44,320 --> 01:32:51,200
 But we basically get the correct answer. Approximately. Now I'd like to pause here briefly before we

903
01:32:51,200 --> 01:32:56,160
 move on to the next exercise, because I'd like us to get an intuitive sense of what d logits is,

904
01:32:56,160 --> 01:33:02,880
 because it has a beautiful and very simple explanation, honestly. So here, I'm taking the logits and

905
01:33:02,880 --> 01:33:09,040
 I'm visualizing it. And we can see that we have a batch of 32 examples of 27 characters. And what

906
01:33:09,040 --> 01:33:14,640
 is the logits intuitively, right? The logits is the probabilities that the probabilities matrix

907
01:33:14,640 --> 01:33:19,120
 in a forward pass. But then here, these black squares are the positions of the correct indices

908
01:33:19,120 --> 01:33:25,680
 where we subtracted a one. And so what is this doing, right? These are the derivatives on

909
01:33:25,680 --> 01:33:33,120
 d logits. And so let's look at just the first row here. So that's what I'm doing here. I'm

910
01:33:33,120 --> 01:33:37,760
 calculating the probabilities of these logits, and then I'm taking just the first row. And this is the

911
01:33:37,760 --> 01:33:43,520
 probability row. And then the logits of the first row and multiplying by n just for us so that

912
01:33:43,520 --> 01:33:48,960
 we don't have the scaling by n in here, and everything is more interpretable. We see that it's

913
01:33:48,960 --> 01:33:53,920
 exactly equal to the probability, of course, but then the position of the correct index has a minus

914
01:33:53,920 --> 01:34:01,120
 equals one. So minus one on that position. And so notice that if you take the logits at zero,

915
01:34:01,120 --> 01:34:09,520
 and you sum it, it actually sums to zero. And so you should think of these gradients here at each

916
01:34:09,520 --> 01:34:18,080
 cell as like a force. We are going to be basically pulling down on the probabilities of the incorrect

917
01:34:18,080 --> 01:34:24,160
 characters. And we're going to be pulling up on the probability at the correct index. And that's

918
01:34:24,160 --> 01:34:32,400
 what's basically happening in each row. And the amount of push and pull is exactly equalized,

919
01:34:32,400 --> 01:34:36,960
 because the sum is zero. So the amount to which we pulled down on the probabilities and the demand

920
01:34:36,960 --> 01:34:42,800
 that we push up on the probability of the correct character is equal. So it's sort of the repulsion

921
01:34:42,800 --> 01:34:47,200
 and the attraction are equal. And think of the neural mat now as a, as a like a massive

922
01:34:47,200 --> 01:34:53,360
 pulley system or something like that, we're up here on top of the logits, and we're pulling up,

923
01:34:53,360 --> 01:34:56,560
 we're pulling down the probabilities of incorrect and pulling up the probability of the correct.

924
01:34:56,560 --> 01:35:02,000
 And in this complicated pulley system, because everything is mathematically just determined,

925
01:35:02,000 --> 01:35:06,800
 just think of it as sort of like this tension translating to this complicating pulley mechanism.

926
01:35:06,800 --> 01:35:11,600
 And then eventually we get a tug on the weights and the biases. And basically in each update,

927
01:35:11,600 --> 01:35:16,000
 we just kind of like tug in the direction that we like for each of these elements. And the

928
01:35:16,000 --> 01:35:20,480
 parameters are slowly given in to the tug. And that's what training in neural mat kind of like

929
01:35:20,480 --> 01:35:26,400
 looks like on a high level. And so I think the forces of push and pull in these gradients are

930
01:35:26,400 --> 01:35:31,120
 actually very intuitive here. We're pushing and pulling on the correct answer and the incorrect

931
01:35:31,120 --> 01:35:37,360
 answers. And the amount of force that we're applying is actually proportional to the probabilities

932
01:35:37,360 --> 01:35:42,000
 that came out in the forward pass. And so for example, if our probabilities came out exactly

933
01:35:42,000 --> 01:35:48,320
 correct, so they would have had zero everywhere except for one at the correct position, then the

934
01:35:48,320 --> 01:35:53,680
 the logits would be all row of zeros for that example, there would be no push and pull. So

935
01:35:53,680 --> 01:35:58,880
 the amount to which your prediction is incorrect is exactly the amount by which you're going to

936
01:35:58,880 --> 01:36:04,320
 get a pull or a push in that dimension. So if you have for example, a very confidently

937
01:36:04,320 --> 01:36:10,080
 mispredicted element here, then what's going to happen is that element is going to be pulled down

938
01:36:10,080 --> 01:36:15,680
 very heavily. And the correct answer is going to be pulled up to the same amount. And the other

939
01:36:15,680 --> 01:36:21,120
 characters are not going to be influenced too much. So the amount to which you mispredict is

940
01:36:21,120 --> 01:36:26,320
 then proportional to the strength of the pull. And that's happening independently in all the

941
01:36:26,320 --> 01:36:31,360
 dimensions of this of this tensor. And it's sort of very intuitive and very used to think through.

942
01:36:31,360 --> 01:36:35,760
 And that's basically the magic of the cross entropy loss and what is doing dynamically

943
01:36:35,760 --> 01:36:39,520
 in the backward pass of the neural mat. So now we get to exercise number three,

944
01:36:39,520 --> 01:36:45,120
 which is a very fun exercise, depending on your definition of fun. And we are going to do for

945
01:36:45,120 --> 01:36:50,080
 batch normalization exactly what we did for cross entropy loss in exercise number two. That is,

946
01:36:50,080 --> 01:36:54,480
 we are going to consider it as a glued single mathematical expression and back propagate through

947
01:36:54,480 --> 01:36:58,800
 it in a very efficient manner, because we are going to derive a much simpler formula for the

948
01:36:58,800 --> 01:37:02,720
 backward pass of batch normalization. And we're going to do that using pen and paper.

949
01:37:02,720 --> 01:37:07,600
 So previously, we've broken up batch normalization into all of the little intermediate pieces and

950
01:37:07,600 --> 01:37:12,400
 all the atomic operations inside it. And then we back propagate it through it one by one.

951
01:37:12,400 --> 01:37:19,440
 Now we just have a single sort of forward pass of a batch room. And it's all glued together.

952
01:37:19,440 --> 01:37:24,480
 And we see that we get the exact same result as before. Now for the batch backward pass,

953
01:37:24,480 --> 01:37:29,520
 we'd like to also implement a single formula basically for back propagating through this

954
01:37:29,520 --> 01:37:33,760
 entire operation that is the batch normalization. So in the forward pass previously,

955
01:37:33,760 --> 01:37:40,240
 we took H pre B and the hidden states of the pre-bacterialization and created H preact,

956
01:37:40,240 --> 01:37:45,280
 which is the hidden states just before the activation. In the batch normalization paper,

957
01:37:45,280 --> 01:37:52,160
 H pre B and is X and H preact is Y. So in the backward pass, what we'd like to do now is we have

958
01:37:52,160 --> 01:37:58,240
 DH preact and we'd like to produce DH pre-bien. And we'd like to do that in a very efficient

959
01:37:58,240 --> 01:38:04,720
 manner. So that's the name of the game calculate DH pre-bien given DH preact. And for the purposes

960
01:38:04,720 --> 01:38:09,680
 of this exercise, we're going to ignore gamma and beta and their derivatives because they take

961
01:38:09,680 --> 01:38:16,000
 on a very simple form in a very similar way to what we did up above. So let's calculate this,

962
01:38:16,560 --> 01:38:23,440
 given that right here. So to help you a little bit like I did before, I started off the implementation

963
01:38:23,440 --> 01:38:28,960
 here on pen and paper and I took two sheets of paper to derive the mathematical formulas for

964
01:38:28,960 --> 01:38:36,080
 the backward pass. And basically to set up the problem, just write out the mu sigma square variance,

965
01:38:36,080 --> 01:38:42,240
 X i hat and Y i exactly as in the paper except for the Bessel correction. And then

966
01:38:42,960 --> 01:38:47,280
 in the backward pass, we have the derivative of the loss with respect to all the elements of Y.

967
01:38:47,280 --> 01:38:54,160
 And remember that Y is a vector. There's there's multiple numbers here. So we have all the derivatives

968
01:38:54,160 --> 01:38:59,600
 with respect to all the Ys. And then there's a Dema and a Beta. And this is kind of like the

969
01:38:59,600 --> 01:39:05,440
 compute graph. The gamma and the Beta, there's the X hat. And then the mu and the sigma square

970
01:39:06,560 --> 01:39:13,840
 and the X. So we have DL by DYI and we won't DL by DXi for all the I's in these vectors.

971
01:39:13,840 --> 01:39:20,560
 So this is the compute graph and you have to be careful because I'm trying to note here that

972
01:39:20,560 --> 01:39:27,600
 these are vectors. There's many nodes here inside X, X hat and Y, but mu and sigma,

973
01:39:27,600 --> 01:39:33,280
 sorry, sigma square are just individual scalars, single numbers. So you have to be careful with

974
01:39:33,280 --> 01:39:36,400
 that. You have to imagine there's multiple nodes here or you're going to get your math wrong.

975
01:39:36,960 --> 01:39:43,520
 So as an example, I would suggest that you go in the following order, one, two, three,

976
01:39:43,520 --> 01:39:48,240
 four in terms of the back propagation. So back propagating to X hat, then to sigma square,

977
01:39:48,240 --> 01:39:54,640
 then into mu and then into X. Just like an anthropological sort in micro grad, we would

978
01:39:54,640 --> 01:39:58,640
 go from right to left. You're doing the exact same thing except you're doing it with symbols

979
01:39:58,640 --> 01:40:06,320
 and on a piece of paper. So for number one, I'm not giving away too much. If you want

980
01:40:06,320 --> 01:40:14,960
 DL of the XI hat, then we just take DL by D YI and multiply by gamma because of this expression

981
01:40:14,960 --> 01:40:22,480
 here, where any individual YI is just gamma times XI hat plus beta. So it doesn't help you too much

982
01:40:22,480 --> 01:40:29,040
 there, but this gives you basically the derivatives for all the accents. And so now try to go through

983
01:40:29,040 --> 01:40:37,440
 this computational graph and derive what is DL by D sigma square. And then what is DL by D mu?

984
01:40:37,440 --> 01:40:43,360
 And then what is DL by D X eventually? So give it a go and I'm going to be revealing the answer

985
01:40:43,360 --> 01:40:48,160
 one piece at a time. Okay, so to get DL by D sigma square, we have to remember again,

986
01:40:48,160 --> 01:40:54,000
 like I mentioned that there are many Xs, X hats here. And remember that sigma square is just a

987
01:40:54,000 --> 01:41:01,040
 single individual number here. So when we look at the expression for DL by D sigma square,

988
01:41:01,040 --> 01:41:09,760
 we have that we have to actually consider all the possible paths that we basically have that

989
01:41:09,760 --> 01:41:15,360
 there's many X hats and they all feed off from the all depend on sigma square. So sigma square

990
01:41:15,360 --> 01:41:19,760
 has a large fan out, there's lots of arrows coming out from sigma square into all the X hats.

991
01:41:20,640 --> 01:41:25,840
 And then there's a back propagating signal from each X hat into sigma square. And that's why we

992
01:41:25,840 --> 01:41:35,040
 actually need to sum over all those eyes from i equal to one to m of the DL by D X i hat, which

993
01:41:35,040 --> 01:41:41,360
 is the global gradient times the X i hat by D sigma square, which is the local gradient

994
01:41:41,360 --> 01:41:47,920
 of this operation here. And then mathematically, I'm just working it out here and I'm simplifying

995
01:41:47,920 --> 01:41:52,560
 and you get a certain expression for DL by D sigma square. We're going to be using this

996
01:41:52,560 --> 01:41:57,280
 expression when we back propagate into mu and then eventually into X. So now let's continue our

997
01:41:57,280 --> 01:42:04,320
 back propagation into mu. So what is DL by D mu? Now again, be careful that mu influences X hat

998
01:42:04,320 --> 01:42:09,840
 and X hat is actually lots of values. So for example, if our mini batch size is 32, as it is in our

999
01:42:09,840 --> 01:42:15,120
 example that we were working on, then this is 32 numbers and 32 arrows going back to mu.

1000
01:42:15,840 --> 01:42:19,840
 And then mu going to sigma square is just a single arrow because sigma square is a scalar.

1001
01:42:19,840 --> 01:42:26,160
 So in total, there are 33 arrows emanating from you. And then all of them have gradients coming

1002
01:42:26,160 --> 01:42:32,400
 into mu and they all need to be summed up. And so that's why when we look at the expression for DL

1003
01:42:32,400 --> 01:42:39,440
 by D mu, I am summing up over all the gradients of DL by D X i hat times the X i hat by D mu.

1004
01:42:40,640 --> 01:42:47,360
 So that's this arrow and the 32 arrows here. And then plus the one arrow from here, which is DL by D

1005
01:42:47,360 --> 01:42:52,800
 sigma square times the sigma square by D mu. So now we have to work out that expression.

1006
01:42:52,800 --> 01:42:59,280
 And let me just reveal the rest of it. Simplifying here is not complicated, the first term, and you

1007
01:42:59,280 --> 01:43:03,200
 just get an expression here. For the second term though, there's something really interesting that

1008
01:43:03,200 --> 01:43:10,560
 happens. When we look at the sigma square by D mu and we simplify, at one point, if we assume

1009
01:43:10,560 --> 01:43:16,800
 that in a special case where mu is actually the average of X i's, as it is in this case,

1010
01:43:16,800 --> 01:43:22,640
 then if we plug that in, then actually the gradient vanishes and becomes exactly zero.

1011
01:43:22,640 --> 01:43:29,520
 And that makes the entire second term cancel. And so these, if you just have a mathematical

1012
01:43:29,520 --> 01:43:34,720
 expression like this, and you look at D sigma square by D mu, you would get some mathematical formula

1013
01:43:34,720 --> 01:43:40,400
 for how mu impacts sigma square. But if it is the special case that mu is actually equal to

1014
01:43:40,400 --> 01:43:45,440
 the average, as it is in the case of rationalization, that gradient will actually vanish and become zero.

1015
01:43:45,440 --> 01:43:51,520
 So the whole term cancels, and we just get a fairly straightforward expression here for DL by D

1016
01:43:51,520 --> 01:43:57,920
 mu. Okay, and now we get to the craziest part, which is deriving DL by D xi, which is ultimately

1017
01:43:57,920 --> 01:44:04,080
 what we're after. Now, let's count. First of all, how many numbers are there inside X? As I

1018
01:44:04,080 --> 01:44:09,040
 mentioned, there are 32 numbers. There are 32 little xi's. And let's count the number of arrows

1019
01:44:09,040 --> 01:44:15,040
 emanating from each xi. There's an arrow going to mu, an arrow going to sigma square. And then

1020
01:44:15,040 --> 01:44:21,520
 there's an arrow going to x hat. But this arrow here, let's group now that a little bit. Each xi hat

1021
01:44:21,520 --> 01:44:29,280
 is just a function of xi and all the other scalars. So xi hat only depends on xi and none of the other

1022
01:44:29,280 --> 01:44:35,520
 axis. And so therefore, there are actually in this single arrow, there are 32 arrows. But those 32

1023
01:44:35,520 --> 01:44:40,720
 arrows are going exactly parallel. They don't interfere. They're just going parallel between x

1024
01:44:40,720 --> 01:44:45,440
 and x hat. You can look at it that way. And so how many arrows are emanating from each xi? There

1025
01:44:45,440 --> 01:44:52,880
 are three arrows, mu sigma square, and the associated x hat. And so in back propagation, we now need

1026
01:44:52,880 --> 01:44:58,640
 to apply the chain rule. And we need to add up those three contributions. So here's what that

1027
01:44:58,640 --> 01:45:06,560
 looks like if I just write that out. We're going through, we're changing through mu sigma square and

1028
01:45:06,560 --> 01:45:13,360
 through x hat. And those three terms are just here. Now we already have three of these. We have

1029
01:45:13,360 --> 01:45:19,600
 dl by d xi hat. We have dl by d mu, which we derived here. And we have dl by d sigma square,

1030
01:45:19,600 --> 01:45:26,320
 which we derived here. But we need three other terms here. This one, this one, and this one.

1031
01:45:26,320 --> 01:45:30,400
 So I invite you to try to derive them. It's not that complicated. You're just looking at

1032
01:45:30,400 --> 01:45:36,400
 these expressions here and differentiating with respect to xi. So give it a shot, but here's the

1033
01:45:36,400 --> 01:45:45,040
 result. Or at least what I got. Yeah, I'm just, I'm just differentiating with respect to xi for

1034
01:45:45,040 --> 01:45:48,320
 all of these expressions. And honestly, I don't think there's anything too tricky here. It's

1035
01:45:48,320 --> 01:45:53,760
 basic calculus. Now it gets a little bit more tricky is we are now going to plug everything

1036
01:45:53,760 --> 01:45:58,240
 together. So all of these terms multiplied with all of these terms and added up according to this

1037
01:45:58,240 --> 01:46:06,480
 formula. And that gets a little bit hairy. So what ends up happening is you get a large expression.

1038
01:46:06,480 --> 01:46:12,240
 And the thing to be very careful with here, of course, is we are working with a dl by d xi for

1039
01:46:12,240 --> 01:46:20,720
 specific by here. But when we are plugging in some of these terms, like say, this term here,

1040
01:46:20,720 --> 01:46:25,760
 dl by d sigma squared, you see how deal by d sigma squared, I end up with an expression.

1041
01:46:25,760 --> 01:46:32,480
 And I'm iterating over little eyes here. But I can't use i as the variable when I plug in here,

1042
01:46:32,480 --> 01:46:38,000
 because this is a different i from this i. This i here is just a place or a local variable for

1043
01:46:38,000 --> 01:46:43,280
 for a for loop in here. So here, when I plug that in, you notice that I rename the i to a j.

1044
01:46:43,280 --> 01:46:49,200
 Because I need to make sure that this j is not that this j is not this i. This j is like like a

1045
01:46:49,200 --> 01:46:54,240
 little local iterator over 32 terms. And so you have to be careful with that. When you're plugging

1046
01:46:54,240 --> 01:46:58,640
 in the expressions from here to here, you may have to rename eyes into j's. You have to be very

1047
01:46:58,640 --> 01:47:06,880
 careful what is actually an i with respect to dl by d xi. So some of these are j's. Some of these

1048
01:47:06,880 --> 01:47:13,920
 are i's. And then we simplify this expression. And I guess like the big thing to notice here is

1049
01:47:13,920 --> 01:47:18,240
 a bunch of terms just going to come out to the front and you can refactor them. There's a signal

1050
01:47:18,240 --> 01:47:23,040
 square plus epsilon raised to the power of negative three over two. This sigma square plus epsilon

1051
01:47:23,040 --> 01:47:28,320
 can be actually separated out into three terms. Each of them are sigma square plus epsilon to the

1052
01:47:28,320 --> 01:47:34,480
 negative one over two. So the three of them multiplied is equal to this. And then those three terms can

1053
01:47:34,480 --> 01:47:39,280
 go different places because of the multiplication. So one of them actually comes out to the front

1054
01:47:39,280 --> 01:47:46,000
 and will end up here outside. One of them joins up with this term and one of them joins up with

1055
01:47:46,000 --> 01:47:51,120
 this other term. And then when you simplify the expression, you'll notice that some of these terms

1056
01:47:51,120 --> 01:47:57,440
 that are coming out are just the xi hats. So you can simplify just by rewriting that. And what we

1057
01:47:57,440 --> 01:48:01,840
 end up with at the end is a fairly simple mathematical expression over here that I cannot simplify

1058
01:48:01,840 --> 01:48:07,120
 further. But basically, you'll notice that it only uses the stuff we have and it derives the

1059
01:48:07,120 --> 01:48:14,320
 thing we need. So we have dl by dy for all the i's. And those are used plenty of times here.

1060
01:48:14,960 --> 01:48:19,200
 And also in the initial, what we're using is these xi hats and xj hats. And they just come from the

1061
01:48:19,200 --> 01:48:26,000
 forward pass. And otherwise, this is a simple expression. And it gives us dl by d xi for all the

1062
01:48:26,000 --> 01:48:32,960
 i's. And that's ultimately what we're interested in. So that's the end of a bachelor backward pass

1063
01:48:32,960 --> 01:48:38,400
 analytically. Let's now implement this final result. Okay, so I implemented the expression

1064
01:48:38,400 --> 01:48:43,600
 into a single line of code here. And you can see that the max diff is tiny. So this is the

1065
01:48:43,600 --> 01:48:50,800
 correct implementation of this formula. Now, I'll just basically tell you that getting this formula

1066
01:48:50,800 --> 01:48:55,680
 here from this mathematical expression was not trivial. And there's a lot going on packed into

1067
01:48:55,680 --> 01:49:00,800
 this one formula. And this is all exercised by itself. Because you have to consider the fact

1068
01:49:00,800 --> 01:49:06,640
 that this formula here is just for a single neuron and a batch of 32 examples. But what I'm doing

1069
01:49:06,640 --> 01:49:12,800
 here is I'm actually, we actually have 64 neurons. And so this expression has to in parallel evaluate

1070
01:49:12,800 --> 01:49:17,840
 the batch from backward pass for all those 64 neurons in parallel independently. So this has

1071
01:49:17,840 --> 01:49:25,840
 to happen basically in every single column of the inputs here. And in addition to that,

1072
01:49:25,840 --> 01:49:29,360
 you see how there are a bunch of sums here. And we need to make sure that when I do those

1073
01:49:29,360 --> 01:49:34,880
 sums that they broadcast correctly onto everything else that's here. And so getting this expression

1074
01:49:34,880 --> 01:49:38,000
 is just like highly non trivial. And I invite you to basically look through it and step through

1075
01:49:38,000 --> 01:49:44,880
 it. And it's a whole exercise to make sure that this checks out. But once all the shapes agree,

1076
01:49:44,880 --> 01:49:48,800
 and once you convince yourself that it's correct, you can also verify that PyTorch gets the exact

1077
01:49:48,800 --> 01:49:53,760
 same answer as well. And so that gives you a lot of peace of mind that this mathematical formula is

1078
01:49:53,760 --> 01:49:59,760
 correctly implemented here and broadcast it correctly and replicated in parallel for all the 64 neurons

1079
01:49:59,760 --> 01:50:06,720
 inside this batch term layer. Okay, and finally exercise number four asks you to put it all together.

1080
01:50:06,720 --> 01:50:11,520
 And here we have a redefinition of the entire problem. So you see that we re-enlistalize a neural

1081
01:50:11,520 --> 01:50:16,880
 net from scratch and everything. And then here, instead of calling the loss that backward, we want

1082
01:50:16,880 --> 01:50:22,640
 to have the manual back propagation here as we derived it up above. So go up, copy paste all the

1083
01:50:22,640 --> 01:50:27,520
 chunks of code that we've already derived, put them here and derive your own gradients, and then

1084
01:50:27,520 --> 01:50:32,800
 optimize this neural net, basically using your own gradients all the way to the calibration of the

1085
01:50:32,800 --> 01:50:37,280
 batch norm and the evaluation of the loss. And I was able to achieve quite a good loss, basically

1086
01:50:37,280 --> 01:50:41,760
 the same loss you would achieve before. And that shouldn't be surprising because all we've done is

1087
01:50:41,760 --> 01:50:48,000
 we've really gotten into loss that backward and we've pulled out all the code and inserted it here.

1088
01:50:48,000 --> 01:50:52,720
 But those gradients are identical and everything is identical and the results are identical. It's

1089
01:50:52,720 --> 01:50:57,760
 just that we have full visibility on exactly what goes on under the hood of lot that backward in

1090
01:50:57,760 --> 01:51:04,160
 this specific case. Okay, and this is all of our code. This is the full backward pass using basically

1091
01:51:04,160 --> 01:51:10,240
 the simplified backward pass for the cross entropy loss and the best normalization. So back propagating

1092
01:51:10,240 --> 01:51:16,160
 through cross entropy, the second layer, the 10 h null linearity, the best normalization

1093
01:51:16,160 --> 01:51:21,760
 through the first layer and through the embedding. And so you see that this is only maybe what is

1094
01:51:21,760 --> 01:51:27,200
 this 20 lines of code or something like that. And that's what gives us gradients. And now we can

1095
01:51:27,200 --> 01:51:32,400
 potentially erase loss and backward. So the way I have the code set up is you should be able to run

1096
01:51:32,400 --> 01:51:37,360
 this entire cell once you fill this in. And this will run for only 100 iterations and then break.

1097
01:51:37,360 --> 01:51:41,760
 And it breaks because it gives you an opportunity to check your gradients against pytorch.

1098
01:51:41,760 --> 01:51:49,680
 So here are gradients we see are not exactly equal. They are approximately equal. And the

1099
01:51:49,680 --> 01:51:54,320
 differences are tiny, one in negative nine or so. And I don't exactly know where they're coming from,

1100
01:51:54,320 --> 01:51:58,400
 to be honest. So once we have some confidence that the gradients are basically correct,

1101
01:51:58,400 --> 01:52:04,480
 we can take out the gradient checking. We can disable this breaking statement.

1102
01:52:04,480 --> 01:52:11,280
 And then we can basically disable loss the backward. We don't need it anymore.

1103
01:52:11,280 --> 01:52:17,600
 Feels amazing to say that. And then here, when we are doing the update, we're not going to use

1104
01:52:17,600 --> 01:52:23,920
 p.grad. This is the old way of pytorch. We don't have that anymore, because we're not doing backward.

1105
01:52:23,920 --> 01:52:28,160
 We are going to use this update, where we you see that I'm iterating over,

1106
01:52:28,160 --> 01:52:33,680
 I've arranged the grads to be in the same order as the parameters, and I'm zipping them up the

1107
01:52:33,680 --> 01:52:38,240
 gradients and the parameters into p and grad. And then here, I'm going to step with just the

1108
01:52:38,240 --> 01:52:47,520
 grad that we derived manually. So the last piece is that none of this now requires gradients from

1109
01:52:47,520 --> 01:52:56,640
 pytorch. And so one thing you can do here, is you can do with torch.no.grad and offset this whole

1110
01:52:56,640 --> 01:53:00,560
 code block. And really what you're saying is you're telling pytorch that, hey, I'm not going to

1111
01:53:00,560 --> 01:53:04,640
 call backward on any of this. And this allows pytorch to be a bit more efficient with all of it.

1112
01:53:04,640 --> 01:53:16,080
 And then we should be able to just run this. And it's running. And you see that loss of the

1113
01:53:16,080 --> 01:53:23,760
 backward is commented out, and we're optimizing. So we're going to lead this run, and hopefully

1114
01:53:23,760 --> 01:53:29,520
 we get a good result. Okay, so I allowed the neural out to finish optimization. Then here,

1115
01:53:29,520 --> 01:53:34,480
 I calibrate the batch parameters, because I did not keep track of the running mean and very

1116
01:53:34,480 --> 01:53:40,400
 variance in their training loop. Then here, I ran the loss, and you see that we actually obtained

1117
01:53:40,400 --> 01:53:45,120
 a pretty good loss, very similar to what we've achieved before. And then here, I'm sampling from

1118
01:53:45,120 --> 01:53:49,920
 the model, and we see some of the name like gibberish that we're sort of used to. So basically, the

1119
01:53:49,920 --> 01:53:56,160
 model worked and samples, per decent results, compared to what we're used to. So everything is

1120
01:53:56,160 --> 01:54:00,800
 the same. But of course, the big deal is that we did not use lots of backward, we did not use

1121
01:54:00,800 --> 01:54:06,080
 pytorch autograd, and we estimated our gradients ourselves by hand. And so hopefully you're looking

1122
01:54:06,080 --> 01:54:10,880
 at this, the backward pass of this neural net, and you're thinking to yourself, actually, that's not

1123
01:54:10,880 --> 01:54:16,640
 too complicated. Each one of these layers is like three lines of code or something like that.

1124
01:54:16,640 --> 01:54:20,800
 And most of it is fairly straightforward, potentially with the notable exception of the

1125
01:54:20,800 --> 01:54:25,840
 batch normalization backward pass. Otherwise, it's pretty good. Okay, and that's everything I wanted

1126
01:54:25,840 --> 01:54:30,880
 to cover for this lecture. So hopefully you found this interesting. And what I liked about it,

1127
01:54:30,880 --> 01:54:37,200
 honestly, is that it gave us a very nice diversity of layers to back propagate through. And I think

1128
01:54:37,200 --> 01:54:41,200
 it gives a pretty nice and comprehensive sense of how these backward passes are implemented and

1129
01:54:41,200 --> 01:54:45,600
 how they work. And you'd be able to derive them yourself. But of course, in practice, you probably

1130
01:54:45,600 --> 01:54:49,840
 don't want to, and you want to use the pytorch autograd. But hopefully you have some intuition

1131
01:54:49,840 --> 01:54:54,720
 about how gradients flow backwards through the neural net, starting at the loss, and how they

1132
01:54:54,720 --> 01:55:00,240
 flow through all the variables and all the intermediate results. And if you understood a good chunk of

1133
01:55:00,240 --> 01:55:04,240
 it, and if you have a sense of that, then you can count yourself as one of these buff doggies on the

1134
01:55:04,240 --> 01:55:10,160
 left, instead of the doies on the right here. Now, in the next lecture, we're actually going to go

1135
01:55:10,160 --> 01:55:16,240
 to recurrent neural nets, LSTMs, and all the other variants of Arnas. And we're going to start to

1136
01:55:16,240 --> 01:55:21,360
 complexify the architecture and start to achieve better log likelihoods. And so I'm really looking

1137
01:55:21,360 --> 01:55:23,680
 forward to that. And I'll see you then.

